{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06013d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a7bbede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c20a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required constants.\n",
    "ROOT_DIR = '../retina_lesion_'\n",
    "VALID_SPLIT = 0.1\n",
    "IMAGE_SIZE = 224 # Image size of resize when applying transforms.\n",
    "BATCH_SIZE = 16 \n",
    "NUM_WORKERS = 4 # Number of parallel processes for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc46424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epohs, model,optimizer, criterion,pretrained):\n",
    "    torch.save({\n",
    "                'epoch':epochs,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss':criterion,\n",
    "                },f\"../retina_lesion_/model_pretrained_{pretrained}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ce3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the plots\n",
    "def save_plots(train_acc, valid_acc, train_loss, valid_loss, pretrained):\n",
    "    \"\"\"\n",
    "    Function to save the loss and accuracy plots to disk.\n",
    "    \"\"\"\n",
    "    # accuracy plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_acc, color='green', linestyle='-', \n",
    "        label='train accuracy'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_acc, color='blue', linestyle='-', \n",
    "        label='validataion accuracy'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"../retina_lesion_/accuracy_pretrained_{pretrained}.png\")\n",
    "    \n",
    "    # loss plots\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='orange', linestyle='-', \n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-', \n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"../retina_lesion_/loss_pretrained_{pretrained}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5106c",
   "metadata": {},
   "source": [
    "### PREPARING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f67c0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab7321b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms\n",
    "def get_train_transform(IMAGE_SIZE, pretrained):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.GaussianBlur(kernel_size = (5,9), sigma=(0.1,5)),\n",
    "        tranforms.RandomAdjustSharpness(sharpness_factor=1,p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform(pretrained)\n",
    "    ])\n",
    "    return train_transform\n",
    "\n",
    "# Validation transforms\n",
    "def get_valid_transform(IMAGE_SIZE, pretrained):\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize_transform(pretrained)\n",
    "    ])\n",
    "    return valid_transform\n",
    "# Image normalization transforms.\n",
    "def normalize_transform(pretrained):\n",
    "    if pretrained: # Normalization for pre-trained weights.\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "    else: # Normalization when training from scratch.\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5]\n",
    "        )\n",
    "    return normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddac585",
   "metadata": {},
   "source": [
    "### Training/validation Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61f4623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(pretrained):\n",
    "    \"\"\"\n",
    "    Function to prepare the Datasets.\n",
    "    :param pretrained: Boolean, True or False.\n",
    "    Returns the training and validation datasets along \n",
    "    with the class names.\n",
    "    \"\"\"\n",
    "    dataset = datasets.ImageFolder(\n",
    "        ROOT_DIR, \n",
    "        transform=(get_train_transform(IMAGE_SIZE, pretrained))\n",
    "    )\n",
    "    dataset_test = datasets.ImageFolder(\n",
    "        ROOT_DIR, \n",
    "        transform=(get_valid_transform(IMAGE_SIZE, pretrained))\n",
    "    )\n",
    "    dataset_size = len(dataset)\n",
    "    # Calculate the validation dataset size.\n",
    "    valid_size = int(VALID_SPLIT*dataset_size)\n",
    "    # Radomize the data indices.\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    # Training and validation sets.\n",
    "    dataset_train = Subset(dataset, indices[:-valid_size])\n",
    "    dataset_valid = Subset(dataset_test, indices[-valid_size:])\n",
    "    return dataset_train, dataset_valid, dataset.classes\n",
    "def get_data_loaders(dataset_train, dataset_valid):\n",
    "    \"\"\"\n",
    "    Prepares the training and validation data loaders.\n",
    "    :param dataset_train: The training dataset.\n",
    "    :param dataset_valid: The validation dataset.\n",
    "    Returns the training and validation data loaders.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=BATCH_SIZE, \n",
    "        shuffle=True, num_workers=NUM_WORKERS\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, batch_size=BATCH_SIZE, \n",
    "        shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "    return train_loader, valid_loader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26374656",
   "metadata": {},
   "source": [
    "### THE EFFICIENTNEBO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "521066b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "def build_model(pretrained=True, fine_tune=True, num_classes=10):\n",
    "    if pretrained:\n",
    "        print('[INFO]: Loading pre-trained weights')\n",
    "    else:\n",
    "        print('[INFO]: Not loading pre-trained weights')\n",
    "    model = models.efficientnet_b0(pretrained=pretrained)\n",
    "    if fine_tune:\n",
    "        print('[INFO]: Fine-tuning all layers...')\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = True\n",
    "    elif not fine_tune:\n",
    "        print('[INFO]: Freezing hidden layers...')\n",
    "        for params in model.parameters():\n",
    "            params.requires_grad = False\n",
    "    # Change the final classification head.\n",
    "    model.classifier[1] = nn.Linear(in_features=1280, out_features=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2031b",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eee9687b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#from model import build_model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_datasets, get_data_loaders\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model, save_plots\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# construct the argument parser\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "#from model import build_model\n",
    "from datasets import get_datasets, get_data_loaders\n",
    "from utils import save_model, save_plots\n",
    "\n",
    "\n",
    "# construct the argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '-e', '--epochs', type=int, default=20,\n",
    "    help='Number of epochs to train our network for'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-pt', '--pretrained', action='store_true',\n",
    "    help='Whether to use pretrained weights or not'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-lr', '--learning-rate', type=float,\n",
    "    dest='learning_rate', default=0.0001,\n",
    "    help='Learning rate for training the model'\n",
    ")\n",
    "args = vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132cc18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea35419a11bf735ffd8e86b26e0a8a14e6dd58c2ad332018fd3c221b1ca9defb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
