{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading required libraries\n",
    "from asyncio import as_completed\n",
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms,models,datasets\n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "import cv2\n",
    "import glob, numpy as np, pandas as pd\n",
    "from glob import glob\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.GaussianBlur(kernel_size=(5,9), sigma=(0.1,5)),\n",
    "        transforms.RandomAdjustSharpness(sharpness_factor=1, p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])    \n",
    "    ])\n",
    "    #validation Transform\n",
    "    valid_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    #test Transform\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24168 6056 6056\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_dir =\"kaggle/train\"\n",
    "valid_dir = \"kaggle/test\"\n",
    "bs = 64\n",
    "num_classes = 5\n",
    "# Load the Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root = train_dir,\n",
    "    transform =train_transform),\n",
    "    'valid': datasets.ImageFolder(root = valid_dir,\n",
    "    transform = valid_transform),\n",
    "    'test': datasets.ImageFolder(root = valid_dir,\n",
    "    transform = test_transform)\n",
    "}\n",
    "\n",
    "#Size of the data to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size  = len(data['test'])\n",
    "\n",
    "#Creating iterators for the DataLoader using DataLoader module\n",
    "train_data = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data = DataLoader(data['valid'],batch_size=bs, shuffle=True)\n",
    "test_data = DataLoader(data['test'],batch_size=bs, shuffle=True)\n",
    "print(train_data_size,valid_data_size,test_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mkhan\\Desktop\\home\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mkhan\\Desktop\\home\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Loading the pretrained model\n",
    "model = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the final layer of the model for transfer learning\n",
    "fc_inputs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256,5),\n",
    "    nn.LogSoftmax(dim=1) # for using NLLLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer and Loss function\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20\n",
      "Batch number : 000, Training: Loss:  1.7010, Accuracy: 0.0625\n",
      "Batch number : 001, Training: Loss:  0.5532, Accuracy: 0.8594\n",
      "Batch number : 002, Training: Loss:  0.5230, Accuracy: 0.8906\n",
      "Batch number : 003, Training: Loss:  0.6184, Accuracy: 0.8594\n",
      "Batch number : 004, Training: Loss:  0.5642, Accuracy: 0.8750\n",
      "Batch number : 005, Training: Loss:  0.5167, Accuracy: 0.8906\n",
      "Batch number : 006, Training: Loss:  0.9077, Accuracy: 0.8125\n",
      "Batch number : 007, Training: Loss:  1.0081, Accuracy: 0.7812\n",
      "Batch number : 008, Training: Loss:  0.2834, Accuracy: 0.9219\n",
      "Batch number : 009, Training: Loss:  1.0343, Accuracy: 0.8125\n",
      "Batch number : 010, Training: Loss:  0.8408, Accuracy: 0.7969\n",
      "Batch number : 011, Training: Loss:  0.5132, Accuracy: 0.8906\n",
      "Batch number : 012, Training: Loss:  0.7006, Accuracy: 0.7969\n",
      "Batch number : 013, Training: Loss:  0.6286, Accuracy: 0.8281\n",
      "Batch number : 014, Training: Loss:  0.6078, Accuracy: 0.8594\n",
      "Batch number : 015, Training: Loss:  0.5616, Accuracy: 0.9062\n",
      "Batch number : 016, Training: Loss:  0.5049, Accuracy: 0.8906\n",
      "Batch number : 017, Training: Loss:  0.7659, Accuracy: 0.8125\n",
      "Batch number : 018, Training: Loss:  0.8117, Accuracy: 0.8125\n",
      "Batch number : 019, Training: Loss:  0.4677, Accuracy: 0.9062\n",
      "Batch number : 020, Training: Loss:  0.6184, Accuracy: 0.8438\n",
      "Batch number : 021, Training: Loss:  0.4219, Accuracy: 0.9219\n",
      "Batch number : 022, Training: Loss:  0.5216, Accuracy: 0.8750\n",
      "Batch number : 023, Training: Loss:  0.5174, Accuracy: 0.9062\n",
      "Batch number : 024, Training: Loss:  0.9404, Accuracy: 0.7812\n",
      "Batch number : 025, Training: Loss:  0.6485, Accuracy: 0.8438\n",
      "Batch number : 026, Training: Loss:  0.6298, Accuracy: 0.8281\n",
      "Batch number : 027, Training: Loss:  0.7321, Accuracy: 0.8281\n",
      "Batch number : 028, Training: Loss:  0.6008, Accuracy: 0.8594\n",
      "Batch number : 029, Training: Loss:  0.7094, Accuracy: 0.7969\n",
      "Batch number : 030, Training: Loss:  0.5256, Accuracy: 0.9062\n",
      "Batch number : 031, Training: Loss:  0.4948, Accuracy: 0.8750\n",
      "Batch number : 032, Training: Loss:  0.4270, Accuracy: 0.8750\n",
      "Batch number : 033, Training: Loss:  0.4543, Accuracy: 0.8906\n",
      "Batch number : 034, Training: Loss:  0.8878, Accuracy: 0.7812\n",
      "Batch number : 035, Training: Loss:  0.4441, Accuracy: 0.8906\n",
      "Batch number : 036, Training: Loss:  0.5029, Accuracy: 0.8594\n",
      "Batch number : 037, Training: Loss:  0.8630, Accuracy: 0.7812\n",
      "Batch number : 038, Training: Loss:  0.6483, Accuracy: 0.8438\n",
      "Batch number : 039, Training: Loss:  0.5961, Accuracy: 0.8594\n",
      "Batch number : 040, Training: Loss:  0.4232, Accuracy: 0.9062\n",
      "Batch number : 041, Training: Loss:  0.5272, Accuracy: 0.8906\n",
      "Batch number : 042, Training: Loss:  0.4703, Accuracy: 0.8594\n",
      "Batch number : 043, Training: Loss:  0.4536, Accuracy: 0.8594\n",
      "Batch number : 044, Training: Loss:  0.4955, Accuracy: 0.8906\n",
      "Batch number : 045, Training: Loss:  0.6390, Accuracy: 0.8281\n",
      "Batch number : 046, Training: Loss:  0.7362, Accuracy: 0.7500\n",
      "Batch number : 047, Training: Loss:  0.6334, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.4455, Accuracy: 0.8906\n",
      "Batch number : 049, Training: Loss:  0.4161, Accuracy: 0.9062\n",
      "Batch number : 050, Training: Loss:  0.4448, Accuracy: 0.8906\n",
      "Batch number : 051, Training: Loss:  0.5453, Accuracy: 0.8750\n",
      "Batch number : 052, Training: Loss:  0.3854, Accuracy: 0.9062\n",
      "Batch number : 053, Training: Loss:  0.3849, Accuracy: 0.9219\n",
      "Batch number : 054, Training: Loss:  0.6374, Accuracy: 0.8125\n",
      "Batch number : 055, Training: Loss:  0.3531, Accuracy: 0.9062\n",
      "Batch number : 056, Training: Loss:  0.3721, Accuracy: 0.9062\n",
      "Batch number : 057, Training: Loss:  0.3476, Accuracy: 0.9219\n",
      "Batch number : 058, Training: Loss:  0.5237, Accuracy: 0.8594\n",
      "Batch number : 059, Training: Loss:  0.3175, Accuracy: 0.9219\n",
      "Batch number : 060, Training: Loss:  0.5988, Accuracy: 0.8438\n",
      "Batch number : 061, Training: Loss:  0.5206, Accuracy: 0.8594\n",
      "Batch number : 062, Training: Loss:  0.5562, Accuracy: 0.8594\n",
      "Batch number : 063, Training: Loss:  0.4274, Accuracy: 0.8750\n",
      "Batch number : 064, Training: Loss:  0.3395, Accuracy: 0.9219\n",
      "Batch number : 065, Training: Loss:  0.5106, Accuracy: 0.8750\n",
      "Batch number : 066, Training: Loss:  0.4975, Accuracy: 0.8906\n",
      "Batch number : 067, Training: Loss:  0.4359, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.5167, Accuracy: 0.8750\n",
      "Batch number : 069, Training: Loss:  0.4499, Accuracy: 0.9062\n",
      "Batch number : 070, Training: Loss:  0.5947, Accuracy: 0.8438\n",
      "Batch number : 071, Training: Loss:  0.3412, Accuracy: 0.9062\n",
      "Batch number : 072, Training: Loss:  0.5577, Accuracy: 0.8438\n",
      "Batch number : 073, Training: Loss:  0.7842, Accuracy: 0.7812\n",
      "Batch number : 074, Training: Loss:  0.4875, Accuracy: 0.8594\n",
      "Batch number : 075, Training: Loss:  0.2714, Accuracy: 0.9531\n",
      "Batch number : 076, Training: Loss:  0.6062, Accuracy: 0.8594\n",
      "Batch number : 077, Training: Loss:  0.7663, Accuracy: 0.7969\n",
      "Batch number : 078, Training: Loss:  0.5976, Accuracy: 0.8281\n",
      "Batch number : 079, Training: Loss:  0.6475, Accuracy: 0.7969\n",
      "Batch number : 080, Training: Loss:  0.5174, Accuracy: 0.8594\n",
      "Batch number : 081, Training: Loss:  0.6449, Accuracy: 0.8281\n",
      "Batch number : 082, Training: Loss:  0.4658, Accuracy: 0.9062\n",
      "Batch number : 083, Training: Loss:  0.6445, Accuracy: 0.7969\n",
      "Batch number : 084, Training: Loss:  0.5185, Accuracy: 0.8594\n",
      "Batch number : 085, Training: Loss:  0.5797, Accuracy: 0.8281\n",
      "Batch number : 086, Training: Loss:  0.4184, Accuracy: 0.9062\n",
      "Batch number : 087, Training: Loss:  0.5589, Accuracy: 0.8438\n",
      "Batch number : 088, Training: Loss:  0.4500, Accuracy: 0.8750\n",
      "Batch number : 089, Training: Loss:  0.4241, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.6446, Accuracy: 0.8125\n",
      "Batch number : 091, Training: Loss:  0.8167, Accuracy: 0.7969\n",
      "Batch number : 092, Training: Loss:  0.4478, Accuracy: 0.8750\n",
      "Batch number : 093, Training: Loss:  0.3306, Accuracy: 0.9375\n",
      "Batch number : 094, Training: Loss:  0.6787, Accuracy: 0.7812\n",
      "Batch number : 095, Training: Loss:  0.5490, Accuracy: 0.8594\n",
      "Batch number : 096, Training: Loss:  0.6267, Accuracy: 0.7969\n",
      "Batch number : 097, Training: Loss:  0.4927, Accuracy: 0.8750\n",
      "Batch number : 098, Training: Loss:  0.3779, Accuracy: 0.9062\n",
      "Batch number : 099, Training: Loss:  0.4479, Accuracy: 0.8750\n",
      "Batch number : 100, Training: Loss:  0.5235, Accuracy: 0.8594\n",
      "Batch number : 101, Training: Loss:  0.3300, Accuracy: 0.9219\n",
      "Batch number : 102, Training: Loss:  0.7887, Accuracy: 0.7656\n",
      "Batch number : 103, Training: Loss:  0.5512, Accuracy: 0.8438\n",
      "Batch number : 104, Training: Loss:  0.5612, Accuracy: 0.8281\n",
      "Batch number : 105, Training: Loss:  0.4168, Accuracy: 0.9062\n",
      "Batch number : 106, Training: Loss:  0.5794, Accuracy: 0.8125\n",
      "Batch number : 107, Training: Loss:  0.4906, Accuracy: 0.8438\n",
      "Batch number : 108, Training: Loss:  0.3300, Accuracy: 0.9219\n",
      "Batch number : 109, Training: Loss:  0.5660, Accuracy: 0.8750\n",
      "Batch number : 110, Training: Loss:  0.6076, Accuracy: 0.8438\n",
      "Batch number : 111, Training: Loss:  0.3379, Accuracy: 0.8906\n",
      "Batch number : 112, Training: Loss:  0.7307, Accuracy: 0.7812\n",
      "Batch number : 113, Training: Loss:  0.4395, Accuracy: 0.8594\n",
      "Batch number : 114, Training: Loss:  0.5766, Accuracy: 0.8438\n",
      "Batch number : 115, Training: Loss:  0.6296, Accuracy: 0.8438\n",
      "Batch number : 116, Training: Loss:  0.5720, Accuracy: 0.8750\n",
      "Batch number : 117, Training: Loss:  0.5016, Accuracy: 0.8594\n",
      "Batch number : 118, Training: Loss:  0.4608, Accuracy: 0.8750\n",
      "Batch number : 119, Training: Loss:  0.5506, Accuracy: 0.8438\n",
      "Batch number : 120, Training: Loss:  0.4585, Accuracy: 0.8906\n",
      "Batch number : 121, Training: Loss:  0.5746, Accuracy: 0.7969\n",
      "Batch number : 122, Training: Loss:  0.6692, Accuracy: 0.8125\n",
      "Batch number : 123, Training: Loss:  0.5222, Accuracy: 0.8438\n",
      "Batch number : 124, Training: Loss:  0.5395, Accuracy: 0.8594\n",
      "Batch number : 125, Training: Loss:  0.4992, Accuracy: 0.8750\n",
      "Batch number : 126, Training: Loss:  0.6331, Accuracy: 0.8281\n",
      "Batch number : 127, Training: Loss:  0.5777, Accuracy: 0.8125\n",
      "Batch number : 128, Training: Loss:  0.3916, Accuracy: 0.9219\n",
      "Batch number : 129, Training: Loss:  0.4010, Accuracy: 0.9062\n",
      "Batch number : 130, Training: Loss:  0.6104, Accuracy: 0.8438\n",
      "Batch number : 131, Training: Loss:  0.5044, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.4853, Accuracy: 0.8750\n",
      "Batch number : 133, Training: Loss:  0.6364, Accuracy: 0.8125\n",
      "Batch number : 134, Training: Loss:  0.3949, Accuracy: 0.9375\n",
      "Batch number : 135, Training: Loss:  0.6393, Accuracy: 0.8438\n",
      "Batch number : 136, Training: Loss:  0.1583, Accuracy: 1.0000\n",
      "Batch number : 137, Training: Loss:  0.6853, Accuracy: 0.8125\n",
      "Batch number : 138, Training: Loss:  0.4623, Accuracy: 0.8594\n",
      "Batch number : 139, Training: Loss:  0.6563, Accuracy: 0.8125\n",
      "Batch number : 140, Training: Loss:  0.4172, Accuracy: 0.8750\n",
      "Batch number : 141, Training: Loss:  0.5022, Accuracy: 0.8438\n",
      "Batch number : 142, Training: Loss:  0.5858, Accuracy: 0.8750\n",
      "Batch number : 143, Training: Loss:  0.4225, Accuracy: 0.9219\n",
      "Batch number : 144, Training: Loss:  0.5186, Accuracy: 0.8594\n",
      "Batch number : 145, Training: Loss:  0.4946, Accuracy: 0.8438\n",
      "Batch number : 146, Training: Loss:  0.4647, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.4110, Accuracy: 0.8750\n",
      "Batch number : 148, Training: Loss:  0.7391, Accuracy: 0.7812\n",
      "Batch number : 149, Training: Loss:  0.5527, Accuracy: 0.8750\n",
      "Batch number : 150, Training: Loss:  0.3374, Accuracy: 0.9375\n",
      "Batch number : 151, Training: Loss:  0.4956, Accuracy: 0.8438\n",
      "Batch number : 152, Training: Loss:  0.8384, Accuracy: 0.7500\n",
      "Batch number : 153, Training: Loss:  0.6015, Accuracy: 0.8438\n",
      "Batch number : 154, Training: Loss:  0.4679, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.5708, Accuracy: 0.8438\n",
      "Batch number : 156, Training: Loss:  0.7085, Accuracy: 0.7656\n",
      "Batch number : 157, Training: Loss:  0.4836, Accuracy: 0.8750\n",
      "Batch number : 158, Training: Loss:  0.6304, Accuracy: 0.8125\n",
      "Batch number : 159, Training: Loss:  0.3846, Accuracy: 0.8906\n",
      "Batch number : 160, Training: Loss:  0.6010, Accuracy: 0.8125\n",
      "Batch number : 161, Training: Loss:  0.3888, Accuracy: 0.8906\n",
      "Batch number : 162, Training: Loss:  0.7988, Accuracy: 0.7812\n",
      "Batch number : 163, Training: Loss:  0.4404, Accuracy: 0.9062\n",
      "Batch number : 164, Training: Loss:  0.4811, Accuracy: 0.8594\n",
      "Batch number : 165, Training: Loss:  0.3887, Accuracy: 0.9062\n",
      "Batch number : 166, Training: Loss:  0.8582, Accuracy: 0.7969\n",
      "Batch number : 167, Training: Loss:  0.5497, Accuracy: 0.8594\n",
      "Batch number : 168, Training: Loss:  0.5204, Accuracy: 0.8438\n",
      "Batch number : 169, Training: Loss:  0.5546, Accuracy: 0.8281\n",
      "Batch number : 170, Training: Loss:  0.3891, Accuracy: 0.9219\n",
      "Batch number : 171, Training: Loss:  0.4525, Accuracy: 0.8750\n",
      "Batch number : 172, Training: Loss:  0.5010, Accuracy: 0.8750\n",
      "Batch number : 173, Training: Loss:  0.5061, Accuracy: 0.8125\n",
      "Batch number : 174, Training: Loss:  0.5853, Accuracy: 0.8125\n",
      "Batch number : 175, Training: Loss:  0.6273, Accuracy: 0.7969\n",
      "Batch number : 176, Training: Loss:  0.6553, Accuracy: 0.8438\n",
      "Batch number : 177, Training: Loss:  0.4439, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.7664, Accuracy: 0.7812\n",
      "Batch number : 179, Training: Loss:  0.5808, Accuracy: 0.8438\n",
      "Batch number : 180, Training: Loss:  0.3677, Accuracy: 0.9375\n",
      "Batch number : 181, Training: Loss:  0.6375, Accuracy: 0.8125\n",
      "Batch number : 182, Training: Loss:  0.5476, Accuracy: 0.8281\n",
      "Batch number : 183, Training: Loss:  0.4800, Accuracy: 0.8750\n",
      "Batch number : 184, Training: Loss:  0.4830, Accuracy: 0.8750\n",
      "Batch number : 185, Training: Loss:  0.7070, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.8791, Accuracy: 0.7188\n",
      "Batch number : 187, Training: Loss:  0.5444, Accuracy: 0.8438\n",
      "Batch number : 188, Training: Loss:  0.5521, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.6295, Accuracy: 0.8281\n",
      "Batch number : 190, Training: Loss:  0.4634, Accuracy: 0.9062\n",
      "Batch number : 191, Training: Loss:  0.6030, Accuracy: 0.8438\n",
      "Batch number : 192, Training: Loss:  0.5254, Accuracy: 0.8438\n",
      "Batch number : 193, Training: Loss:  0.4198, Accuracy: 0.8906\n",
      "Batch number : 194, Training: Loss:  0.2713, Accuracy: 0.9531\n",
      "Batch number : 195, Training: Loss:  0.6315, Accuracy: 0.8438\n",
      "Batch number : 196, Training: Loss:  0.3703, Accuracy: 0.8906\n",
      "Batch number : 197, Training: Loss:  0.2865, Accuracy: 0.9531\n",
      "Batch number : 198, Training: Loss:  0.3798, Accuracy: 0.9062\n",
      "Batch number : 199, Training: Loss:  0.3787, Accuracy: 0.9375\n",
      "Batch number : 200, Training: Loss:  0.6715, Accuracy: 0.8125\n",
      "Batch number : 201, Training: Loss:  0.2927, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.5784, Accuracy: 0.8438\n",
      "Batch number : 203, Training: Loss:  0.6255, Accuracy: 0.8125\n",
      "Batch number : 204, Training: Loss:  0.4318, Accuracy: 0.8594\n",
      "Batch number : 205, Training: Loss:  0.4630, Accuracy: 0.8594\n",
      "Batch number : 206, Training: Loss:  0.5377, Accuracy: 0.8281\n",
      "Batch number : 207, Training: Loss:  0.6400, Accuracy: 0.8125\n",
      "Batch number : 208, Training: Loss:  0.6529, Accuracy: 0.8281\n",
      "Batch number : 209, Training: Loss:  0.4961, Accuracy: 0.8594\n",
      "Batch number : 210, Training: Loss:  0.4563, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.5656, Accuracy: 0.8281\n",
      "Batch number : 212, Training: Loss:  0.5257, Accuracy: 0.8438\n",
      "Batch number : 213, Training: Loss:  0.6407, Accuracy: 0.8125\n",
      "Batch number : 214, Training: Loss:  0.2848, Accuracy: 0.9219\n",
      "Batch number : 215, Training: Loss:  0.4402, Accuracy: 0.8750\n",
      "Batch number : 216, Training: Loss:  0.4906, Accuracy: 0.8906\n",
      "Batch number : 217, Training: Loss:  0.4526, Accuracy: 0.9062\n",
      "Batch number : 218, Training: Loss:  0.6248, Accuracy: 0.8438\n",
      "Batch number : 219, Training: Loss:  0.4200, Accuracy: 0.8906\n",
      "Batch number : 220, Training: Loss:  0.5771, Accuracy: 0.8438\n",
      "Batch number : 221, Training: Loss:  0.4362, Accuracy: 0.8750\n",
      "Batch number : 222, Training: Loss:  0.5563, Accuracy: 0.8125\n",
      "Batch number : 223, Training: Loss:  0.4292, Accuracy: 0.9062\n",
      "Batch number : 224, Training: Loss:  0.4753, Accuracy: 0.8594\n",
      "Batch number : 225, Training: Loss:  0.6384, Accuracy: 0.8125\n",
      "Batch number : 226, Training: Loss:  0.5558, Accuracy: 0.8281\n",
      "Batch number : 227, Training: Loss:  0.3789, Accuracy: 0.9062\n",
      "Batch number : 228, Training: Loss:  0.5056, Accuracy: 0.8906\n",
      "Batch number : 229, Training: Loss:  0.3486, Accuracy: 0.9062\n",
      "Batch number : 230, Training: Loss:  0.3709, Accuracy: 0.9219\n",
      "Batch number : 231, Training: Loss:  0.5979, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.4222, Accuracy: 0.9219\n",
      "Batch number : 233, Training: Loss:  0.5550, Accuracy: 0.8438\n",
      "Batch number : 234, Training: Loss:  0.6405, Accuracy: 0.8125\n",
      "Batch number : 235, Training: Loss:  0.3976, Accuracy: 0.9062\n",
      "Batch number : 236, Training: Loss:  0.4882, Accuracy: 0.8281\n",
      "Batch number : 237, Training: Loss:  0.4896, Accuracy: 0.8281\n",
      "Batch number : 238, Training: Loss:  0.5389, Accuracy: 0.8438\n",
      "Batch number : 239, Training: Loss:  0.5413, Accuracy: 0.8594\n",
      "Batch number : 240, Training: Loss:  0.6041, Accuracy: 0.8438\n",
      "Batch number : 241, Training: Loss:  0.5479, Accuracy: 0.8906\n",
      "Batch number : 242, Training: Loss:  0.3833, Accuracy: 0.9062\n",
      "Batch number : 243, Training: Loss:  0.5464, Accuracy: 0.8594\n",
      "Batch number : 244, Training: Loss:  0.7626, Accuracy: 0.7969\n",
      "Batch number : 245, Training: Loss:  0.5797, Accuracy: 0.8594\n",
      "Batch number : 246, Training: Loss:  0.4316, Accuracy: 0.8750\n",
      "Batch number : 247, Training: Loss:  0.6509, Accuracy: 0.7969\n",
      "Batch number : 248, Training: Loss:  0.6078, Accuracy: 0.8438\n",
      "Batch number : 249, Training: Loss:  0.6022, Accuracy: 0.8750\n",
      "Batch number : 250, Training: Loss:  0.5363, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.5030, Accuracy: 0.8281\n",
      "Batch number : 252, Training: Loss:  0.4465, Accuracy: 0.8750\n",
      "Batch number : 253, Training: Loss:  0.4689, Accuracy: 0.8438\n",
      "Batch number : 254, Training: Loss:  0.5281, Accuracy: 0.8281\n",
      "Batch number : 255, Training: Loss:  0.6169, Accuracy: 0.8125\n",
      "Batch number : 256, Training: Loss:  0.5526, Accuracy: 0.8438\n",
      "Batch number : 257, Training: Loss:  0.4879, Accuracy: 0.8438\n",
      "Batch number : 258, Training: Loss:  0.7082, Accuracy: 0.7812\n",
      "Batch number : 259, Training: Loss:  0.3151, Accuracy: 0.9219\n",
      "Batch number : 260, Training: Loss:  0.5174, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.5951, Accuracy: 0.8438\n",
      "Batch number : 262, Training: Loss:  0.6346, Accuracy: 0.8125\n",
      "Batch number : 263, Training: Loss:  0.4140, Accuracy: 0.8438\n",
      "Batch number : 264, Training: Loss:  0.4783, Accuracy: 0.8438\n",
      "Batch number : 265, Training: Loss:  0.4382, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.4866, Accuracy: 0.8750\n",
      "Batch number : 267, Training: Loss:  0.5487, Accuracy: 0.8125\n",
      "Batch number : 268, Training: Loss:  0.5998, Accuracy: 0.7969\n",
      "Batch number : 269, Training: Loss:  0.8202, Accuracy: 0.8125\n",
      "Batch number : 270, Training: Loss:  0.6035, Accuracy: 0.8125\n",
      "Batch number : 271, Training: Loss:  0.4894, Accuracy: 0.8750\n",
      "Batch number : 272, Training: Loss:  0.4120, Accuracy: 0.8594\n",
      "Batch number : 273, Training: Loss:  0.5964, Accuracy: 0.8438\n",
      "Batch number : 274, Training: Loss:  0.5254, Accuracy: 0.8906\n",
      "Batch number : 275, Training: Loss:  0.5860, Accuracy: 0.8438\n",
      "Batch number : 276, Training: Loss:  0.5514, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.6209, Accuracy: 0.7969\n",
      "Batch number : 278, Training: Loss:  0.3228, Accuracy: 0.8906\n",
      "Batch number : 279, Training: Loss:  0.4459, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.4146, Accuracy: 0.8750\n",
      "Batch number : 281, Training: Loss:  0.6158, Accuracy: 0.8125\n",
      "Batch number : 282, Training: Loss:  0.5648, Accuracy: 0.8125\n",
      "Batch number : 283, Training: Loss:  0.5238, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.2804, Accuracy: 0.9531\n",
      "Batch number : 285, Training: Loss:  0.7923, Accuracy: 0.7812\n",
      "Batch number : 286, Training: Loss:  0.5961, Accuracy: 0.8281\n",
      "Batch number : 287, Training: Loss:  0.4690, Accuracy: 0.8281\n",
      "Batch number : 288, Training: Loss:  0.5437, Accuracy: 0.8281\n",
      "Batch number : 289, Training: Loss:  0.7004, Accuracy: 0.7812\n",
      "Batch number : 290, Training: Loss:  0.7166, Accuracy: 0.7344\n",
      "Batch number : 291, Training: Loss:  0.6123, Accuracy: 0.8438\n",
      "Batch number : 292, Training: Loss:  0.5858, Accuracy: 0.8281\n",
      "Batch number : 293, Training: Loss:  0.5419, Accuracy: 0.8281\n",
      "Batch number : 294, Training: Loss:  0.5349, Accuracy: 0.8594\n",
      "Batch number : 295, Training: Loss:  0.3949, Accuracy: 0.8906\n",
      "Batch number : 296, Training: Loss:  0.5264, Accuracy: 0.8281\n",
      "Batch number : 297, Training: Loss:  0.5050, Accuracy: 0.8750\n",
      "Batch number : 298, Training: Loss:  0.4417, Accuracy: 0.8438\n",
      "Batch number : 299, Training: Loss:  0.5642, Accuracy: 0.8438\n",
      "Batch number : 300, Training: Loss:  0.7314, Accuracy: 0.7812\n",
      "Batch number : 301, Training: Loss:  0.3385, Accuracy: 0.9062\n",
      "Batch number : 302, Training: Loss:  0.5482, Accuracy: 0.8594\n",
      "Batch number : 303, Training: Loss:  0.4837, Accuracy: 0.8594\n",
      "Batch number : 304, Training: Loss:  0.5908, Accuracy: 0.8438\n",
      "Batch number : 305, Training: Loss:  0.4171, Accuracy: 0.8906\n",
      "Batch number : 306, Training: Loss:  0.4937, Accuracy: 0.8594\n",
      "Batch number : 307, Training: Loss:  0.6785, Accuracy: 0.7500\n",
      "Batch number : 308, Training: Loss:  0.4650, Accuracy: 0.8750\n",
      "Batch number : 309, Training: Loss:  0.5303, Accuracy: 0.8750\n",
      "Batch number : 310, Training: Loss:  0.5619, Accuracy: 0.8594\n",
      "Batch number : 311, Training: Loss:  0.4931, Accuracy: 0.8594\n",
      "Batch number : 312, Training: Loss:  0.7241, Accuracy: 0.7969\n",
      "Batch number : 313, Training: Loss:  0.5583, Accuracy: 0.8125\n",
      "Batch number : 314, Training: Loss:  0.5120, Accuracy: 0.8438\n",
      "Batch number : 315, Training: Loss:  0.3835, Accuracy: 0.8750\n",
      "Batch number : 316, Training: Loss:  0.4699, Accuracy: 0.8906\n",
      "Batch number : 317, Training: Loss:  0.4166, Accuracy: 0.8906\n",
      "Batch number : 318, Training: Loss:  0.5146, Accuracy: 0.8906\n",
      "Batch number : 319, Training: Loss:  0.4762, Accuracy: 0.8906\n",
      "Batch number : 320, Training: Loss:  0.3003, Accuracy: 0.9062\n",
      "Batch number : 321, Training: Loss:  0.7143, Accuracy: 0.7656\n",
      "Batch number : 322, Training: Loss:  0.4679, Accuracy: 0.8750\n",
      "Batch number : 323, Training: Loss:  0.4475, Accuracy: 0.8438\n",
      "Batch number : 324, Training: Loss:  0.5492, Accuracy: 0.8438\n",
      "Batch number : 325, Training: Loss:  0.5535, Accuracy: 0.8750\n",
      "Batch number : 326, Training: Loss:  0.5830, Accuracy: 0.8125\n",
      "Batch number : 327, Training: Loss:  0.3607, Accuracy: 0.9062\n",
      "Batch number : 328, Training: Loss:  0.5003, Accuracy: 0.8594\n",
      "Batch number : 329, Training: Loss:  0.6923, Accuracy: 0.7969\n",
      "Batch number : 330, Training: Loss:  0.5424, Accuracy: 0.8750\n",
      "Batch number : 331, Training: Loss:  0.6874, Accuracy: 0.7969\n",
      "Batch number : 332, Training: Loss:  0.6568, Accuracy: 0.8438\n",
      "Batch number : 333, Training: Loss:  0.5060, Accuracy: 0.8750\n",
      "Batch number : 334, Training: Loss:  0.5056, Accuracy: 0.8281\n",
      "Batch number : 335, Training: Loss:  0.5273, Accuracy: 0.8594\n",
      "Batch number : 336, Training: Loss:  0.4326, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.8412, Accuracy: 0.7969\n",
      "Batch number : 338, Training: Loss:  0.3822, Accuracy: 0.9219\n",
      "Batch number : 339, Training: Loss:  0.6524, Accuracy: 0.7812\n",
      "Batch number : 340, Training: Loss:  0.4489, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.4392, Accuracy: 0.9062\n",
      "Batch number : 342, Training: Loss:  0.6112, Accuracy: 0.8281\n",
      "Batch number : 343, Training: Loss:  0.4247, Accuracy: 0.8906\n",
      "Batch number : 344, Training: Loss:  0.5515, Accuracy: 0.8750\n",
      "Batch number : 345, Training: Loss:  0.6109, Accuracy: 0.8281\n",
      "Batch number : 346, Training: Loss:  0.5986, Accuracy: 0.8281\n",
      "Batch number : 347, Training: Loss:  0.5053, Accuracy: 0.8750\n",
      "Batch number : 348, Training: Loss:  0.4053, Accuracy: 0.8906\n",
      "Batch number : 349, Training: Loss:  0.4543, Accuracy: 0.8750\n",
      "Batch number : 350, Training: Loss:  0.5741, Accuracy: 0.8281\n",
      "Batch number : 351, Training: Loss:  0.5219, Accuracy: 0.8281\n",
      "Batch number : 352, Training: Loss:  0.6044, Accuracy: 0.8438\n",
      "Batch number : 353, Training: Loss:  0.3990, Accuracy: 0.8906\n",
      "Batch number : 354, Training: Loss:  0.3929, Accuracy: 0.8750\n",
      "Batch number : 355, Training: Loss:  0.4617, Accuracy: 0.8906\n",
      "Batch number : 356, Training: Loss:  0.3859, Accuracy: 0.8594\n",
      "Batch number : 357, Training: Loss:  0.7173, Accuracy: 0.8125\n",
      "Batch number : 358, Training: Loss:  0.4905, Accuracy: 0.8750\n",
      "Batch number : 359, Training: Loss:  0.6898, Accuracy: 0.8438\n",
      "Batch number : 360, Training: Loss:  0.4486, Accuracy: 0.8906\n",
      "Batch number : 361, Training: Loss:  0.4816, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.5217, Accuracy: 0.8438\n",
      "Batch number : 363, Training: Loss:  0.3361, Accuracy: 0.9062\n",
      "Batch number : 364, Training: Loss:  0.5676, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.7084, Accuracy: 0.7500\n",
      "Batch number : 366, Training: Loss:  0.3771, Accuracy: 0.8906\n",
      "Batch number : 367, Training: Loss:  0.2741, Accuracy: 0.9531\n",
      "Batch number : 368, Training: Loss:  0.7227, Accuracy: 0.8125\n",
      "Batch number : 369, Training: Loss:  0.5340, Accuracy: 0.8594\n",
      "Batch number : 370, Training: Loss:  0.3285, Accuracy: 0.9219\n",
      "Batch number : 371, Training: Loss:  0.4920, Accuracy: 0.8438\n",
      "Batch number : 372, Training: Loss:  0.6003, Accuracy: 0.8281\n",
      "Batch number : 373, Training: Loss:  0.7322, Accuracy: 0.7656\n",
      "Batch number : 374, Training: Loss:  0.3891, Accuracy: 0.8906\n",
      "Batch number : 375, Training: Loss:  0.4565, Accuracy: 0.8750\n",
      "Batch number : 376, Training: Loss:  0.5760, Accuracy: 0.8594\n",
      "Batch number : 377, Training: Loss:  0.5157, Accuracy: 0.8250\n",
      "Epoch: 2/20\n",
      "Batch number : 000, Training: Loss:  0.6004, Accuracy: 0.7969\n",
      "Batch number : 001, Training: Loss:  0.6035, Accuracy: 0.7969\n",
      "Batch number : 002, Training: Loss:  0.4634, Accuracy: 0.8594\n",
      "Batch number : 003, Training: Loss:  0.4495, Accuracy: 0.9219\n",
      "Batch number : 004, Training: Loss:  0.4225, Accuracy: 0.9062\n",
      "Batch number : 005, Training: Loss:  0.5730, Accuracy: 0.8438\n",
      "Batch number : 006, Training: Loss:  0.3915, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.5099, Accuracy: 0.8438\n",
      "Batch number : 008, Training: Loss:  0.4215, Accuracy: 0.8906\n",
      "Batch number : 009, Training: Loss:  0.4109, Accuracy: 0.8750\n",
      "Batch number : 010, Training: Loss:  0.8323, Accuracy: 0.7812\n",
      "Batch number : 011, Training: Loss:  0.4389, Accuracy: 0.8594\n",
      "Batch number : 012, Training: Loss:  0.5015, Accuracy: 0.8281\n",
      "Batch number : 013, Training: Loss:  0.4945, Accuracy: 0.8281\n",
      "Batch number : 014, Training: Loss:  0.6702, Accuracy: 0.7812\n",
      "Batch number : 015, Training: Loss:  0.4080, Accuracy: 0.9062\n",
      "Batch number : 016, Training: Loss:  0.4679, Accuracy: 0.8594\n",
      "Batch number : 017, Training: Loss:  0.3084, Accuracy: 0.9375\n",
      "Batch number : 018, Training: Loss:  0.2879, Accuracy: 0.9375\n",
      "Batch number : 019, Training: Loss:  0.3595, Accuracy: 0.8906\n",
      "Batch number : 020, Training: Loss:  0.4866, Accuracy: 0.8750\n",
      "Batch number : 021, Training: Loss:  0.4286, Accuracy: 0.8750\n",
      "Batch number : 022, Training: Loss:  0.4880, Accuracy: 0.8594\n",
      "Batch number : 023, Training: Loss:  0.6029, Accuracy: 0.8281\n",
      "Batch number : 024, Training: Loss:  0.7803, Accuracy: 0.7969\n",
      "Batch number : 025, Training: Loss:  0.5169, Accuracy: 0.8281\n",
      "Batch number : 026, Training: Loss:  0.3530, Accuracy: 0.8906\n",
      "Batch number : 027, Training: Loss:  0.5865, Accuracy: 0.8125\n",
      "Batch number : 028, Training: Loss:  0.5227, Accuracy: 0.8281\n",
      "Batch number : 029, Training: Loss:  0.4843, Accuracy: 0.8594\n",
      "Batch number : 030, Training: Loss:  0.6676, Accuracy: 0.8125\n",
      "Batch number : 031, Training: Loss:  0.4658, Accuracy: 0.8438\n",
      "Batch number : 032, Training: Loss:  0.4026, Accuracy: 0.9219\n",
      "Batch number : 033, Training: Loss:  0.3380, Accuracy: 0.9062\n",
      "Batch number : 034, Training: Loss:  0.6290, Accuracy: 0.7812\n",
      "Batch number : 035, Training: Loss:  0.6843, Accuracy: 0.8281\n",
      "Batch number : 036, Training: Loss:  0.4686, Accuracy: 0.8594\n",
      "Batch number : 037, Training: Loss:  0.6252, Accuracy: 0.8281\n",
      "Batch number : 038, Training: Loss:  0.3559, Accuracy: 0.9219\n",
      "Batch number : 039, Training: Loss:  0.4503, Accuracy: 0.8906\n",
      "Batch number : 040, Training: Loss:  0.6095, Accuracy: 0.7969\n",
      "Batch number : 041, Training: Loss:  0.4251, Accuracy: 0.8594\n",
      "Batch number : 042, Training: Loss:  0.4430, Accuracy: 0.8594\n",
      "Batch number : 043, Training: Loss:  0.4926, Accuracy: 0.8438\n",
      "Batch number : 044, Training: Loss:  0.3755, Accuracy: 0.9062\n",
      "Batch number : 045, Training: Loss:  0.3379, Accuracy: 0.9062\n",
      "Batch number : 046, Training: Loss:  0.4763, Accuracy: 0.8906\n",
      "Batch number : 047, Training: Loss:  0.4647, Accuracy: 0.8906\n",
      "Batch number : 048, Training: Loss:  0.6613, Accuracy: 0.7812\n",
      "Batch number : 049, Training: Loss:  0.6805, Accuracy: 0.8125\n",
      "Batch number : 050, Training: Loss:  0.4532, Accuracy: 0.8906\n",
      "Batch number : 051, Training: Loss:  0.8206, Accuracy: 0.7969\n",
      "Batch number : 052, Training: Loss:  0.6482, Accuracy: 0.8438\n",
      "Batch number : 053, Training: Loss:  0.5866, Accuracy: 0.8125\n",
      "Batch number : 054, Training: Loss:  0.5830, Accuracy: 0.8594\n",
      "Batch number : 055, Training: Loss:  0.6601, Accuracy: 0.7969\n",
      "Batch number : 056, Training: Loss:  0.5315, Accuracy: 0.8750\n",
      "Batch number : 057, Training: Loss:  0.5140, Accuracy: 0.8594\n",
      "Batch number : 058, Training: Loss:  0.5320, Accuracy: 0.8281\n",
      "Batch number : 059, Training: Loss:  0.4143, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.6600, Accuracy: 0.8281\n",
      "Batch number : 061, Training: Loss:  0.7372, Accuracy: 0.7656\n",
      "Batch number : 062, Training: Loss:  0.5114, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.3075, Accuracy: 0.9062\n",
      "Batch number : 064, Training: Loss:  0.5145, Accuracy: 0.8594\n",
      "Batch number : 065, Training: Loss:  0.6862, Accuracy: 0.7812\n",
      "Batch number : 066, Training: Loss:  0.5125, Accuracy: 0.8594\n",
      "Batch number : 067, Training: Loss:  0.7697, Accuracy: 0.8281\n",
      "Batch number : 068, Training: Loss:  0.4478, Accuracy: 0.8594\n",
      "Batch number : 069, Training: Loss:  0.6462, Accuracy: 0.7656\n",
      "Batch number : 070, Training: Loss:  0.5055, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.4280, Accuracy: 0.8438\n",
      "Batch number : 072, Training: Loss:  0.4854, Accuracy: 0.9062\n",
      "Batch number : 073, Training: Loss:  0.3641, Accuracy: 0.9062\n",
      "Batch number : 074, Training: Loss:  0.3694, Accuracy: 0.9062\n",
      "Batch number : 075, Training: Loss:  0.4357, Accuracy: 0.8750\n",
      "Batch number : 076, Training: Loss:  0.6978, Accuracy: 0.8281\n",
      "Batch number : 077, Training: Loss:  0.7650, Accuracy: 0.7656\n",
      "Batch number : 078, Training: Loss:  0.4751, Accuracy: 0.8438\n",
      "Batch number : 079, Training: Loss:  0.4099, Accuracy: 0.8750\n",
      "Batch number : 080, Training: Loss:  0.3802, Accuracy: 0.8750\n",
      "Batch number : 081, Training: Loss:  0.5480, Accuracy: 0.8750\n",
      "Batch number : 082, Training: Loss:  0.5125, Accuracy: 0.7969\n",
      "Batch number : 083, Training: Loss:  0.4942, Accuracy: 0.8750\n",
      "Batch number : 084, Training: Loss:  0.4969, Accuracy: 0.8750\n",
      "Batch number : 085, Training: Loss:  0.5305, Accuracy: 0.8906\n",
      "Batch number : 086, Training: Loss:  0.6179, Accuracy: 0.8594\n",
      "Batch number : 087, Training: Loss:  0.5698, Accuracy: 0.8281\n",
      "Batch number : 088, Training: Loss:  0.4992, Accuracy: 0.8594\n",
      "Batch number : 089, Training: Loss:  0.4276, Accuracy: 0.8438\n",
      "Batch number : 090, Training: Loss:  0.3844, Accuracy: 0.8906\n",
      "Batch number : 091, Training: Loss:  0.2863, Accuracy: 0.9375\n",
      "Batch number : 092, Training: Loss:  0.5846, Accuracy: 0.8125\n",
      "Batch number : 093, Training: Loss:  0.5737, Accuracy: 0.8281\n",
      "Batch number : 094, Training: Loss:  0.3785, Accuracy: 0.8750\n",
      "Batch number : 095, Training: Loss:  0.2968, Accuracy: 0.9375\n",
      "Batch number : 096, Training: Loss:  0.7408, Accuracy: 0.7500\n",
      "Batch number : 097, Training: Loss:  0.6361, Accuracy: 0.8125\n",
      "Batch number : 098, Training: Loss:  0.6280, Accuracy: 0.8438\n",
      "Batch number : 099, Training: Loss:  0.6508, Accuracy: 0.8125\n",
      "Batch number : 100, Training: Loss:  0.4889, Accuracy: 0.8750\n",
      "Batch number : 101, Training: Loss:  0.4594, Accuracy: 0.8594\n",
      "Batch number : 102, Training: Loss:  0.7293, Accuracy: 0.7500\n",
      "Batch number : 103, Training: Loss:  0.6138, Accuracy: 0.7812\n",
      "Batch number : 104, Training: Loss:  0.4176, Accuracy: 0.9062\n",
      "Batch number : 105, Training: Loss:  0.5834, Accuracy: 0.8281\n",
      "Batch number : 106, Training: Loss:  0.8700, Accuracy: 0.7344\n",
      "Batch number : 107, Training: Loss:  0.5633, Accuracy: 0.8438\n",
      "Batch number : 108, Training: Loss:  0.7903, Accuracy: 0.7656\n",
      "Batch number : 109, Training: Loss:  0.3874, Accuracy: 0.8906\n",
      "Batch number : 110, Training: Loss:  0.6260, Accuracy: 0.8594\n",
      "Batch number : 111, Training: Loss:  0.6036, Accuracy: 0.7812\n",
      "Batch number : 112, Training: Loss:  0.4529, Accuracy: 0.8750\n",
      "Batch number : 113, Training: Loss:  0.5731, Accuracy: 0.8281\n",
      "Batch number : 114, Training: Loss:  0.5682, Accuracy: 0.8125\n",
      "Batch number : 115, Training: Loss:  0.3832, Accuracy: 0.8750\n",
      "Batch number : 116, Training: Loss:  0.4591, Accuracy: 0.8594\n",
      "Batch number : 117, Training: Loss:  0.4374, Accuracy: 0.8594\n",
      "Batch number : 118, Training: Loss:  0.3882, Accuracy: 0.9062\n",
      "Batch number : 119, Training: Loss:  0.4601, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.7950, Accuracy: 0.7656\n",
      "Batch number : 121, Training: Loss:  0.4848, Accuracy: 0.8438\n",
      "Batch number : 122, Training: Loss:  0.4611, Accuracy: 0.8906\n",
      "Batch number : 123, Training: Loss:  0.5402, Accuracy: 0.7969\n",
      "Batch number : 124, Training: Loss:  0.5368, Accuracy: 0.8594\n",
      "Batch number : 125, Training: Loss:  0.3171, Accuracy: 0.9375\n",
      "Batch number : 126, Training: Loss:  0.4793, Accuracy: 0.8438\n",
      "Batch number : 127, Training: Loss:  0.3450, Accuracy: 0.9062\n",
      "Batch number : 128, Training: Loss:  0.3912, Accuracy: 0.8438\n",
      "Batch number : 129, Training: Loss:  0.5114, Accuracy: 0.8750\n",
      "Batch number : 130, Training: Loss:  0.4089, Accuracy: 0.8750\n",
      "Batch number : 131, Training: Loss:  0.5161, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.6478, Accuracy: 0.8438\n",
      "Batch number : 133, Training: Loss:  0.4657, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.3962, Accuracy: 0.8906\n",
      "Batch number : 135, Training: Loss:  0.5057, Accuracy: 0.8750\n",
      "Batch number : 136, Training: Loss:  0.5117, Accuracy: 0.8594\n",
      "Batch number : 137, Training: Loss:  0.3948, Accuracy: 0.9219\n",
      "Batch number : 138, Training: Loss:  0.5079, Accuracy: 0.8281\n",
      "Batch number : 139, Training: Loss:  0.4672, Accuracy: 0.8750\n",
      "Batch number : 140, Training: Loss:  0.5834, Accuracy: 0.8281\n",
      "Batch number : 141, Training: Loss:  0.4719, Accuracy: 0.8438\n",
      "Batch number : 142, Training: Loss:  0.5443, Accuracy: 0.8281\n",
      "Batch number : 143, Training: Loss:  0.3773, Accuracy: 0.9062\n",
      "Batch number : 144, Training: Loss:  0.5623, Accuracy: 0.8281\n",
      "Batch number : 145, Training: Loss:  0.6764, Accuracy: 0.8125\n",
      "Batch number : 146, Training: Loss:  0.5178, Accuracy: 0.8594\n",
      "Batch number : 147, Training: Loss:  0.3890, Accuracy: 0.8750\n",
      "Batch number : 148, Training: Loss:  0.5527, Accuracy: 0.7969\n",
      "Batch number : 149, Training: Loss:  0.5014, Accuracy: 0.8594\n",
      "Batch number : 150, Training: Loss:  0.4314, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.3961, Accuracy: 0.8750\n",
      "Batch number : 152, Training: Loss:  0.6434, Accuracy: 0.7969\n",
      "Batch number : 153, Training: Loss:  0.4534, Accuracy: 0.8750\n",
      "Batch number : 154, Training: Loss:  0.4183, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.4199, Accuracy: 0.8750\n",
      "Batch number : 156, Training: Loss:  0.4182, Accuracy: 0.8438\n",
      "Batch number : 157, Training: Loss:  0.5782, Accuracy: 0.8125\n",
      "Batch number : 158, Training: Loss:  0.4147, Accuracy: 0.8906\n",
      "Batch number : 159, Training: Loss:  0.5781, Accuracy: 0.8438\n",
      "Batch number : 160, Training: Loss:  0.5518, Accuracy: 0.8750\n",
      "Batch number : 161, Training: Loss:  0.5850, Accuracy: 0.8125\n",
      "Batch number : 162, Training: Loss:  0.5791, Accuracy: 0.8281\n",
      "Batch number : 163, Training: Loss:  0.5457, Accuracy: 0.8438\n",
      "Batch number : 164, Training: Loss:  0.6577, Accuracy: 0.8125\n",
      "Batch number : 165, Training: Loss:  0.6163, Accuracy: 0.8125\n",
      "Batch number : 166, Training: Loss:  0.5544, Accuracy: 0.8281\n",
      "Batch number : 167, Training: Loss:  0.4010, Accuracy: 0.9062\n",
      "Batch number : 168, Training: Loss:  0.4519, Accuracy: 0.8594\n",
      "Batch number : 169, Training: Loss:  0.6837, Accuracy: 0.7969\n",
      "Batch number : 170, Training: Loss:  0.4425, Accuracy: 0.8906\n",
      "Batch number : 171, Training: Loss:  0.3897, Accuracy: 0.8750\n",
      "Batch number : 172, Training: Loss:  0.5270, Accuracy: 0.8594\n",
      "Batch number : 173, Training: Loss:  0.3892, Accuracy: 0.8906\n",
      "Batch number : 174, Training: Loss:  0.3165, Accuracy: 0.9062\n",
      "Batch number : 175, Training: Loss:  0.6764, Accuracy: 0.8594\n",
      "Batch number : 176, Training: Loss:  0.4488, Accuracy: 0.8750\n",
      "Batch number : 177, Training: Loss:  0.4600, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.4124, Accuracy: 0.8594\n",
      "Batch number : 179, Training: Loss:  0.5669, Accuracy: 0.8594\n",
      "Batch number : 180, Training: Loss:  0.6352, Accuracy: 0.8281\n",
      "Batch number : 181, Training: Loss:  0.6327, Accuracy: 0.7812\n",
      "Batch number : 182, Training: Loss:  0.5698, Accuracy: 0.8125\n",
      "Batch number : 183, Training: Loss:  0.9567, Accuracy: 0.7188\n",
      "Batch number : 184, Training: Loss:  0.4634, Accuracy: 0.8906\n",
      "Batch number : 185, Training: Loss:  0.4542, Accuracy: 0.9062\n",
      "Batch number : 186, Training: Loss:  0.5739, Accuracy: 0.8594\n",
      "Batch number : 187, Training: Loss:  0.4990, Accuracy: 0.8906\n",
      "Batch number : 188, Training: Loss:  0.8041, Accuracy: 0.7812\n",
      "Batch number : 189, Training: Loss:  0.5776, Accuracy: 0.8125\n",
      "Batch number : 190, Training: Loss:  0.7515, Accuracy: 0.7812\n",
      "Batch number : 191, Training: Loss:  0.4577, Accuracy: 0.8438\n",
      "Batch number : 192, Training: Loss:  0.5309, Accuracy: 0.8438\n",
      "Batch number : 193, Training: Loss:  0.3170, Accuracy: 0.9219\n",
      "Batch number : 194, Training: Loss:  0.3823, Accuracy: 0.8750\n",
      "Batch number : 195, Training: Loss:  0.5104, Accuracy: 0.8750\n",
      "Batch number : 196, Training: Loss:  0.5123, Accuracy: 0.8281\n",
      "Batch number : 197, Training: Loss:  0.4973, Accuracy: 0.9219\n",
      "Batch number : 198, Training: Loss:  0.3247, Accuracy: 0.9219\n",
      "Batch number : 199, Training: Loss:  0.6239, Accuracy: 0.8125\n",
      "Batch number : 200, Training: Loss:  0.5054, Accuracy: 0.8438\n",
      "Batch number : 201, Training: Loss:  0.6382, Accuracy: 0.8281\n",
      "Batch number : 202, Training: Loss:  0.4855, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.4009, Accuracy: 0.8750\n",
      "Batch number : 204, Training: Loss:  0.4223, Accuracy: 0.8906\n",
      "Batch number : 205, Training: Loss:  0.6213, Accuracy: 0.8281\n",
      "Batch number : 206, Training: Loss:  0.3805, Accuracy: 0.9062\n",
      "Batch number : 207, Training: Loss:  0.3974, Accuracy: 0.8906\n",
      "Batch number : 208, Training: Loss:  0.3957, Accuracy: 0.9062\n",
      "Batch number : 209, Training: Loss:  0.5436, Accuracy: 0.8594\n",
      "Batch number : 210, Training: Loss:  0.6601, Accuracy: 0.8281\n",
      "Batch number : 211, Training: Loss:  0.6003, Accuracy: 0.8281\n",
      "Batch number : 212, Training: Loss:  0.7415, Accuracy: 0.7969\n",
      "Batch number : 213, Training: Loss:  0.4061, Accuracy: 0.8750\n",
      "Batch number : 214, Training: Loss:  0.4682, Accuracy: 0.8906\n",
      "Batch number : 215, Training: Loss:  0.5390, Accuracy: 0.8438\n",
      "Batch number : 216, Training: Loss:  0.7114, Accuracy: 0.8125\n",
      "Batch number : 217, Training: Loss:  0.5987, Accuracy: 0.7969\n",
      "Batch number : 218, Training: Loss:  0.6234, Accuracy: 0.8125\n",
      "Batch number : 219, Training: Loss:  0.4558, Accuracy: 0.8750\n",
      "Batch number : 220, Training: Loss:  0.5531, Accuracy: 0.8281\n",
      "Batch number : 221, Training: Loss:  0.4619, Accuracy: 0.8594\n",
      "Batch number : 222, Training: Loss:  0.6186, Accuracy: 0.8125\n",
      "Batch number : 223, Training: Loss:  0.3552, Accuracy: 0.9062\n",
      "Batch number : 224, Training: Loss:  0.4413, Accuracy: 0.8906\n",
      "Batch number : 225, Training: Loss:  0.5465, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.4696, Accuracy: 0.8750\n",
      "Batch number : 227, Training: Loss:  0.3048, Accuracy: 0.9062\n",
      "Batch number : 228, Training: Loss:  0.4806, Accuracy: 0.8594\n",
      "Batch number : 229, Training: Loss:  0.4343, Accuracy: 0.8750\n",
      "Batch number : 230, Training: Loss:  0.4730, Accuracy: 0.8594\n",
      "Batch number : 231, Training: Loss:  0.3708, Accuracy: 0.8906\n",
      "Batch number : 232, Training: Loss:  0.4914, Accuracy: 0.8438\n",
      "Batch number : 233, Training: Loss:  0.7528, Accuracy: 0.7969\n",
      "Batch number : 234, Training: Loss:  0.4699, Accuracy: 0.8750\n",
      "Batch number : 235, Training: Loss:  0.5124, Accuracy: 0.8750\n",
      "Batch number : 236, Training: Loss:  0.3921, Accuracy: 0.9062\n",
      "Batch number : 237, Training: Loss:  0.4000, Accuracy: 0.9219\n",
      "Batch number : 238, Training: Loss:  0.5902, Accuracy: 0.8281\n",
      "Batch number : 239, Training: Loss:  0.5097, Accuracy: 0.8438\n",
      "Batch number : 240, Training: Loss:  0.4119, Accuracy: 0.8906\n",
      "Batch number : 241, Training: Loss:  0.4849, Accuracy: 0.8750\n",
      "Batch number : 242, Training: Loss:  0.8403, Accuracy: 0.7812\n",
      "Batch number : 243, Training: Loss:  0.4073, Accuracy: 0.8750\n",
      "Batch number : 244, Training: Loss:  0.4504, Accuracy: 0.8594\n",
      "Batch number : 245, Training: Loss:  0.2978, Accuracy: 0.9219\n",
      "Batch number : 246, Training: Loss:  0.4274, Accuracy: 0.8750\n",
      "Batch number : 247, Training: Loss:  0.5199, Accuracy: 0.8594\n",
      "Batch number : 248, Training: Loss:  0.6995, Accuracy: 0.8125\n",
      "Batch number : 249, Training: Loss:  0.3785, Accuracy: 0.9062\n",
      "Batch number : 250, Training: Loss:  0.4746, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.7682, Accuracy: 0.7344\n",
      "Batch number : 252, Training: Loss:  0.7063, Accuracy: 0.8281\n",
      "Batch number : 253, Training: Loss:  0.5066, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.3515, Accuracy: 0.9062\n",
      "Batch number : 255, Training: Loss:  0.4855, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.8357, Accuracy: 0.7188\n",
      "Batch number : 257, Training: Loss:  0.4988, Accuracy: 0.8594\n",
      "Batch number : 258, Training: Loss:  0.4538, Accuracy: 0.8750\n",
      "Batch number : 259, Training: Loss:  0.3260, Accuracy: 0.9219\n",
      "Batch number : 260, Training: Loss:  0.6015, Accuracy: 0.8125\n",
      "Batch number : 261, Training: Loss:  0.4849, Accuracy: 0.8750\n",
      "Batch number : 262, Training: Loss:  0.3889, Accuracy: 0.8750\n",
      "Batch number : 263, Training: Loss:  0.6445, Accuracy: 0.7969\n",
      "Batch number : 264, Training: Loss:  0.5930, Accuracy: 0.8438\n",
      "Batch number : 265, Training: Loss:  0.5726, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.4653, Accuracy: 0.8594\n",
      "Batch number : 267, Training: Loss:  0.4661, Accuracy: 0.8594\n",
      "Batch number : 268, Training: Loss:  0.4102, Accuracy: 0.8438\n",
      "Batch number : 269, Training: Loss:  0.5004, Accuracy: 0.8750\n",
      "Batch number : 270, Training: Loss:  0.4726, Accuracy: 0.8438\n",
      "Batch number : 271, Training: Loss:  0.4478, Accuracy: 0.8750\n",
      "Batch number : 272, Training: Loss:  0.3933, Accuracy: 0.8906\n",
      "Batch number : 273, Training: Loss:  0.5384, Accuracy: 0.8750\n",
      "Batch number : 274, Training: Loss:  0.4966, Accuracy: 0.8750\n",
      "Batch number : 275, Training: Loss:  0.7168, Accuracy: 0.7656\n",
      "Batch number : 276, Training: Loss:  0.5050, Accuracy: 0.8594\n",
      "Batch number : 277, Training: Loss:  0.5655, Accuracy: 0.8438\n",
      "Batch number : 278, Training: Loss:  0.4378, Accuracy: 0.9219\n",
      "Batch number : 279, Training: Loss:  0.6115, Accuracy: 0.8750\n",
      "Batch number : 280, Training: Loss:  0.4327, Accuracy: 0.8750\n",
      "Batch number : 281, Training: Loss:  0.4680, Accuracy: 0.8281\n",
      "Batch number : 282, Training: Loss:  0.5389, Accuracy: 0.8125\n",
      "Batch number : 283, Training: Loss:  0.3074, Accuracy: 0.9219\n",
      "Batch number : 284, Training: Loss:  0.4326, Accuracy: 0.8438\n",
      "Batch number : 285, Training: Loss:  0.2053, Accuracy: 0.9688\n",
      "Batch number : 286, Training: Loss:  0.3987, Accuracy: 0.8750\n",
      "Batch number : 287, Training: Loss:  0.3555, Accuracy: 0.8906\n",
      "Batch number : 288, Training: Loss:  0.6031, Accuracy: 0.8594\n",
      "Batch number : 289, Training: Loss:  0.3213, Accuracy: 0.8906\n",
      "Batch number : 290, Training: Loss:  0.2859, Accuracy: 0.9062\n",
      "Batch number : 291, Training: Loss:  0.4427, Accuracy: 0.9062\n",
      "Batch number : 292, Training: Loss:  0.5213, Accuracy: 0.8281\n",
      "Batch number : 293, Training: Loss:  0.4374, Accuracy: 0.8906\n",
      "Batch number : 294, Training: Loss:  0.3377, Accuracy: 0.8906\n",
      "Batch number : 295, Training: Loss:  0.5155, Accuracy: 0.8438\n",
      "Batch number : 296, Training: Loss:  0.5033, Accuracy: 0.8438\n",
      "Batch number : 297, Training: Loss:  0.5723, Accuracy: 0.8281\n",
      "Batch number : 298, Training: Loss:  0.5186, Accuracy: 0.8750\n",
      "Batch number : 299, Training: Loss:  0.6333, Accuracy: 0.7812\n",
      "Batch number : 300, Training: Loss:  0.4899, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.6403, Accuracy: 0.8125\n",
      "Batch number : 302, Training: Loss:  0.4894, Accuracy: 0.8438\n",
      "Batch number : 303, Training: Loss:  0.3315, Accuracy: 0.9062\n",
      "Batch number : 304, Training: Loss:  0.4334, Accuracy: 0.8594\n",
      "Batch number : 305, Training: Loss:  0.4080, Accuracy: 0.9062\n",
      "Batch number : 306, Training: Loss:  0.4965, Accuracy: 0.8594\n",
      "Batch number : 307, Training: Loss:  0.3646, Accuracy: 0.8906\n",
      "Batch number : 308, Training: Loss:  0.3915, Accuracy: 0.9062\n",
      "Batch number : 309, Training: Loss:  0.4254, Accuracy: 0.8906\n",
      "Batch number : 310, Training: Loss:  0.4809, Accuracy: 0.8594\n",
      "Batch number : 311, Training: Loss:  0.5992, Accuracy: 0.8594\n",
      "Batch number : 312, Training: Loss:  0.3641, Accuracy: 0.9062\n",
      "Batch number : 313, Training: Loss:  0.7523, Accuracy: 0.7812\n",
      "Batch number : 314, Training: Loss:  0.4288, Accuracy: 0.8906\n",
      "Batch number : 315, Training: Loss:  0.5399, Accuracy: 0.8594\n",
      "Batch number : 316, Training: Loss:  0.4934, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.3776, Accuracy: 0.9219\n",
      "Batch number : 318, Training: Loss:  0.6026, Accuracy: 0.8125\n",
      "Batch number : 319, Training: Loss:  0.4555, Accuracy: 0.8750\n",
      "Batch number : 320, Training: Loss:  0.4813, Accuracy: 0.8906\n",
      "Batch number : 321, Training: Loss:  0.4376, Accuracy: 0.8594\n",
      "Batch number : 322, Training: Loss:  0.3860, Accuracy: 0.8906\n",
      "Batch number : 323, Training: Loss:  0.6025, Accuracy: 0.8594\n",
      "Batch number : 324, Training: Loss:  0.7320, Accuracy: 0.7969\n",
      "Batch number : 325, Training: Loss:  0.4911, Accuracy: 0.8281\n",
      "Batch number : 326, Training: Loss:  0.4149, Accuracy: 0.8906\n",
      "Batch number : 327, Training: Loss:  0.7133, Accuracy: 0.8438\n",
      "Batch number : 328, Training: Loss:  0.3711, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.5462, Accuracy: 0.8438\n",
      "Batch number : 330, Training: Loss:  0.4821, Accuracy: 0.8438\n",
      "Batch number : 331, Training: Loss:  0.4667, Accuracy: 0.8750\n",
      "Batch number : 332, Training: Loss:  0.3677, Accuracy: 0.9062\n",
      "Batch number : 333, Training: Loss:  0.6139, Accuracy: 0.8281\n",
      "Batch number : 334, Training: Loss:  0.4145, Accuracy: 0.8750\n",
      "Batch number : 335, Training: Loss:  0.3490, Accuracy: 0.9062\n",
      "Batch number : 336, Training: Loss:  0.5358, Accuracy: 0.8594\n",
      "Batch number : 337, Training: Loss:  0.7181, Accuracy: 0.8125\n",
      "Batch number : 338, Training: Loss:  0.4114, Accuracy: 0.8750\n",
      "Batch number : 339, Training: Loss:  0.4713, Accuracy: 0.8750\n",
      "Batch number : 340, Training: Loss:  0.4005, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.4154, Accuracy: 0.9062\n",
      "Batch number : 342, Training: Loss:  0.4104, Accuracy: 0.8906\n",
      "Batch number : 343, Training: Loss:  0.7458, Accuracy: 0.7969\n",
      "Batch number : 344, Training: Loss:  0.3218, Accuracy: 0.9219\n",
      "Batch number : 345, Training: Loss:  0.6676, Accuracy: 0.8125\n",
      "Batch number : 346, Training: Loss:  0.4995, Accuracy: 0.8438\n",
      "Batch number : 347, Training: Loss:  0.4315, Accuracy: 0.8750\n",
      "Batch number : 348, Training: Loss:  0.5280, Accuracy: 0.8438\n",
      "Batch number : 349, Training: Loss:  0.4072, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.9070, Accuracy: 0.7500\n",
      "Batch number : 351, Training: Loss:  0.5772, Accuracy: 0.8281\n",
      "Batch number : 352, Training: Loss:  0.7245, Accuracy: 0.7812\n",
      "Batch number : 353, Training: Loss:  0.5764, Accuracy: 0.8438\n",
      "Batch number : 354, Training: Loss:  0.4484, Accuracy: 0.8750\n",
      "Batch number : 355, Training: Loss:  0.4660, Accuracy: 0.8906\n",
      "Batch number : 356, Training: Loss:  0.3572, Accuracy: 0.9062\n",
      "Batch number : 357, Training: Loss:  0.4679, Accuracy: 0.8750\n",
      "Batch number : 358, Training: Loss:  0.5430, Accuracy: 0.8438\n",
      "Batch number : 359, Training: Loss:  0.4384, Accuracy: 0.8594\n",
      "Batch number : 360, Training: Loss:  0.3889, Accuracy: 0.8750\n",
      "Batch number : 361, Training: Loss:  0.5584, Accuracy: 0.8281\n",
      "Batch number : 362, Training: Loss:  0.6732, Accuracy: 0.8594\n",
      "Batch number : 363, Training: Loss:  0.6603, Accuracy: 0.8438\n",
      "Batch number : 364, Training: Loss:  0.3541, Accuracy: 0.8906\n",
      "Batch number : 365, Training: Loss:  0.3526, Accuracy: 0.8906\n",
      "Batch number : 366, Training: Loss:  0.4499, Accuracy: 0.8750\n",
      "Batch number : 367, Training: Loss:  0.6753, Accuracy: 0.7656\n",
      "Batch number : 368, Training: Loss:  0.4785, Accuracy: 0.8438\n",
      "Batch number : 369, Training: Loss:  0.6029, Accuracy: 0.8281\n",
      "Batch number : 370, Training: Loss:  0.4805, Accuracy: 0.8594\n",
      "Batch number : 371, Training: Loss:  0.3877, Accuracy: 0.9062\n",
      "Batch number : 372, Training: Loss:  0.5195, Accuracy: 0.8594\n",
      "Batch number : 373, Training: Loss:  0.6829, Accuracy: 0.7656\n",
      "Batch number : 374, Training: Loss:  0.3779, Accuracy: 0.9062\n",
      "Batch number : 375, Training: Loss:  0.4276, Accuracy: 0.8750\n",
      "Batch number : 376, Training: Loss:  0.3934, Accuracy: 0.9062\n",
      "Batch number : 377, Training: Loss:  0.4745, Accuracy: 0.9000\n",
      "Epoch: 3/20\n",
      "Batch number : 000, Training: Loss:  0.7235, Accuracy: 0.7656\n",
      "Batch number : 001, Training: Loss:  0.4746, Accuracy: 0.8438\n",
      "Batch number : 002, Training: Loss:  0.5130, Accuracy: 0.8438\n",
      "Batch number : 003, Training: Loss:  0.3971, Accuracy: 0.9219\n",
      "Batch number : 004, Training: Loss:  0.5525, Accuracy: 0.7969\n",
      "Batch number : 005, Training: Loss:  0.5309, Accuracy: 0.8438\n",
      "Batch number : 006, Training: Loss:  0.7264, Accuracy: 0.7969\n",
      "Batch number : 007, Training: Loss:  0.4761, Accuracy: 0.8281\n",
      "Batch number : 008, Training: Loss:  0.5750, Accuracy: 0.8281\n",
      "Batch number : 009, Training: Loss:  0.5225, Accuracy: 0.8438\n",
      "Batch number : 010, Training: Loss:  0.6154, Accuracy: 0.7812\n",
      "Batch number : 011, Training: Loss:  0.4685, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.3968, Accuracy: 0.8750\n",
      "Batch number : 013, Training: Loss:  0.5364, Accuracy: 0.8281\n",
      "Batch number : 014, Training: Loss:  0.5405, Accuracy: 0.8281\n",
      "Batch number : 015, Training: Loss:  0.4025, Accuracy: 0.8594\n",
      "Batch number : 016, Training: Loss:  0.3364, Accuracy: 0.9219\n",
      "Batch number : 017, Training: Loss:  0.6867, Accuracy: 0.7969\n",
      "Batch number : 018, Training: Loss:  0.6387, Accuracy: 0.8438\n",
      "Batch number : 019, Training: Loss:  0.4754, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.3793, Accuracy: 0.9062\n",
      "Batch number : 021, Training: Loss:  0.5597, Accuracy: 0.8281\n",
      "Batch number : 022, Training: Loss:  0.4315, Accuracy: 0.8750\n",
      "Batch number : 023, Training: Loss:  0.3418, Accuracy: 0.9375\n",
      "Batch number : 024, Training: Loss:  0.6106, Accuracy: 0.8281\n",
      "Batch number : 025, Training: Loss:  0.4662, Accuracy: 0.8906\n",
      "Batch number : 026, Training: Loss:  0.3895, Accuracy: 0.8906\n",
      "Batch number : 027, Training: Loss:  0.4017, Accuracy: 0.8750\n",
      "Batch number : 028, Training: Loss:  0.4731, Accuracy: 0.8438\n",
      "Batch number : 029, Training: Loss:  0.5682, Accuracy: 0.8438\n",
      "Batch number : 030, Training: Loss:  0.8081, Accuracy: 0.8125\n",
      "Batch number : 031, Training: Loss:  0.4113, Accuracy: 0.9062\n",
      "Batch number : 032, Training: Loss:  0.2323, Accuracy: 0.9531\n",
      "Batch number : 033, Training: Loss:  0.5202, Accuracy: 0.8750\n",
      "Batch number : 034, Training: Loss:  0.3941, Accuracy: 0.8906\n",
      "Batch number : 035, Training: Loss:  0.5271, Accuracy: 0.8594\n",
      "Batch number : 036, Training: Loss:  0.5283, Accuracy: 0.8125\n",
      "Batch number : 037, Training: Loss:  0.2996, Accuracy: 0.9219\n",
      "Batch number : 038, Training: Loss:  0.6547, Accuracy: 0.7969\n",
      "Batch number : 039, Training: Loss:  0.3590, Accuracy: 0.9062\n",
      "Batch number : 040, Training: Loss:  0.3949, Accuracy: 0.9219\n",
      "Batch number : 041, Training: Loss:  0.5059, Accuracy: 0.8594\n",
      "Batch number : 042, Training: Loss:  0.5635, Accuracy: 0.8438\n",
      "Batch number : 043, Training: Loss:  0.4105, Accuracy: 0.8750\n",
      "Batch number : 044, Training: Loss:  0.4442, Accuracy: 0.8750\n",
      "Batch number : 045, Training: Loss:  0.5372, Accuracy: 0.8438\n",
      "Batch number : 046, Training: Loss:  0.6197, Accuracy: 0.7969\n",
      "Batch number : 047, Training: Loss:  0.5479, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.5138, Accuracy: 0.8594\n",
      "Batch number : 049, Training: Loss:  0.3622, Accuracy: 0.9062\n",
      "Batch number : 050, Training: Loss:  0.4231, Accuracy: 0.9062\n",
      "Batch number : 051, Training: Loss:  0.4209, Accuracy: 0.8906\n",
      "Batch number : 052, Training: Loss:  0.4200, Accuracy: 0.8906\n",
      "Batch number : 053, Training: Loss:  0.4391, Accuracy: 0.8594\n",
      "Batch number : 054, Training: Loss:  0.3032, Accuracy: 0.9219\n",
      "Batch number : 055, Training: Loss:  0.3894, Accuracy: 0.8906\n",
      "Batch number : 056, Training: Loss:  0.7114, Accuracy: 0.7969\n",
      "Batch number : 057, Training: Loss:  0.6206, Accuracy: 0.8281\n",
      "Batch number : 058, Training: Loss:  0.6488, Accuracy: 0.8281\n",
      "Batch number : 059, Training: Loss:  0.3201, Accuracy: 0.9219\n",
      "Batch number : 060, Training: Loss:  0.5947, Accuracy: 0.8438\n",
      "Batch number : 061, Training: Loss:  0.4714, Accuracy: 0.8594\n",
      "Batch number : 062, Training: Loss:  0.3383, Accuracy: 0.9375\n",
      "Batch number : 063, Training: Loss:  0.4912, Accuracy: 0.8906\n",
      "Batch number : 064, Training: Loss:  0.4135, Accuracy: 0.8750\n",
      "Batch number : 065, Training: Loss:  0.6673, Accuracy: 0.8125\n",
      "Batch number : 066, Training: Loss:  0.3961, Accuracy: 0.8750\n",
      "Batch number : 067, Training: Loss:  0.2817, Accuracy: 0.9531\n",
      "Batch number : 068, Training: Loss:  0.6056, Accuracy: 0.8281\n",
      "Batch number : 069, Training: Loss:  0.5834, Accuracy: 0.7969\n",
      "Batch number : 070, Training: Loss:  0.5785, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.4110, Accuracy: 0.8594\n",
      "Batch number : 072, Training: Loss:  0.6508, Accuracy: 0.8281\n",
      "Batch number : 073, Training: Loss:  0.6644, Accuracy: 0.7656\n",
      "Batch number : 074, Training: Loss:  0.4797, Accuracy: 0.8906\n",
      "Batch number : 075, Training: Loss:  0.6168, Accuracy: 0.8438\n",
      "Batch number : 076, Training: Loss:  0.5740, Accuracy: 0.8281\n",
      "Batch number : 077, Training: Loss:  0.4184, Accuracy: 0.9219\n",
      "Batch number : 078, Training: Loss:  0.5362, Accuracy: 0.8125\n",
      "Batch number : 079, Training: Loss:  0.6438, Accuracy: 0.7812\n",
      "Batch number : 080, Training: Loss:  0.4243, Accuracy: 0.8906\n",
      "Batch number : 081, Training: Loss:  0.4285, Accuracy: 0.9062\n",
      "Batch number : 082, Training: Loss:  0.4830, Accuracy: 0.8438\n",
      "Batch number : 083, Training: Loss:  0.3082, Accuracy: 0.9375\n",
      "Batch number : 084, Training: Loss:  0.3837, Accuracy: 0.9062\n",
      "Batch number : 085, Training: Loss:  0.4779, Accuracy: 0.9062\n",
      "Batch number : 086, Training: Loss:  0.6527, Accuracy: 0.8438\n",
      "Batch number : 087, Training: Loss:  0.5514, Accuracy: 0.8594\n",
      "Batch number : 088, Training: Loss:  0.3713, Accuracy: 0.9219\n",
      "Batch number : 089, Training: Loss:  0.7282, Accuracy: 0.8281\n",
      "Batch number : 090, Training: Loss:  0.7322, Accuracy: 0.7969\n",
      "Batch number : 091, Training: Loss:  0.5003, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.6845, Accuracy: 0.8281\n",
      "Batch number : 093, Training: Loss:  0.4532, Accuracy: 0.9219\n",
      "Batch number : 094, Training: Loss:  0.4675, Accuracy: 0.8594\n",
      "Batch number : 095, Training: Loss:  0.3829, Accuracy: 0.9219\n",
      "Batch number : 096, Training: Loss:  0.3762, Accuracy: 0.9062\n",
      "Batch number : 097, Training: Loss:  0.5561, Accuracy: 0.8594\n",
      "Batch number : 098, Training: Loss:  0.6762, Accuracy: 0.7812\n",
      "Batch number : 099, Training: Loss:  0.6714, Accuracy: 0.8125\n",
      "Batch number : 100, Training: Loss:  0.4455, Accuracy: 0.8281\n",
      "Batch number : 101, Training: Loss:  0.7230, Accuracy: 0.8281\n",
      "Batch number : 102, Training: Loss:  0.5527, Accuracy: 0.8438\n",
      "Batch number : 103, Training: Loss:  0.4402, Accuracy: 0.8750\n",
      "Batch number : 104, Training: Loss:  0.4256, Accuracy: 0.8750\n",
      "Batch number : 105, Training: Loss:  0.5100, Accuracy: 0.8594\n",
      "Batch number : 106, Training: Loss:  0.3936, Accuracy: 0.8750\n",
      "Batch number : 107, Training: Loss:  0.3296, Accuracy: 0.9375\n",
      "Batch number : 108, Training: Loss:  0.3026, Accuracy: 0.9219\n",
      "Batch number : 109, Training: Loss:  0.4945, Accuracy: 0.8594\n",
      "Batch number : 110, Training: Loss:  0.6587, Accuracy: 0.7969\n",
      "Batch number : 111, Training: Loss:  0.5197, Accuracy: 0.8594\n",
      "Batch number : 112, Training: Loss:  0.2121, Accuracy: 0.9688\n",
      "Batch number : 113, Training: Loss:  0.5097, Accuracy: 0.8594\n",
      "Batch number : 114, Training: Loss:  0.5226, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.8293, Accuracy: 0.7812\n",
      "Batch number : 116, Training: Loss:  0.4410, Accuracy: 0.8125\n",
      "Batch number : 117, Training: Loss:  0.4029, Accuracy: 0.8750\n",
      "Batch number : 118, Training: Loss:  0.3479, Accuracy: 0.9219\n",
      "Batch number : 119, Training: Loss:  0.4420, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.5126, Accuracy: 0.8438\n",
      "Batch number : 121, Training: Loss:  0.6660, Accuracy: 0.7656\n",
      "Batch number : 122, Training: Loss:  0.6207, Accuracy: 0.7969\n",
      "Batch number : 123, Training: Loss:  0.4967, Accuracy: 0.8906\n",
      "Batch number : 124, Training: Loss:  0.5023, Accuracy: 0.8594\n",
      "Batch number : 125, Training: Loss:  0.6443, Accuracy: 0.8281\n",
      "Batch number : 126, Training: Loss:  0.8047, Accuracy: 0.7500\n",
      "Batch number : 127, Training: Loss:  0.4149, Accuracy: 0.8594\n",
      "Batch number : 128, Training: Loss:  0.5030, Accuracy: 0.8750\n",
      "Batch number : 129, Training: Loss:  0.6512, Accuracy: 0.8438\n",
      "Batch number : 130, Training: Loss:  0.7434, Accuracy: 0.7812\n",
      "Batch number : 131, Training: Loss:  0.4875, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.4503, Accuracy: 0.8906\n",
      "Batch number : 133, Training: Loss:  0.4828, Accuracy: 0.8906\n",
      "Batch number : 134, Training: Loss:  0.5709, Accuracy: 0.7812\n",
      "Batch number : 135, Training: Loss:  0.4744, Accuracy: 0.9062\n",
      "Batch number : 136, Training: Loss:  0.4788, Accuracy: 0.8750\n",
      "Batch number : 137, Training: Loss:  0.4845, Accuracy: 0.8281\n",
      "Batch number : 138, Training: Loss:  0.5120, Accuracy: 0.8750\n",
      "Batch number : 139, Training: Loss:  0.3896, Accuracy: 0.8750\n",
      "Batch number : 140, Training: Loss:  0.4373, Accuracy: 0.8438\n",
      "Batch number : 141, Training: Loss:  0.5118, Accuracy: 0.8438\n",
      "Batch number : 142, Training: Loss:  0.4085, Accuracy: 0.8906\n",
      "Batch number : 143, Training: Loss:  0.5109, Accuracy: 0.8594\n",
      "Batch number : 144, Training: Loss:  0.4026, Accuracy: 0.9062\n",
      "Batch number : 145, Training: Loss:  0.6132, Accuracy: 0.7969\n",
      "Batch number : 146, Training: Loss:  0.5126, Accuracy: 0.8438\n",
      "Batch number : 147, Training: Loss:  0.6698, Accuracy: 0.8125\n",
      "Batch number : 148, Training: Loss:  0.6833, Accuracy: 0.7656\n",
      "Batch number : 149, Training: Loss:  0.4585, Accuracy: 0.8594\n",
      "Batch number : 150, Training: Loss:  0.4053, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.4725, Accuracy: 0.8594\n",
      "Batch number : 152, Training: Loss:  0.3959, Accuracy: 0.8750\n",
      "Batch number : 153, Training: Loss:  0.5764, Accuracy: 0.8594\n",
      "Batch number : 154, Training: Loss:  0.3117, Accuracy: 0.9219\n",
      "Batch number : 155, Training: Loss:  0.4921, Accuracy: 0.8906\n",
      "Batch number : 156, Training: Loss:  0.3933, Accuracy: 0.8594\n",
      "Batch number : 157, Training: Loss:  0.4657, Accuracy: 0.8750\n",
      "Batch number : 158, Training: Loss:  0.2806, Accuracy: 0.9219\n",
      "Batch number : 159, Training: Loss:  0.4309, Accuracy: 0.8750\n",
      "Batch number : 160, Training: Loss:  0.5467, Accuracy: 0.8594\n",
      "Batch number : 161, Training: Loss:  0.3377, Accuracy: 0.9375\n",
      "Batch number : 162, Training: Loss:  0.5857, Accuracy: 0.8594\n",
      "Batch number : 163, Training: Loss:  0.3950, Accuracy: 0.8906\n",
      "Batch number : 164, Training: Loss:  0.5044, Accuracy: 0.8438\n",
      "Batch number : 165, Training: Loss:  0.4547, Accuracy: 0.8438\n",
      "Batch number : 166, Training: Loss:  0.4207, Accuracy: 0.8750\n",
      "Batch number : 167, Training: Loss:  0.3909, Accuracy: 0.8906\n",
      "Batch number : 168, Training: Loss:  0.5574, Accuracy: 0.8594\n",
      "Batch number : 169, Training: Loss:  0.5742, Accuracy: 0.8438\n",
      "Batch number : 170, Training: Loss:  0.5642, Accuracy: 0.8438\n",
      "Batch number : 171, Training: Loss:  0.3326, Accuracy: 0.9062\n",
      "Batch number : 172, Training: Loss:  0.4380, Accuracy: 0.8906\n",
      "Batch number : 173, Training: Loss:  0.4918, Accuracy: 0.8281\n",
      "Batch number : 174, Training: Loss:  0.5671, Accuracy: 0.8125\n",
      "Batch number : 175, Training: Loss:  0.4512, Accuracy: 0.8594\n",
      "Batch number : 176, Training: Loss:  0.4896, Accuracy: 0.8750\n",
      "Batch number : 177, Training: Loss:  0.4232, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.5493, Accuracy: 0.8281\n",
      "Batch number : 179, Training: Loss:  0.5436, Accuracy: 0.8438\n",
      "Batch number : 180, Training: Loss:  0.5818, Accuracy: 0.8281\n",
      "Batch number : 181, Training: Loss:  0.5089, Accuracy: 0.8594\n",
      "Batch number : 182, Training: Loss:  0.2927, Accuracy: 0.9219\n",
      "Batch number : 183, Training: Loss:  0.5887, Accuracy: 0.8438\n",
      "Batch number : 184, Training: Loss:  0.6172, Accuracy: 0.7969\n",
      "Batch number : 185, Training: Loss:  0.4869, Accuracy: 0.8438\n",
      "Batch number : 186, Training: Loss:  0.4615, Accuracy: 0.8594\n",
      "Batch number : 187, Training: Loss:  0.7160, Accuracy: 0.7969\n",
      "Batch number : 188, Training: Loss:  0.4427, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.6321, Accuracy: 0.7969\n",
      "Batch number : 190, Training: Loss:  0.3850, Accuracy: 0.8906\n",
      "Batch number : 191, Training: Loss:  0.6558, Accuracy: 0.8125\n",
      "Batch number : 192, Training: Loss:  0.4291, Accuracy: 0.9062\n",
      "Batch number : 193, Training: Loss:  0.4726, Accuracy: 0.8438\n",
      "Batch number : 194, Training: Loss:  0.5006, Accuracy: 0.8281\n",
      "Batch number : 195, Training: Loss:  0.4898, Accuracy: 0.8594\n",
      "Batch number : 196, Training: Loss:  0.5104, Accuracy: 0.8438\n",
      "Batch number : 197, Training: Loss:  0.4274, Accuracy: 0.8750\n",
      "Batch number : 198, Training: Loss:  0.3740, Accuracy: 0.8906\n",
      "Batch number : 199, Training: Loss:  0.3534, Accuracy: 0.9219\n",
      "Batch number : 200, Training: Loss:  0.6737, Accuracy: 0.7812\n",
      "Batch number : 201, Training: Loss:  0.4404, Accuracy: 0.8594\n",
      "Batch number : 202, Training: Loss:  0.4970, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.5215, Accuracy: 0.8438\n",
      "Batch number : 204, Training: Loss:  0.4140, Accuracy: 0.8906\n",
      "Batch number : 205, Training: Loss:  0.4650, Accuracy: 0.8438\n",
      "Batch number : 206, Training: Loss:  0.4551, Accuracy: 0.8594\n",
      "Batch number : 207, Training: Loss:  0.5432, Accuracy: 0.8125\n",
      "Batch number : 208, Training: Loss:  0.3835, Accuracy: 0.8906\n",
      "Batch number : 209, Training: Loss:  0.4990, Accuracy: 0.8438\n",
      "Batch number : 210, Training: Loss:  0.4415, Accuracy: 0.8750\n",
      "Batch number : 211, Training: Loss:  0.5630, Accuracy: 0.8438\n",
      "Batch number : 212, Training: Loss:  0.6136, Accuracy: 0.7969\n",
      "Batch number : 213, Training: Loss:  0.5625, Accuracy: 0.8281\n",
      "Batch number : 214, Training: Loss:  0.5142, Accuracy: 0.8594\n",
      "Batch number : 215, Training: Loss:  0.4313, Accuracy: 0.8750\n",
      "Batch number : 216, Training: Loss:  0.5942, Accuracy: 0.8281\n",
      "Batch number : 217, Training: Loss:  0.4179, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.4956, Accuracy: 0.8281\n",
      "Batch number : 219, Training: Loss:  0.4174, Accuracy: 0.8594\n",
      "Batch number : 220, Training: Loss:  0.4720, Accuracy: 0.8594\n",
      "Batch number : 221, Training: Loss:  0.6279, Accuracy: 0.7812\n",
      "Batch number : 222, Training: Loss:  0.5389, Accuracy: 0.8438\n",
      "Batch number : 223, Training: Loss:  0.6331, Accuracy: 0.8281\n",
      "Batch number : 224, Training: Loss:  0.4782, Accuracy: 0.8750\n",
      "Batch number : 225, Training: Loss:  0.6443, Accuracy: 0.7812\n",
      "Batch number : 226, Training: Loss:  0.4457, Accuracy: 0.8906\n",
      "Batch number : 227, Training: Loss:  0.4617, Accuracy: 0.8750\n",
      "Batch number : 228, Training: Loss:  0.5138, Accuracy: 0.8281\n",
      "Batch number : 229, Training: Loss:  0.4425, Accuracy: 0.8750\n",
      "Batch number : 230, Training: Loss:  0.2214, Accuracy: 0.9844\n",
      "Batch number : 231, Training: Loss:  0.5058, Accuracy: 0.8750\n",
      "Batch number : 232, Training: Loss:  0.5721, Accuracy: 0.8438\n",
      "Batch number : 233, Training: Loss:  0.5288, Accuracy: 0.8438\n",
      "Batch number : 234, Training: Loss:  0.5307, Accuracy: 0.8438\n",
      "Batch number : 235, Training: Loss:  0.8357, Accuracy: 0.8125\n",
      "Batch number : 236, Training: Loss:  0.4369, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.3376, Accuracy: 0.9219\n",
      "Batch number : 238, Training: Loss:  0.4463, Accuracy: 0.8906\n",
      "Batch number : 239, Training: Loss:  0.7116, Accuracy: 0.7656\n",
      "Batch number : 240, Training: Loss:  0.4755, Accuracy: 0.9062\n",
      "Batch number : 241, Training: Loss:  0.7088, Accuracy: 0.7812\n",
      "Batch number : 242, Training: Loss:  0.4094, Accuracy: 0.8750\n",
      "Batch number : 243, Training: Loss:  0.5444, Accuracy: 0.8281\n",
      "Batch number : 244, Training: Loss:  0.6312, Accuracy: 0.7812\n",
      "Batch number : 245, Training: Loss:  0.4870, Accuracy: 0.8906\n",
      "Batch number : 246, Training: Loss:  0.6499, Accuracy: 0.8125\n",
      "Batch number : 247, Training: Loss:  0.5524, Accuracy: 0.8281\n",
      "Batch number : 248, Training: Loss:  0.4504, Accuracy: 0.8438\n",
      "Batch number : 249, Training: Loss:  0.6283, Accuracy: 0.8125\n",
      "Batch number : 250, Training: Loss:  0.3836, Accuracy: 0.9219\n",
      "Batch number : 251, Training: Loss:  0.4617, Accuracy: 0.9062\n",
      "Batch number : 252, Training: Loss:  0.5894, Accuracy: 0.8125\n",
      "Batch number : 253, Training: Loss:  0.7421, Accuracy: 0.7812\n",
      "Batch number : 254, Training: Loss:  0.5145, Accuracy: 0.8594\n",
      "Batch number : 255, Training: Loss:  0.4762, Accuracy: 0.8594\n",
      "Batch number : 256, Training: Loss:  0.5440, Accuracy: 0.8281\n",
      "Batch number : 257, Training: Loss:  0.5965, Accuracy: 0.8750\n",
      "Batch number : 258, Training: Loss:  0.3235, Accuracy: 0.9219\n",
      "Batch number : 259, Training: Loss:  0.5081, Accuracy: 0.8906\n",
      "Batch number : 260, Training: Loss:  0.5172, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.3294, Accuracy: 0.9219\n",
      "Batch number : 262, Training: Loss:  0.6639, Accuracy: 0.8281\n",
      "Batch number : 263, Training: Loss:  0.4670, Accuracy: 0.8750\n",
      "Batch number : 264, Training: Loss:  0.6704, Accuracy: 0.7969\n",
      "Batch number : 265, Training: Loss:  0.6105, Accuracy: 0.8125\n",
      "Batch number : 266, Training: Loss:  0.3258, Accuracy: 0.9062\n",
      "Batch number : 267, Training: Loss:  0.4765, Accuracy: 0.8438\n",
      "Batch number : 268, Training: Loss:  0.5955, Accuracy: 0.8125\n",
      "Batch number : 269, Training: Loss:  0.4661, Accuracy: 0.8594\n",
      "Batch number : 270, Training: Loss:  0.4623, Accuracy: 0.8750\n",
      "Batch number : 271, Training: Loss:  0.5580, Accuracy: 0.8438\n",
      "Batch number : 272, Training: Loss:  0.5474, Accuracy: 0.8125\n",
      "Batch number : 273, Training: Loss:  0.5897, Accuracy: 0.8281\n",
      "Batch number : 274, Training: Loss:  0.3725, Accuracy: 0.8906\n",
      "Batch number : 275, Training: Loss:  0.5545, Accuracy: 0.7969\n",
      "Batch number : 276, Training: Loss:  0.6450, Accuracy: 0.8125\n",
      "Batch number : 277, Training: Loss:  0.4184, Accuracy: 0.8281\n",
      "Batch number : 278, Training: Loss:  0.6737, Accuracy: 0.7969\n",
      "Batch number : 279, Training: Loss:  0.6337, Accuracy: 0.8438\n",
      "Batch number : 280, Training: Loss:  0.4568, Accuracy: 0.9062\n",
      "Batch number : 281, Training: Loss:  0.6802, Accuracy: 0.7969\n",
      "Batch number : 282, Training: Loss:  0.4020, Accuracy: 0.9062\n",
      "Batch number : 283, Training: Loss:  0.5247, Accuracy: 0.8125\n",
      "Batch number : 284, Training: Loss:  0.5891, Accuracy: 0.8594\n",
      "Batch number : 285, Training: Loss:  0.3674, Accuracy: 0.8906\n",
      "Batch number : 286, Training: Loss:  0.6190, Accuracy: 0.7969\n",
      "Batch number : 287, Training: Loss:  0.5479, Accuracy: 0.8281\n",
      "Batch number : 288, Training: Loss:  0.4371, Accuracy: 0.8906\n",
      "Batch number : 289, Training: Loss:  0.3312, Accuracy: 0.9219\n",
      "Batch number : 290, Training: Loss:  0.5173, Accuracy: 0.8281\n",
      "Batch number : 291, Training: Loss:  0.4595, Accuracy: 0.8281\n",
      "Batch number : 292, Training: Loss:  0.3277, Accuracy: 0.9219\n",
      "Batch number : 293, Training: Loss:  0.3250, Accuracy: 0.9062\n",
      "Batch number : 294, Training: Loss:  0.6458, Accuracy: 0.7812\n",
      "Batch number : 295, Training: Loss:  0.4218, Accuracy: 0.8750\n",
      "Batch number : 296, Training: Loss:  0.5133, Accuracy: 0.8594\n",
      "Batch number : 297, Training: Loss:  0.5171, Accuracy: 0.8438\n",
      "Batch number : 298, Training: Loss:  0.3431, Accuracy: 0.9062\n",
      "Batch number : 299, Training: Loss:  0.4425, Accuracy: 0.8594\n",
      "Batch number : 300, Training: Loss:  0.6441, Accuracy: 0.7969\n",
      "Batch number : 301, Training: Loss:  0.3834, Accuracy: 0.8750\n",
      "Batch number : 302, Training: Loss:  0.6569, Accuracy: 0.7656\n",
      "Batch number : 303, Training: Loss:  0.6940, Accuracy: 0.7969\n",
      "Batch number : 304, Training: Loss:  0.5466, Accuracy: 0.8438\n",
      "Batch number : 305, Training: Loss:  0.4895, Accuracy: 0.8438\n",
      "Batch number : 306, Training: Loss:  0.4024, Accuracy: 0.8906\n",
      "Batch number : 307, Training: Loss:  0.4847, Accuracy: 0.9219\n",
      "Batch number : 308, Training: Loss:  0.4494, Accuracy: 0.8594\n",
      "Batch number : 309, Training: Loss:  0.4792, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.5101, Accuracy: 0.8594\n",
      "Batch number : 311, Training: Loss:  0.3001, Accuracy: 0.9219\n",
      "Batch number : 312, Training: Loss:  0.6433, Accuracy: 0.8125\n",
      "Batch number : 313, Training: Loss:  0.5160, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.2603, Accuracy: 0.9219\n",
      "Batch number : 315, Training: Loss:  0.6088, Accuracy: 0.8125\n",
      "Batch number : 316, Training: Loss:  0.4847, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.7728, Accuracy: 0.7812\n",
      "Batch number : 318, Training: Loss:  0.4529, Accuracy: 0.8906\n",
      "Batch number : 319, Training: Loss:  0.4146, Accuracy: 0.8750\n",
      "Batch number : 320, Training: Loss:  0.4090, Accuracy: 0.8906\n",
      "Batch number : 321, Training: Loss:  0.4841, Accuracy: 0.8594\n",
      "Batch number : 322, Training: Loss:  0.4589, Accuracy: 0.8594\n",
      "Batch number : 323, Training: Loss:  0.4608, Accuracy: 0.8438\n",
      "Batch number : 324, Training: Loss:  0.3310, Accuracy: 0.9375\n",
      "Batch number : 325, Training: Loss:  0.3952, Accuracy: 0.9219\n",
      "Batch number : 326, Training: Loss:  0.2970, Accuracy: 0.9219\n",
      "Batch number : 327, Training: Loss:  0.4049, Accuracy: 0.8906\n",
      "Batch number : 328, Training: Loss:  0.3734, Accuracy: 0.9219\n",
      "Batch number : 329, Training: Loss:  0.4360, Accuracy: 0.8750\n",
      "Batch number : 330, Training: Loss:  0.5079, Accuracy: 0.8594\n",
      "Batch number : 331, Training: Loss:  0.4940, Accuracy: 0.8594\n",
      "Batch number : 332, Training: Loss:  0.4250, Accuracy: 0.8750\n",
      "Batch number : 333, Training: Loss:  0.6147, Accuracy: 0.8438\n",
      "Batch number : 334, Training: Loss:  0.5451, Accuracy: 0.8438\n",
      "Batch number : 335, Training: Loss:  0.4781, Accuracy: 0.8750\n",
      "Batch number : 336, Training: Loss:  0.6407, Accuracy: 0.7969\n",
      "Batch number : 337, Training: Loss:  0.5531, Accuracy: 0.8438\n",
      "Batch number : 338, Training: Loss:  0.5713, Accuracy: 0.8125\n",
      "Batch number : 339, Training: Loss:  0.3691, Accuracy: 0.9219\n",
      "Batch number : 340, Training: Loss:  0.6402, Accuracy: 0.8281\n",
      "Batch number : 341, Training: Loss:  0.5352, Accuracy: 0.8594\n",
      "Batch number : 342, Training: Loss:  0.3794, Accuracy: 0.8906\n",
      "Batch number : 343, Training: Loss:  0.4067, Accuracy: 0.9219\n",
      "Batch number : 344, Training: Loss:  0.8130, Accuracy: 0.7656\n",
      "Batch number : 345, Training: Loss:  0.6732, Accuracy: 0.7812\n",
      "Batch number : 346, Training: Loss:  0.5442, Accuracy: 0.8594\n",
      "Batch number : 347, Training: Loss:  0.6062, Accuracy: 0.8125\n",
      "Batch number : 348, Training: Loss:  0.5696, Accuracy: 0.8281\n",
      "Batch number : 349, Training: Loss:  0.5668, Accuracy: 0.8438\n",
      "Batch number : 350, Training: Loss:  0.6059, Accuracy: 0.8125\n",
      "Batch number : 351, Training: Loss:  0.5753, Accuracy: 0.8438\n",
      "Batch number : 352, Training: Loss:  0.5371, Accuracy: 0.8594\n",
      "Batch number : 353, Training: Loss:  0.4463, Accuracy: 0.9062\n",
      "Batch number : 354, Training: Loss:  0.6622, Accuracy: 0.8281\n",
      "Batch number : 355, Training: Loss:  0.5125, Accuracy: 0.8125\n",
      "Batch number : 356, Training: Loss:  0.5318, Accuracy: 0.8281\n",
      "Batch number : 357, Training: Loss:  0.4476, Accuracy: 0.8750\n",
      "Batch number : 358, Training: Loss:  0.5538, Accuracy: 0.8281\n",
      "Batch number : 359, Training: Loss:  0.5727, Accuracy: 0.8281\n",
      "Batch number : 360, Training: Loss:  0.7234, Accuracy: 0.7656\n",
      "Batch number : 361, Training: Loss:  0.3549, Accuracy: 0.9062\n",
      "Batch number : 362, Training: Loss:  0.4666, Accuracy: 0.8594\n",
      "Batch number : 363, Training: Loss:  0.5681, Accuracy: 0.7969\n",
      "Batch number : 364, Training: Loss:  0.3842, Accuracy: 0.9062\n",
      "Batch number : 365, Training: Loss:  0.3222, Accuracy: 0.9219\n",
      "Batch number : 366, Training: Loss:  0.4697, Accuracy: 0.8281\n",
      "Batch number : 367, Training: Loss:  0.2504, Accuracy: 0.9688\n",
      "Batch number : 368, Training: Loss:  0.7104, Accuracy: 0.7812\n",
      "Batch number : 369, Training: Loss:  0.4120, Accuracy: 0.8750\n",
      "Batch number : 370, Training: Loss:  0.3953, Accuracy: 0.8750\n",
      "Batch number : 371, Training: Loss:  0.3455, Accuracy: 0.8906\n",
      "Batch number : 372, Training: Loss:  0.4434, Accuracy: 0.8594\n",
      "Batch number : 373, Training: Loss:  0.5186, Accuracy: 0.8438\n",
      "Batch number : 374, Training: Loss:  0.4392, Accuracy: 0.8750\n",
      "Batch number : 375, Training: Loss:  0.5511, Accuracy: 0.8438\n",
      "Batch number : 376, Training: Loss:  0.4608, Accuracy: 0.8438\n",
      "Batch number : 377, Training: Loss:  0.9721, Accuracy: 0.7500\n",
      "Epoch: 4/20\n",
      "Batch number : 000, Training: Loss:  0.5414, Accuracy: 0.8594\n",
      "Batch number : 001, Training: Loss:  0.2675, Accuracy: 0.9688\n",
      "Batch number : 002, Training: Loss:  0.5179, Accuracy: 0.8594\n",
      "Batch number : 003, Training: Loss:  0.5825, Accuracy: 0.8125\n",
      "Batch number : 004, Training: Loss:  0.6115, Accuracy: 0.8750\n",
      "Batch number : 005, Training: Loss:  0.4841, Accuracy: 0.8750\n",
      "Batch number : 006, Training: Loss:  0.6271, Accuracy: 0.7812\n",
      "Batch number : 007, Training: Loss:  0.5017, Accuracy: 0.8594\n",
      "Batch number : 008, Training: Loss:  0.4378, Accuracy: 0.8594\n",
      "Batch number : 009, Training: Loss:  0.3251, Accuracy: 0.9062\n",
      "Batch number : 010, Training: Loss:  0.5155, Accuracy: 0.8281\n",
      "Batch number : 011, Training: Loss:  0.4658, Accuracy: 0.8281\n",
      "Batch number : 012, Training: Loss:  0.5273, Accuracy: 0.8594\n",
      "Batch number : 013, Training: Loss:  0.4261, Accuracy: 0.8750\n",
      "Batch number : 014, Training: Loss:  0.4882, Accuracy: 0.8750\n",
      "Batch number : 015, Training: Loss:  0.4611, Accuracy: 0.8906\n",
      "Batch number : 016, Training: Loss:  0.6219, Accuracy: 0.8281\n",
      "Batch number : 017, Training: Loss:  0.5208, Accuracy: 0.8281\n",
      "Batch number : 018, Training: Loss:  0.4259, Accuracy: 0.8750\n",
      "Batch number : 019, Training: Loss:  0.4972, Accuracy: 0.8438\n",
      "Batch number : 020, Training: Loss:  0.5094, Accuracy: 0.8594\n",
      "Batch number : 021, Training: Loss:  0.5337, Accuracy: 0.8281\n",
      "Batch number : 022, Training: Loss:  0.5328, Accuracy: 0.8281\n",
      "Batch number : 023, Training: Loss:  0.6315, Accuracy: 0.8438\n",
      "Batch number : 024, Training: Loss:  0.5183, Accuracy: 0.8438\n",
      "Batch number : 025, Training: Loss:  0.3656, Accuracy: 0.9062\n",
      "Batch number : 026, Training: Loss:  0.5122, Accuracy: 0.8281\n",
      "Batch number : 027, Training: Loss:  0.3254, Accuracy: 0.9062\n",
      "Batch number : 028, Training: Loss:  0.2561, Accuracy: 0.9531\n",
      "Batch number : 029, Training: Loss:  0.4143, Accuracy: 0.8594\n",
      "Batch number : 030, Training: Loss:  0.3482, Accuracy: 0.9062\n",
      "Batch number : 031, Training: Loss:  0.4412, Accuracy: 0.8594\n",
      "Batch number : 032, Training: Loss:  0.1796, Accuracy: 0.9531\n",
      "Batch number : 033, Training: Loss:  0.8164, Accuracy: 0.7812\n",
      "Batch number : 034, Training: Loss:  0.8065, Accuracy: 0.7969\n",
      "Batch number : 035, Training: Loss:  0.5801, Accuracy: 0.8438\n",
      "Batch number : 036, Training: Loss:  0.5598, Accuracy: 0.8438\n",
      "Batch number : 037, Training: Loss:  0.4124, Accuracy: 0.8750\n",
      "Batch number : 038, Training: Loss:  0.4947, Accuracy: 0.8594\n",
      "Batch number : 039, Training: Loss:  0.6096, Accuracy: 0.7969\n",
      "Batch number : 040, Training: Loss:  0.5381, Accuracy: 0.8594\n",
      "Batch number : 041, Training: Loss:  0.3825, Accuracy: 0.9219\n",
      "Batch number : 042, Training: Loss:  0.5948, Accuracy: 0.8125\n",
      "Batch number : 043, Training: Loss:  0.5621, Accuracy: 0.7812\n",
      "Batch number : 044, Training: Loss:  0.4372, Accuracy: 0.8906\n",
      "Batch number : 045, Training: Loss:  0.5688, Accuracy: 0.7969\n",
      "Batch number : 046, Training: Loss:  0.4673, Accuracy: 0.8750\n",
      "Batch number : 047, Training: Loss:  0.3151, Accuracy: 0.9219\n",
      "Batch number : 048, Training: Loss:  0.3113, Accuracy: 0.8906\n",
      "Batch number : 049, Training: Loss:  0.8206, Accuracy: 0.7500\n",
      "Batch number : 050, Training: Loss:  0.4065, Accuracy: 0.8750\n",
      "Batch number : 051, Training: Loss:  0.5496, Accuracy: 0.8281\n",
      "Batch number : 052, Training: Loss:  0.3788, Accuracy: 0.8906\n",
      "Batch number : 053, Training: Loss:  0.5704, Accuracy: 0.8125\n",
      "Batch number : 054, Training: Loss:  0.5413, Accuracy: 0.8438\n",
      "Batch number : 055, Training: Loss:  0.3921, Accuracy: 0.9062\n",
      "Batch number : 056, Training: Loss:  0.4638, Accuracy: 0.8750\n",
      "Batch number : 057, Training: Loss:  0.3377, Accuracy: 0.9219\n",
      "Batch number : 058, Training: Loss:  0.3787, Accuracy: 0.9062\n",
      "Batch number : 059, Training: Loss:  0.3584, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.5495, Accuracy: 0.8125\n",
      "Batch number : 061, Training: Loss:  0.4150, Accuracy: 0.8594\n",
      "Batch number : 062, Training: Loss:  0.3602, Accuracy: 0.9062\n",
      "Batch number : 063, Training: Loss:  0.3637, Accuracy: 0.9219\n",
      "Batch number : 064, Training: Loss:  0.2699, Accuracy: 0.9375\n",
      "Batch number : 065, Training: Loss:  0.4305, Accuracy: 0.8594\n",
      "Batch number : 066, Training: Loss:  0.4935, Accuracy: 0.8594\n",
      "Batch number : 067, Training: Loss:  0.4505, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.3996, Accuracy: 0.8750\n",
      "Batch number : 069, Training: Loss:  0.3747, Accuracy: 0.8750\n",
      "Batch number : 070, Training: Loss:  0.4547, Accuracy: 0.8438\n",
      "Batch number : 071, Training: Loss:  0.3473, Accuracy: 0.8906\n",
      "Batch number : 072, Training: Loss:  0.4421, Accuracy: 0.8594\n",
      "Batch number : 073, Training: Loss:  0.3503, Accuracy: 0.9062\n",
      "Batch number : 074, Training: Loss:  0.7812, Accuracy: 0.7500\n",
      "Batch number : 075, Training: Loss:  0.5147, Accuracy: 0.8281\n",
      "Batch number : 076, Training: Loss:  0.5950, Accuracy: 0.7969\n",
      "Batch number : 077, Training: Loss:  0.5242, Accuracy: 0.8594\n",
      "Batch number : 078, Training: Loss:  0.4887, Accuracy: 0.8750\n",
      "Batch number : 079, Training: Loss:  0.5829, Accuracy: 0.7969\n",
      "Batch number : 080, Training: Loss:  0.5313, Accuracy: 0.8281\n",
      "Batch number : 081, Training: Loss:  0.5422, Accuracy: 0.7969\n",
      "Batch number : 082, Training: Loss:  0.4196, Accuracy: 0.8594\n",
      "Batch number : 083, Training: Loss:  0.4738, Accuracy: 0.8594\n",
      "Batch number : 084, Training: Loss:  0.4569, Accuracy: 0.8750\n",
      "Batch number : 085, Training: Loss:  0.8443, Accuracy: 0.7344\n",
      "Batch number : 086, Training: Loss:  0.4657, Accuracy: 0.8750\n",
      "Batch number : 087, Training: Loss:  0.6393, Accuracy: 0.7969\n",
      "Batch number : 088, Training: Loss:  0.5628, Accuracy: 0.8438\n",
      "Batch number : 089, Training: Loss:  0.6015, Accuracy: 0.8281\n",
      "Batch number : 090, Training: Loss:  0.7588, Accuracy: 0.7656\n",
      "Batch number : 091, Training: Loss:  0.4963, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.3907, Accuracy: 0.9062\n",
      "Batch number : 093, Training: Loss:  0.4794, Accuracy: 0.8906\n",
      "Batch number : 094, Training: Loss:  0.2686, Accuracy: 0.9375\n",
      "Batch number : 095, Training: Loss:  0.6840, Accuracy: 0.8281\n",
      "Batch number : 096, Training: Loss:  0.5977, Accuracy: 0.8594\n",
      "Batch number : 097, Training: Loss:  0.4267, Accuracy: 0.8594\n",
      "Batch number : 098, Training: Loss:  0.4896, Accuracy: 0.8125\n",
      "Batch number : 099, Training: Loss:  0.3898, Accuracy: 0.9062\n",
      "Batch number : 100, Training: Loss:  0.4591, Accuracy: 0.8438\n",
      "Batch number : 101, Training: Loss:  0.5798, Accuracy: 0.7969\n",
      "Batch number : 102, Training: Loss:  0.6264, Accuracy: 0.8125\n",
      "Batch number : 103, Training: Loss:  0.7427, Accuracy: 0.7969\n",
      "Batch number : 104, Training: Loss:  0.3556, Accuracy: 0.9375\n",
      "Batch number : 105, Training: Loss:  0.3374, Accuracy: 0.9375\n",
      "Batch number : 106, Training: Loss:  0.4620, Accuracy: 0.8594\n",
      "Batch number : 107, Training: Loss:  0.6300, Accuracy: 0.8125\n",
      "Batch number : 108, Training: Loss:  0.3746, Accuracy: 0.9062\n",
      "Batch number : 109, Training: Loss:  0.5675, Accuracy: 0.8281\n",
      "Batch number : 110, Training: Loss:  0.5511, Accuracy: 0.8281\n",
      "Batch number : 111, Training: Loss:  0.5104, Accuracy: 0.8594\n",
      "Batch number : 112, Training: Loss:  0.3240, Accuracy: 0.9219\n",
      "Batch number : 113, Training: Loss:  0.4927, Accuracy: 0.8281\n",
      "Batch number : 114, Training: Loss:  0.4216, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.4602, Accuracy: 0.8594\n",
      "Batch number : 116, Training: Loss:  0.5690, Accuracy: 0.8281\n",
      "Batch number : 117, Training: Loss:  0.5561, Accuracy: 0.8438\n",
      "Batch number : 118, Training: Loss:  0.7978, Accuracy: 0.7656\n",
      "Batch number : 119, Training: Loss:  0.6642, Accuracy: 0.8125\n",
      "Batch number : 120, Training: Loss:  0.4118, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.4121, Accuracy: 0.9062\n",
      "Batch number : 122, Training: Loss:  0.4062, Accuracy: 0.9375\n",
      "Batch number : 123, Training: Loss:  0.5413, Accuracy: 0.8594\n",
      "Batch number : 124, Training: Loss:  0.5587, Accuracy: 0.8438\n",
      "Batch number : 125, Training: Loss:  0.4719, Accuracy: 0.8594\n",
      "Batch number : 126, Training: Loss:  0.5926, Accuracy: 0.8125\n",
      "Batch number : 127, Training: Loss:  0.4233, Accuracy: 0.8594\n",
      "Batch number : 128, Training: Loss:  0.5372, Accuracy: 0.8438\n",
      "Batch number : 129, Training: Loss:  0.6335, Accuracy: 0.8281\n",
      "Batch number : 130, Training: Loss:  0.3319, Accuracy: 0.9219\n",
      "Batch number : 131, Training: Loss:  0.4424, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.6796, Accuracy: 0.7969\n",
      "Batch number : 133, Training: Loss:  0.5494, Accuracy: 0.8438\n",
      "Batch number : 134, Training: Loss:  0.5013, Accuracy: 0.8594\n",
      "Batch number : 135, Training: Loss:  0.4824, Accuracy: 0.8594\n",
      "Batch number : 136, Training: Loss:  0.4314, Accuracy: 0.8750\n",
      "Batch number : 137, Training: Loss:  0.4221, Accuracy: 0.8906\n",
      "Batch number : 138, Training: Loss:  0.5638, Accuracy: 0.8281\n",
      "Batch number : 139, Training: Loss:  0.5354, Accuracy: 0.8438\n",
      "Batch number : 140, Training: Loss:  0.6059, Accuracy: 0.8125\n",
      "Batch number : 141, Training: Loss:  0.3800, Accuracy: 0.8906\n",
      "Batch number : 142, Training: Loss:  0.4179, Accuracy: 0.8594\n",
      "Batch number : 143, Training: Loss:  0.6440, Accuracy: 0.8125\n",
      "Batch number : 144, Training: Loss:  0.5835, Accuracy: 0.8594\n",
      "Batch number : 145, Training: Loss:  0.5507, Accuracy: 0.8125\n",
      "Batch number : 146, Training: Loss:  0.6120, Accuracy: 0.7969\n",
      "Batch number : 147, Training: Loss:  0.4766, Accuracy: 0.8438\n",
      "Batch number : 148, Training: Loss:  0.5385, Accuracy: 0.8594\n",
      "Batch number : 149, Training: Loss:  0.5337, Accuracy: 0.8281\n",
      "Batch number : 150, Training: Loss:  0.3863, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.4357, Accuracy: 0.9219\n",
      "Batch number : 152, Training: Loss:  0.5269, Accuracy: 0.8125\n",
      "Batch number : 153, Training: Loss:  0.3984, Accuracy: 0.9062\n",
      "Batch number : 154, Training: Loss:  0.7220, Accuracy: 0.7812\n",
      "Batch number : 155, Training: Loss:  0.4341, Accuracy: 0.8906\n",
      "Batch number : 156, Training: Loss:  0.3806, Accuracy: 0.9062\n",
      "Batch number : 157, Training: Loss:  0.4054, Accuracy: 0.8906\n",
      "Batch number : 158, Training: Loss:  0.6345, Accuracy: 0.8125\n",
      "Batch number : 159, Training: Loss:  0.5872, Accuracy: 0.8281\n",
      "Batch number : 160, Training: Loss:  0.4114, Accuracy: 0.8750\n",
      "Batch number : 161, Training: Loss:  0.3208, Accuracy: 0.9219\n",
      "Batch number : 162, Training: Loss:  0.4060, Accuracy: 0.8594\n",
      "Batch number : 163, Training: Loss:  0.6025, Accuracy: 0.7812\n",
      "Batch number : 164, Training: Loss:  0.4919, Accuracy: 0.8438\n",
      "Batch number : 165, Training: Loss:  0.6655, Accuracy: 0.8281\n",
      "Batch number : 166, Training: Loss:  0.2910, Accuracy: 0.9375\n",
      "Batch number : 167, Training: Loss:  0.4750, Accuracy: 0.8281\n",
      "Batch number : 168, Training: Loss:  0.4588, Accuracy: 0.8906\n",
      "Batch number : 169, Training: Loss:  0.3774, Accuracy: 0.8906\n",
      "Batch number : 170, Training: Loss:  0.3866, Accuracy: 0.8906\n",
      "Batch number : 171, Training: Loss:  0.7735, Accuracy: 0.7656\n",
      "Batch number : 172, Training: Loss:  0.3545, Accuracy: 0.9062\n",
      "Batch number : 173, Training: Loss:  0.3815, Accuracy: 0.8906\n",
      "Batch number : 174, Training: Loss:  0.5670, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.6869, Accuracy: 0.7812\n",
      "Batch number : 176, Training: Loss:  0.5305, Accuracy: 0.8125\n",
      "Batch number : 177, Training: Loss:  0.5776, Accuracy: 0.8281\n",
      "Batch number : 178, Training: Loss:  0.5995, Accuracy: 0.8281\n",
      "Batch number : 179, Training: Loss:  0.4597, Accuracy: 0.8594\n",
      "Batch number : 180, Training: Loss:  0.4281, Accuracy: 0.8594\n",
      "Batch number : 181, Training: Loss:  0.5188, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.4316, Accuracy: 0.8750\n",
      "Batch number : 183, Training: Loss:  0.7450, Accuracy: 0.7969\n",
      "Batch number : 184, Training: Loss:  0.5013, Accuracy: 0.8750\n",
      "Batch number : 185, Training: Loss:  0.5360, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.3491, Accuracy: 0.8906\n",
      "Batch number : 187, Training: Loss:  0.2969, Accuracy: 0.9219\n",
      "Batch number : 188, Training: Loss:  0.4284, Accuracy: 0.9062\n",
      "Batch number : 189, Training: Loss:  0.4791, Accuracy: 0.8594\n",
      "Batch number : 190, Training: Loss:  0.5134, Accuracy: 0.8125\n",
      "Batch number : 191, Training: Loss:  0.4653, Accuracy: 0.8125\n",
      "Batch number : 192, Training: Loss:  0.7002, Accuracy: 0.7344\n",
      "Batch number : 193, Training: Loss:  0.5936, Accuracy: 0.8750\n",
      "Batch number : 194, Training: Loss:  0.6521, Accuracy: 0.8281\n",
      "Batch number : 195, Training: Loss:  0.6102, Accuracy: 0.7969\n",
      "Batch number : 196, Training: Loss:  0.6023, Accuracy: 0.8125\n",
      "Batch number : 197, Training: Loss:  0.4169, Accuracy: 0.8906\n",
      "Batch number : 198, Training: Loss:  0.6204, Accuracy: 0.7812\n",
      "Batch number : 199, Training: Loss:  0.5632, Accuracy: 0.8281\n",
      "Batch number : 200, Training: Loss:  0.4733, Accuracy: 0.8594\n",
      "Batch number : 201, Training: Loss:  0.4600, Accuracy: 0.8750\n",
      "Batch number : 202, Training: Loss:  0.5659, Accuracy: 0.8438\n",
      "Batch number : 203, Training: Loss:  0.6471, Accuracy: 0.8125\n",
      "Batch number : 204, Training: Loss:  0.3562, Accuracy: 0.9062\n",
      "Batch number : 205, Training: Loss:  0.4766, Accuracy: 0.8438\n",
      "Batch number : 206, Training: Loss:  0.6713, Accuracy: 0.8125\n",
      "Batch number : 207, Training: Loss:  0.4651, Accuracy: 0.8594\n",
      "Batch number : 208, Training: Loss:  0.6607, Accuracy: 0.7969\n",
      "Batch number : 209, Training: Loss:  0.6410, Accuracy: 0.8281\n",
      "Batch number : 210, Training: Loss:  0.4759, Accuracy: 0.8438\n",
      "Batch number : 211, Training: Loss:  0.4653, Accuracy: 0.8750\n",
      "Batch number : 212, Training: Loss:  0.5182, Accuracy: 0.8594\n",
      "Batch number : 213, Training: Loss:  0.4531, Accuracy: 0.8750\n",
      "Batch number : 214, Training: Loss:  0.5497, Accuracy: 0.8281\n",
      "Batch number : 215, Training: Loss:  0.4501, Accuracy: 0.9062\n",
      "Batch number : 216, Training: Loss:  0.4626, Accuracy: 0.8750\n",
      "Batch number : 217, Training: Loss:  0.5189, Accuracy: 0.8281\n",
      "Batch number : 218, Training: Loss:  0.5080, Accuracy: 0.8594\n",
      "Batch number : 219, Training: Loss:  0.2789, Accuracy: 0.9375\n",
      "Batch number : 220, Training: Loss:  0.4593, Accuracy: 0.8750\n",
      "Batch number : 221, Training: Loss:  0.4516, Accuracy: 0.8750\n",
      "Batch number : 222, Training: Loss:  0.3477, Accuracy: 0.9062\n",
      "Batch number : 223, Training: Loss:  0.3099, Accuracy: 0.9062\n",
      "Batch number : 224, Training: Loss:  0.6549, Accuracy: 0.8438\n",
      "Batch number : 225, Training: Loss:  0.6461, Accuracy: 0.8281\n",
      "Batch number : 226, Training: Loss:  0.4358, Accuracy: 0.8594\n",
      "Batch number : 227, Training: Loss:  0.4692, Accuracy: 0.8125\n",
      "Batch number : 228, Training: Loss:  0.5071, Accuracy: 0.8438\n",
      "Batch number : 229, Training: Loss:  0.4047, Accuracy: 0.9219\n",
      "Batch number : 230, Training: Loss:  0.5034, Accuracy: 0.8438\n",
      "Batch number : 231, Training: Loss:  0.4017, Accuracy: 0.9062\n",
      "Batch number : 232, Training: Loss:  0.5344, Accuracy: 0.8594\n",
      "Batch number : 233, Training: Loss:  0.4690, Accuracy: 0.8906\n",
      "Batch number : 234, Training: Loss:  0.4990, Accuracy: 0.8594\n",
      "Batch number : 235, Training: Loss:  0.4489, Accuracy: 0.8906\n",
      "Batch number : 236, Training: Loss:  0.4130, Accuracy: 0.8906\n",
      "Batch number : 237, Training: Loss:  0.6947, Accuracy: 0.7969\n",
      "Batch number : 238, Training: Loss:  0.3363, Accuracy: 0.9219\n",
      "Batch number : 239, Training: Loss:  0.5351, Accuracy: 0.8438\n",
      "Batch number : 240, Training: Loss:  0.3307, Accuracy: 0.9375\n",
      "Batch number : 241, Training: Loss:  0.4956, Accuracy: 0.8750\n",
      "Batch number : 242, Training: Loss:  0.2943, Accuracy: 0.9219\n",
      "Batch number : 243, Training: Loss:  0.3800, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.4644, Accuracy: 0.8438\n",
      "Batch number : 245, Training: Loss:  0.7217, Accuracy: 0.7969\n",
      "Batch number : 246, Training: Loss:  0.5701, Accuracy: 0.8281\n",
      "Batch number : 247, Training: Loss:  0.3735, Accuracy: 0.8750\n",
      "Batch number : 248, Training: Loss:  0.5290, Accuracy: 0.8125\n",
      "Batch number : 249, Training: Loss:  0.6619, Accuracy: 0.7812\n",
      "Batch number : 250, Training: Loss:  0.5589, Accuracy: 0.8281\n",
      "Batch number : 251, Training: Loss:  0.4305, Accuracy: 0.8906\n",
      "Batch number : 252, Training: Loss:  0.4623, Accuracy: 0.8594\n",
      "Batch number : 253, Training: Loss:  0.4217, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.5490, Accuracy: 0.8438\n",
      "Batch number : 255, Training: Loss:  0.4091, Accuracy: 0.8594\n",
      "Batch number : 256, Training: Loss:  0.4634, Accuracy: 0.8750\n",
      "Batch number : 257, Training: Loss:  0.4664, Accuracy: 0.8906\n",
      "Batch number : 258, Training: Loss:  0.2755, Accuracy: 0.9219\n",
      "Batch number : 259, Training: Loss:  0.7565, Accuracy: 0.7656\n",
      "Batch number : 260, Training: Loss:  0.5455, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.7214, Accuracy: 0.7969\n",
      "Batch number : 262, Training: Loss:  0.4130, Accuracy: 0.8750\n",
      "Batch number : 263, Training: Loss:  0.5866, Accuracy: 0.8125\n",
      "Batch number : 264, Training: Loss:  0.5582, Accuracy: 0.8750\n",
      "Batch number : 265, Training: Loss:  0.6015, Accuracy: 0.8438\n",
      "Batch number : 266, Training: Loss:  0.5492, Accuracy: 0.8906\n",
      "Batch number : 267, Training: Loss:  0.5113, Accuracy: 0.8281\n",
      "Batch number : 268, Training: Loss:  0.2982, Accuracy: 0.9375\n",
      "Batch number : 269, Training: Loss:  0.6580, Accuracy: 0.8281\n",
      "Batch number : 270, Training: Loss:  0.5787, Accuracy: 0.8281\n",
      "Batch number : 271, Training: Loss:  0.5867, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.5305, Accuracy: 0.8438\n",
      "Batch number : 273, Training: Loss:  0.4144, Accuracy: 0.8906\n",
      "Batch number : 274, Training: Loss:  0.8379, Accuracy: 0.7656\n",
      "Batch number : 275, Training: Loss:  0.3150, Accuracy: 0.9219\n",
      "Batch number : 276, Training: Loss:  0.4857, Accuracy: 0.8594\n",
      "Batch number : 277, Training: Loss:  0.5387, Accuracy: 0.8125\n",
      "Batch number : 278, Training: Loss:  0.2643, Accuracy: 0.9375\n",
      "Batch number : 279, Training: Loss:  0.6543, Accuracy: 0.7812\n",
      "Batch number : 280, Training: Loss:  0.5879, Accuracy: 0.8438\n",
      "Batch number : 281, Training: Loss:  0.3269, Accuracy: 0.9375\n",
      "Batch number : 282, Training: Loss:  0.5996, Accuracy: 0.7969\n",
      "Batch number : 283, Training: Loss:  0.6120, Accuracy: 0.8281\n",
      "Batch number : 284, Training: Loss:  0.5302, Accuracy: 0.8281\n",
      "Batch number : 285, Training: Loss:  0.4672, Accuracy: 0.8750\n",
      "Batch number : 286, Training: Loss:  0.3530, Accuracy: 0.8906\n",
      "Batch number : 287, Training: Loss:  0.4237, Accuracy: 0.8906\n",
      "Batch number : 288, Training: Loss:  0.4506, Accuracy: 0.8594\n",
      "Batch number : 289, Training: Loss:  0.4551, Accuracy: 0.8750\n",
      "Batch number : 290, Training: Loss:  0.3770, Accuracy: 0.8906\n",
      "Batch number : 291, Training: Loss:  0.6071, Accuracy: 0.7656\n",
      "Batch number : 292, Training: Loss:  0.4700, Accuracy: 0.8906\n",
      "Batch number : 293, Training: Loss:  0.4006, Accuracy: 0.9219\n",
      "Batch number : 294, Training: Loss:  0.5277, Accuracy: 0.8281\n",
      "Batch number : 295, Training: Loss:  0.4976, Accuracy: 0.8750\n",
      "Batch number : 296, Training: Loss:  0.6105, Accuracy: 0.8438\n",
      "Batch number : 297, Training: Loss:  0.6356, Accuracy: 0.7656\n",
      "Batch number : 298, Training: Loss:  0.4155, Accuracy: 0.8438\n",
      "Batch number : 299, Training: Loss:  0.5159, Accuracy: 0.8594\n",
      "Batch number : 300, Training: Loss:  0.5098, Accuracy: 0.8750\n",
      "Batch number : 301, Training: Loss:  0.6415, Accuracy: 0.8125\n",
      "Batch number : 302, Training: Loss:  0.6113, Accuracy: 0.8281\n",
      "Batch number : 303, Training: Loss:  0.5668, Accuracy: 0.8594\n",
      "Batch number : 304, Training: Loss:  0.5124, Accuracy: 0.8125\n",
      "Batch number : 305, Training: Loss:  0.5702, Accuracy: 0.8281\n",
      "Batch number : 306, Training: Loss:  0.4459, Accuracy: 0.8750\n",
      "Batch number : 307, Training: Loss:  0.4108, Accuracy: 0.8594\n",
      "Batch number : 308, Training: Loss:  0.4897, Accuracy: 0.8438\n",
      "Batch number : 309, Training: Loss:  0.3500, Accuracy: 0.9062\n",
      "Batch number : 310, Training: Loss:  0.4659, Accuracy: 0.8750\n",
      "Batch number : 311, Training: Loss:  0.4939, Accuracy: 0.8594\n",
      "Batch number : 312, Training: Loss:  0.5209, Accuracy: 0.8125\n",
      "Batch number : 313, Training: Loss:  0.6013, Accuracy: 0.8125\n",
      "Batch number : 314, Training: Loss:  0.4323, Accuracy: 0.8750\n",
      "Batch number : 315, Training: Loss:  0.5626, Accuracy: 0.8125\n",
      "Batch number : 316, Training: Loss:  0.4157, Accuracy: 0.8906\n",
      "Batch number : 317, Training: Loss:  0.4194, Accuracy: 0.8594\n",
      "Batch number : 318, Training: Loss:  0.4995, Accuracy: 0.8594\n",
      "Batch number : 319, Training: Loss:  0.5218, Accuracy: 0.8281\n",
      "Batch number : 320, Training: Loss:  0.4401, Accuracy: 0.8906\n",
      "Batch number : 321, Training: Loss:  0.6790, Accuracy: 0.8125\n",
      "Batch number : 322, Training: Loss:  0.4605, Accuracy: 0.8906\n",
      "Batch number : 323, Training: Loss:  0.3346, Accuracy: 0.9375\n",
      "Batch number : 324, Training: Loss:  0.5124, Accuracy: 0.8281\n",
      "Batch number : 325, Training: Loss:  0.2809, Accuracy: 0.9375\n",
      "Batch number : 326, Training: Loss:  0.4520, Accuracy: 0.8594\n",
      "Batch number : 327, Training: Loss:  0.8551, Accuracy: 0.7969\n",
      "Batch number : 328, Training: Loss:  0.5233, Accuracy: 0.8750\n",
      "Batch number : 329, Training: Loss:  0.3786, Accuracy: 0.9062\n",
      "Batch number : 330, Training: Loss:  0.3740, Accuracy: 0.8750\n",
      "Batch number : 331, Training: Loss:  0.6351, Accuracy: 0.8281\n",
      "Batch number : 332, Training: Loss:  0.3287, Accuracy: 0.8906\n",
      "Batch number : 333, Training: Loss:  0.4876, Accuracy: 0.8594\n",
      "Batch number : 334, Training: Loss:  0.4508, Accuracy: 0.8594\n",
      "Batch number : 335, Training: Loss:  0.4847, Accuracy: 0.8438\n",
      "Batch number : 336, Training: Loss:  0.5242, Accuracy: 0.8750\n",
      "Batch number : 337, Training: Loss:  0.4005, Accuracy: 0.9062\n",
      "Batch number : 338, Training: Loss:  0.4205, Accuracy: 0.8750\n",
      "Batch number : 339, Training: Loss:  0.4986, Accuracy: 0.8125\n",
      "Batch number : 340, Training: Loss:  0.5177, Accuracy: 0.8438\n",
      "Batch number : 341, Training: Loss:  0.4138, Accuracy: 0.8438\n",
      "Batch number : 342, Training: Loss:  0.5878, Accuracy: 0.7969\n",
      "Batch number : 343, Training: Loss:  0.5538, Accuracy: 0.8438\n",
      "Batch number : 344, Training: Loss:  0.3646, Accuracy: 0.9062\n",
      "Batch number : 345, Training: Loss:  0.5370, Accuracy: 0.8438\n",
      "Batch number : 346, Training: Loss:  0.4478, Accuracy: 0.8750\n",
      "Batch number : 347, Training: Loss:  0.5424, Accuracy: 0.8281\n",
      "Batch number : 348, Training: Loss:  0.2440, Accuracy: 0.9688\n",
      "Batch number : 349, Training: Loss:  0.3514, Accuracy: 0.9062\n",
      "Batch number : 350, Training: Loss:  0.6886, Accuracy: 0.7969\n",
      "Batch number : 351, Training: Loss:  0.3147, Accuracy: 0.9062\n",
      "Batch number : 352, Training: Loss:  0.4486, Accuracy: 0.8906\n",
      "Batch number : 353, Training: Loss:  0.6298, Accuracy: 0.8281\n",
      "Batch number : 354, Training: Loss:  0.5377, Accuracy: 0.8438\n",
      "Batch number : 355, Training: Loss:  0.3894, Accuracy: 0.9062\n",
      "Batch number : 356, Training: Loss:  0.3875, Accuracy: 0.9062\n",
      "Batch number : 357, Training: Loss:  0.4692, Accuracy: 0.8594\n",
      "Batch number : 358, Training: Loss:  0.4339, Accuracy: 0.8750\n",
      "Batch number : 359, Training: Loss:  0.4261, Accuracy: 0.8906\n",
      "Batch number : 360, Training: Loss:  0.2373, Accuracy: 0.9531\n",
      "Batch number : 361, Training: Loss:  0.5084, Accuracy: 0.8594\n",
      "Batch number : 362, Training: Loss:  0.7184, Accuracy: 0.8281\n",
      "Batch number : 363, Training: Loss:  0.4907, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.5514, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.3897, Accuracy: 0.8750\n",
      "Batch number : 366, Training: Loss:  0.7525, Accuracy: 0.7656\n",
      "Batch number : 367, Training: Loss:  0.4107, Accuracy: 0.8594\n",
      "Batch number : 368, Training: Loss:  0.4772, Accuracy: 0.8281\n",
      "Batch number : 369, Training: Loss:  0.4507, Accuracy: 0.8594\n",
      "Batch number : 370, Training: Loss:  0.3772, Accuracy: 0.9219\n",
      "Batch number : 371, Training: Loss:  0.3700, Accuracy: 0.9062\n",
      "Batch number : 372, Training: Loss:  0.3564, Accuracy: 0.8906\n",
      "Batch number : 373, Training: Loss:  0.5514, Accuracy: 0.8281\n",
      "Batch number : 374, Training: Loss:  0.6675, Accuracy: 0.7656\n",
      "Batch number : 375, Training: Loss:  0.4902, Accuracy: 0.8438\n",
      "Batch number : 376, Training: Loss:  0.5841, Accuracy: 0.8125\n",
      "Batch number : 377, Training: Loss:  0.4335, Accuracy: 0.8250\n",
      "Epoch: 5/20\n",
      "Batch number : 000, Training: Loss:  0.5312, Accuracy: 0.8594\n",
      "Batch number : 001, Training: Loss:  0.5180, Accuracy: 0.8750\n",
      "Batch number : 002, Training: Loss:  0.4529, Accuracy: 0.8281\n",
      "Batch number : 003, Training: Loss:  0.4039, Accuracy: 0.8594\n",
      "Batch number : 004, Training: Loss:  0.5555, Accuracy: 0.8750\n",
      "Batch number : 005, Training: Loss:  0.5996, Accuracy: 0.8281\n",
      "Batch number : 006, Training: Loss:  0.7605, Accuracy: 0.7969\n",
      "Batch number : 007, Training: Loss:  0.4395, Accuracy: 0.8906\n",
      "Batch number : 008, Training: Loss:  0.3914, Accuracy: 0.9062\n",
      "Batch number : 009, Training: Loss:  0.6724, Accuracy: 0.7656\n",
      "Batch number : 010, Training: Loss:  0.5480, Accuracy: 0.8125\n",
      "Batch number : 011, Training: Loss:  0.2988, Accuracy: 0.9062\n",
      "Batch number : 012, Training: Loss:  0.4505, Accuracy: 0.8438\n",
      "Batch number : 013, Training: Loss:  0.4727, Accuracy: 0.8125\n",
      "Batch number : 014, Training: Loss:  0.2048, Accuracy: 0.9531\n",
      "Batch number : 015, Training: Loss:  0.3871, Accuracy: 0.8750\n",
      "Batch number : 016, Training: Loss:  0.6234, Accuracy: 0.8281\n",
      "Batch number : 017, Training: Loss:  0.4730, Accuracy: 0.8438\n",
      "Batch number : 018, Training: Loss:  0.4153, Accuracy: 0.8594\n",
      "Batch number : 019, Training: Loss:  0.4130, Accuracy: 0.9062\n",
      "Batch number : 020, Training: Loss:  0.3982, Accuracy: 0.8906\n",
      "Batch number : 021, Training: Loss:  0.4163, Accuracy: 0.8750\n",
      "Batch number : 022, Training: Loss:  0.5216, Accuracy: 0.8438\n",
      "Batch number : 023, Training: Loss:  0.1473, Accuracy: 1.0000\n",
      "Batch number : 024, Training: Loss:  0.4776, Accuracy: 0.8750\n",
      "Batch number : 025, Training: Loss:  0.3097, Accuracy: 0.8906\n",
      "Batch number : 026, Training: Loss:  0.4467, Accuracy: 0.8594\n",
      "Batch number : 027, Training: Loss:  0.5006, Accuracy: 0.8750\n",
      "Batch number : 028, Training: Loss:  0.5023, Accuracy: 0.8594\n",
      "Batch number : 029, Training: Loss:  0.4483, Accuracy: 0.8906\n",
      "Batch number : 030, Training: Loss:  0.2876, Accuracy: 0.9219\n",
      "Batch number : 031, Training: Loss:  0.5564, Accuracy: 0.8125\n",
      "Batch number : 032, Training: Loss:  0.6553, Accuracy: 0.7812\n",
      "Batch number : 033, Training: Loss:  0.4858, Accuracy: 0.8438\n",
      "Batch number : 034, Training: Loss:  0.8119, Accuracy: 0.7500\n",
      "Batch number : 035, Training: Loss:  0.4656, Accuracy: 0.9062\n",
      "Batch number : 036, Training: Loss:  0.5388, Accuracy: 0.8438\n",
      "Batch number : 037, Training: Loss:  0.3402, Accuracy: 0.9688\n",
      "Batch number : 038, Training: Loss:  0.4619, Accuracy: 0.8750\n",
      "Batch number : 039, Training: Loss:  0.4078, Accuracy: 0.8750\n",
      "Batch number : 040, Training: Loss:  0.5705, Accuracy: 0.8438\n",
      "Batch number : 041, Training: Loss:  0.4827, Accuracy: 0.8594\n",
      "Batch number : 042, Training: Loss:  0.3799, Accuracy: 0.8906\n",
      "Batch number : 043, Training: Loss:  0.2476, Accuracy: 0.9219\n",
      "Batch number : 044, Training: Loss:  0.8501, Accuracy: 0.7500\n",
      "Batch number : 045, Training: Loss:  0.3906, Accuracy: 0.8594\n",
      "Batch number : 046, Training: Loss:  0.3728, Accuracy: 0.8906\n",
      "Batch number : 047, Training: Loss:  0.3919, Accuracy: 0.8750\n",
      "Batch number : 048, Training: Loss:  0.4515, Accuracy: 0.8594\n",
      "Batch number : 049, Training: Loss:  0.3175, Accuracy: 0.8906\n",
      "Batch number : 050, Training: Loss:  0.6038, Accuracy: 0.8438\n",
      "Batch number : 051, Training: Loss:  0.5748, Accuracy: 0.8125\n",
      "Batch number : 052, Training: Loss:  0.5784, Accuracy: 0.7969\n",
      "Batch number : 053, Training: Loss:  0.5904, Accuracy: 0.8281\n",
      "Batch number : 054, Training: Loss:  0.5379, Accuracy: 0.8281\n",
      "Batch number : 055, Training: Loss:  0.4917, Accuracy: 0.9062\n",
      "Batch number : 056, Training: Loss:  0.3922, Accuracy: 0.9062\n",
      "Batch number : 057, Training: Loss:  0.3976, Accuracy: 0.8906\n",
      "Batch number : 058, Training: Loss:  0.4366, Accuracy: 0.8594\n",
      "Batch number : 059, Training: Loss:  0.5613, Accuracy: 0.7969\n",
      "Batch number : 060, Training: Loss:  0.6232, Accuracy: 0.8125\n",
      "Batch number : 061, Training: Loss:  0.4424, Accuracy: 0.8750\n",
      "Batch number : 062, Training: Loss:  0.5226, Accuracy: 0.8281\n",
      "Batch number : 063, Training: Loss:  0.4149, Accuracy: 0.8750\n",
      "Batch number : 064, Training: Loss:  0.4355, Accuracy: 0.8750\n",
      "Batch number : 065, Training: Loss:  0.3891, Accuracy: 0.8906\n",
      "Batch number : 066, Training: Loss:  0.2216, Accuracy: 0.9531\n",
      "Batch number : 067, Training: Loss:  0.3357, Accuracy: 0.9219\n",
      "Batch number : 068, Training: Loss:  0.5872, Accuracy: 0.8125\n",
      "Batch number : 069, Training: Loss:  0.5633, Accuracy: 0.8594\n",
      "Batch number : 070, Training: Loss:  0.7389, Accuracy: 0.7656\n",
      "Batch number : 071, Training: Loss:  0.3939, Accuracy: 0.8750\n",
      "Batch number : 072, Training: Loss:  0.4386, Accuracy: 0.8750\n",
      "Batch number : 073, Training: Loss:  0.4090, Accuracy: 0.8750\n",
      "Batch number : 074, Training: Loss:  0.3446, Accuracy: 0.9219\n",
      "Batch number : 075, Training: Loss:  0.3125, Accuracy: 0.9219\n",
      "Batch number : 076, Training: Loss:  0.4089, Accuracy: 0.8594\n",
      "Batch number : 077, Training: Loss:  0.4237, Accuracy: 0.8750\n",
      "Batch number : 078, Training: Loss:  0.3674, Accuracy: 0.8906\n",
      "Batch number : 079, Training: Loss:  0.3770, Accuracy: 0.8906\n",
      "Batch number : 080, Training: Loss:  0.7885, Accuracy: 0.7500\n",
      "Batch number : 081, Training: Loss:  0.8655, Accuracy: 0.7656\n",
      "Batch number : 082, Training: Loss:  0.6450, Accuracy: 0.7812\n",
      "Batch number : 083, Training: Loss:  0.5994, Accuracy: 0.8438\n",
      "Batch number : 084, Training: Loss:  0.4827, Accuracy: 0.8281\n",
      "Batch number : 085, Training: Loss:  0.5199, Accuracy: 0.8438\n",
      "Batch number : 086, Training: Loss:  0.5652, Accuracy: 0.8281\n",
      "Batch number : 087, Training: Loss:  0.5632, Accuracy: 0.8594\n",
      "Batch number : 088, Training: Loss:  0.5817, Accuracy: 0.8438\n",
      "Batch number : 089, Training: Loss:  0.4710, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.3220, Accuracy: 0.9375\n",
      "Batch number : 091, Training: Loss:  0.5451, Accuracy: 0.8281\n",
      "Batch number : 092, Training: Loss:  0.5330, Accuracy: 0.8438\n",
      "Batch number : 093, Training: Loss:  0.5964, Accuracy: 0.8438\n",
      "Batch number : 094, Training: Loss:  0.5310, Accuracy: 0.8281\n",
      "Batch number : 095, Training: Loss:  0.7859, Accuracy: 0.7500\n",
      "Batch number : 096, Training: Loss:  0.5183, Accuracy: 0.8438\n",
      "Batch number : 097, Training: Loss:  0.5097, Accuracy: 0.8438\n",
      "Batch number : 098, Training: Loss:  0.4515, Accuracy: 0.8906\n",
      "Batch number : 099, Training: Loss:  0.4243, Accuracy: 0.8594\n",
      "Batch number : 100, Training: Loss:  0.4651, Accuracy: 0.8750\n",
      "Batch number : 101, Training: Loss:  0.6051, Accuracy: 0.8125\n",
      "Batch number : 102, Training: Loss:  0.4985, Accuracy: 0.8594\n",
      "Batch number : 103, Training: Loss:  0.4984, Accuracy: 0.8750\n",
      "Batch number : 104, Training: Loss:  0.6143, Accuracy: 0.7812\n",
      "Batch number : 105, Training: Loss:  0.4805, Accuracy: 0.8281\n",
      "Batch number : 106, Training: Loss:  0.5523, Accuracy: 0.8594\n",
      "Batch number : 107, Training: Loss:  0.4618, Accuracy: 0.8750\n",
      "Batch number : 108, Training: Loss:  0.3033, Accuracy: 0.9219\n",
      "Batch number : 109, Training: Loss:  0.3675, Accuracy: 0.8906\n",
      "Batch number : 110, Training: Loss:  0.5359, Accuracy: 0.8281\n",
      "Batch number : 111, Training: Loss:  0.6962, Accuracy: 0.8125\n",
      "Batch number : 112, Training: Loss:  0.2944, Accuracy: 0.9062\n",
      "Batch number : 113, Training: Loss:  0.4226, Accuracy: 0.8750\n",
      "Batch number : 114, Training: Loss:  0.3769, Accuracy: 0.9062\n",
      "Batch number : 115, Training: Loss:  0.4387, Accuracy: 0.8906\n",
      "Batch number : 116, Training: Loss:  0.5238, Accuracy: 0.8594\n",
      "Batch number : 117, Training: Loss:  0.6700, Accuracy: 0.7969\n",
      "Batch number : 118, Training: Loss:  0.6010, Accuracy: 0.8281\n",
      "Batch number : 119, Training: Loss:  0.3829, Accuracy: 0.9062\n",
      "Batch number : 120, Training: Loss:  0.3734, Accuracy: 0.8906\n",
      "Batch number : 121, Training: Loss:  0.4843, Accuracy: 0.8750\n",
      "Batch number : 122, Training: Loss:  0.3318, Accuracy: 0.9219\n",
      "Batch number : 123, Training: Loss:  0.5364, Accuracy: 0.8438\n",
      "Batch number : 124, Training: Loss:  0.6370, Accuracy: 0.7969\n",
      "Batch number : 125, Training: Loss:  0.5867, Accuracy: 0.8281\n",
      "Batch number : 126, Training: Loss:  0.4350, Accuracy: 0.8594\n",
      "Batch number : 127, Training: Loss:  0.4160, Accuracy: 0.9062\n",
      "Batch number : 128, Training: Loss:  0.2684, Accuracy: 0.9375\n",
      "Batch number : 129, Training: Loss:  0.3879, Accuracy: 0.8906\n",
      "Batch number : 130, Training: Loss:  0.6689, Accuracy: 0.8125\n",
      "Batch number : 131, Training: Loss:  0.4264, Accuracy: 0.8594\n",
      "Batch number : 132, Training: Loss:  0.4118, Accuracy: 0.9062\n",
      "Batch number : 133, Training: Loss:  0.5999, Accuracy: 0.8438\n",
      "Batch number : 134, Training: Loss:  0.3615, Accuracy: 0.8594\n",
      "Batch number : 135, Training: Loss:  0.6523, Accuracy: 0.8125\n",
      "Batch number : 136, Training: Loss:  0.6546, Accuracy: 0.7969\n",
      "Batch number : 137, Training: Loss:  0.4405, Accuracy: 0.8750\n",
      "Batch number : 138, Training: Loss:  0.3351, Accuracy: 0.9062\n",
      "Batch number : 139, Training: Loss:  0.5952, Accuracy: 0.7969\n",
      "Batch number : 140, Training: Loss:  0.5195, Accuracy: 0.8594\n",
      "Batch number : 141, Training: Loss:  0.5358, Accuracy: 0.8438\n",
      "Batch number : 142, Training: Loss:  0.4874, Accuracy: 0.8594\n",
      "Batch number : 143, Training: Loss:  0.5792, Accuracy: 0.7969\n",
      "Batch number : 144, Training: Loss:  0.5374, Accuracy: 0.8750\n",
      "Batch number : 145, Training: Loss:  0.6213, Accuracy: 0.8281\n",
      "Batch number : 146, Training: Loss:  0.3940, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.4458, Accuracy: 0.8750\n",
      "Batch number : 148, Training: Loss:  0.7346, Accuracy: 0.7344\n",
      "Batch number : 149, Training: Loss:  0.4479, Accuracy: 0.8594\n",
      "Batch number : 150, Training: Loss:  0.3990, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.2932, Accuracy: 0.9219\n",
      "Batch number : 152, Training: Loss:  0.4742, Accuracy: 0.8750\n",
      "Batch number : 153, Training: Loss:  0.2790, Accuracy: 0.9219\n",
      "Batch number : 154, Training: Loss:  0.4935, Accuracy: 0.8750\n",
      "Batch number : 155, Training: Loss:  0.4372, Accuracy: 0.8438\n",
      "Batch number : 156, Training: Loss:  0.5165, Accuracy: 0.8594\n",
      "Batch number : 157, Training: Loss:  0.6269, Accuracy: 0.8125\n",
      "Batch number : 158, Training: Loss:  0.5341, Accuracy: 0.8438\n",
      "Batch number : 159, Training: Loss:  0.5661, Accuracy: 0.8438\n",
      "Batch number : 160, Training: Loss:  0.6960, Accuracy: 0.8438\n",
      "Batch number : 161, Training: Loss:  0.7628, Accuracy: 0.7344\n",
      "Batch number : 162, Training: Loss:  0.3779, Accuracy: 0.9375\n",
      "Batch number : 163, Training: Loss:  0.5457, Accuracy: 0.8594\n",
      "Batch number : 164, Training: Loss:  0.3516, Accuracy: 0.9219\n",
      "Batch number : 165, Training: Loss:  0.5029, Accuracy: 0.8594\n",
      "Batch number : 166, Training: Loss:  0.3457, Accuracy: 0.9062\n",
      "Batch number : 167, Training: Loss:  0.3218, Accuracy: 0.9375\n",
      "Batch number : 168, Training: Loss:  0.4509, Accuracy: 0.8906\n",
      "Batch number : 169, Training: Loss:  0.6956, Accuracy: 0.7812\n",
      "Batch number : 170, Training: Loss:  0.4774, Accuracy: 0.8750\n",
      "Batch number : 171, Training: Loss:  0.2965, Accuracy: 0.9062\n",
      "Batch number : 172, Training: Loss:  0.4188, Accuracy: 0.8906\n",
      "Batch number : 173, Training: Loss:  0.5664, Accuracy: 0.8438\n",
      "Batch number : 174, Training: Loss:  0.5246, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.7094, Accuracy: 0.8125\n",
      "Batch number : 176, Training: Loss:  0.5376, Accuracy: 0.8125\n",
      "Batch number : 177, Training: Loss:  0.5059, Accuracy: 0.8438\n",
      "Batch number : 178, Training: Loss:  0.6284, Accuracy: 0.8281\n",
      "Batch number : 179, Training: Loss:  0.4741, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.4786, Accuracy: 0.8281\n",
      "Batch number : 181, Training: Loss:  0.5351, Accuracy: 0.8281\n",
      "Batch number : 182, Training: Loss:  0.6210, Accuracy: 0.7656\n",
      "Batch number : 183, Training: Loss:  0.5667, Accuracy: 0.8281\n",
      "Batch number : 184, Training: Loss:  0.6146, Accuracy: 0.7969\n",
      "Batch number : 185, Training: Loss:  0.4373, Accuracy: 0.8594\n",
      "Batch number : 186, Training: Loss:  0.3574, Accuracy: 0.9062\n",
      "Batch number : 187, Training: Loss:  0.5935, Accuracy: 0.7969\n",
      "Batch number : 188, Training: Loss:  0.4030, Accuracy: 0.9062\n",
      "Batch number : 189, Training: Loss:  0.5071, Accuracy: 0.8281\n",
      "Batch number : 190, Training: Loss:  0.7480, Accuracy: 0.7656\n",
      "Batch number : 191, Training: Loss:  0.4859, Accuracy: 0.8281\n",
      "Batch number : 192, Training: Loss:  0.5262, Accuracy: 0.8438\n",
      "Batch number : 193, Training: Loss:  0.3644, Accuracy: 0.8750\n",
      "Batch number : 194, Training: Loss:  0.5170, Accuracy: 0.8750\n",
      "Batch number : 195, Training: Loss:  0.3871, Accuracy: 0.9219\n",
      "Batch number : 196, Training: Loss:  0.4191, Accuracy: 0.8594\n",
      "Batch number : 197, Training: Loss:  0.3880, Accuracy: 0.9219\n",
      "Batch number : 198, Training: Loss:  0.7460, Accuracy: 0.7969\n",
      "Batch number : 199, Training: Loss:  0.5398, Accuracy: 0.8594\n",
      "Batch number : 200, Training: Loss:  0.7038, Accuracy: 0.7812\n",
      "Batch number : 201, Training: Loss:  0.5096, Accuracy: 0.8594\n",
      "Batch number : 202, Training: Loss:  0.5219, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.4047, Accuracy: 0.8750\n",
      "Batch number : 204, Training: Loss:  0.4860, Accuracy: 0.8594\n",
      "Batch number : 205, Training: Loss:  0.6877, Accuracy: 0.8125\n",
      "Batch number : 206, Training: Loss:  0.2787, Accuracy: 0.9219\n",
      "Batch number : 207, Training: Loss:  0.4554, Accuracy: 0.8281\n",
      "Batch number : 208, Training: Loss:  0.6277, Accuracy: 0.8438\n",
      "Batch number : 209, Training: Loss:  0.4385, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.5567, Accuracy: 0.8281\n",
      "Batch number : 211, Training: Loss:  0.3922, Accuracy: 0.8750\n",
      "Batch number : 212, Training: Loss:  0.4041, Accuracy: 0.8906\n",
      "Batch number : 213, Training: Loss:  0.4027, Accuracy: 0.8906\n",
      "Batch number : 214, Training: Loss:  0.4861, Accuracy: 0.8594\n",
      "Batch number : 215, Training: Loss:  0.4943, Accuracy: 0.8438\n",
      "Batch number : 216, Training: Loss:  0.3379, Accuracy: 0.8906\n",
      "Batch number : 217, Training: Loss:  0.4467, Accuracy: 0.8594\n",
      "Batch number : 218, Training: Loss:  0.5817, Accuracy: 0.8594\n",
      "Batch number : 219, Training: Loss:  0.3885, Accuracy: 0.8906\n",
      "Batch number : 220, Training: Loss:  0.6299, Accuracy: 0.7969\n",
      "Batch number : 221, Training: Loss:  0.4886, Accuracy: 0.8438\n",
      "Batch number : 222, Training: Loss:  0.5366, Accuracy: 0.8906\n",
      "Batch number : 223, Training: Loss:  0.4072, Accuracy: 0.9219\n",
      "Batch number : 224, Training: Loss:  0.4336, Accuracy: 0.8750\n",
      "Batch number : 225, Training: Loss:  0.4119, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.5672, Accuracy: 0.8281\n",
      "Batch number : 227, Training: Loss:  0.3244, Accuracy: 0.9375\n",
      "Batch number : 228, Training: Loss:  0.6295, Accuracy: 0.7969\n",
      "Batch number : 229, Training: Loss:  0.4035, Accuracy: 0.8438\n",
      "Batch number : 230, Training: Loss:  0.5117, Accuracy: 0.8594\n",
      "Batch number : 231, Training: Loss:  0.4438, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.7415, Accuracy: 0.8125\n",
      "Batch number : 233, Training: Loss:  0.4263, Accuracy: 0.8906\n",
      "Batch number : 234, Training: Loss:  0.5819, Accuracy: 0.7656\n",
      "Batch number : 235, Training: Loss:  0.4156, Accuracy: 0.8281\n",
      "Batch number : 236, Training: Loss:  0.4533, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.3982, Accuracy: 0.8906\n",
      "Batch number : 238, Training: Loss:  0.4754, Accuracy: 0.8594\n",
      "Batch number : 239, Training: Loss:  0.4323, Accuracy: 0.9062\n",
      "Batch number : 240, Training: Loss:  0.2982, Accuracy: 0.9375\n",
      "Batch number : 241, Training: Loss:  0.7155, Accuracy: 0.7812\n",
      "Batch number : 242, Training: Loss:  0.6794, Accuracy: 0.7969\n",
      "Batch number : 243, Training: Loss:  0.6470, Accuracy: 0.8281\n",
      "Batch number : 244, Training: Loss:  0.4453, Accuracy: 0.8438\n",
      "Batch number : 245, Training: Loss:  0.3393, Accuracy: 0.8750\n",
      "Batch number : 246, Training: Loss:  0.2907, Accuracy: 0.9062\n",
      "Batch number : 247, Training: Loss:  0.4441, Accuracy: 0.8750\n",
      "Batch number : 248, Training: Loss:  0.5497, Accuracy: 0.8750\n",
      "Batch number : 249, Training: Loss:  0.5368, Accuracy: 0.8438\n",
      "Batch number : 250, Training: Loss:  0.6755, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.5032, Accuracy: 0.8594\n",
      "Batch number : 252, Training: Loss:  0.3729, Accuracy: 0.8906\n",
      "Batch number : 253, Training: Loss:  0.4565, Accuracy: 0.8438\n",
      "Batch number : 254, Training: Loss:  0.6799, Accuracy: 0.7500\n",
      "Batch number : 255, Training: Loss:  0.6653, Accuracy: 0.8125\n",
      "Batch number : 256, Training: Loss:  0.4835, Accuracy: 0.8594\n",
      "Batch number : 257, Training: Loss:  0.4196, Accuracy: 0.8906\n",
      "Batch number : 258, Training: Loss:  0.5412, Accuracy: 0.8438\n",
      "Batch number : 259, Training: Loss:  0.4357, Accuracy: 0.9062\n",
      "Batch number : 260, Training: Loss:  0.5157, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.5374, Accuracy: 0.8125\n",
      "Batch number : 262, Training: Loss:  0.4499, Accuracy: 0.8750\n",
      "Batch number : 263, Training: Loss:  0.3933, Accuracy: 0.8750\n",
      "Batch number : 264, Training: Loss:  0.5816, Accuracy: 0.8281\n",
      "Batch number : 265, Training: Loss:  0.4260, Accuracy: 0.8750\n",
      "Batch number : 266, Training: Loss:  0.9340, Accuracy: 0.7656\n",
      "Batch number : 267, Training: Loss:  0.3449, Accuracy: 0.9219\n",
      "Batch number : 268, Training: Loss:  0.5432, Accuracy: 0.8438\n",
      "Batch number : 269, Training: Loss:  0.6137, Accuracy: 0.8438\n",
      "Batch number : 270, Training: Loss:  0.4697, Accuracy: 0.8594\n",
      "Batch number : 271, Training: Loss:  0.5385, Accuracy: 0.8906\n",
      "Batch number : 272, Training: Loss:  0.4868, Accuracy: 0.8438\n",
      "Batch number : 273, Training: Loss:  0.7827, Accuracy: 0.7656\n",
      "Batch number : 274, Training: Loss:  0.6559, Accuracy: 0.7969\n",
      "Batch number : 275, Training: Loss:  0.4376, Accuracy: 0.8906\n",
      "Batch number : 276, Training: Loss:  0.5370, Accuracy: 0.8906\n",
      "Batch number : 277, Training: Loss:  0.5473, Accuracy: 0.8594\n",
      "Batch number : 278, Training: Loss:  0.4182, Accuracy: 0.8594\n",
      "Batch number : 279, Training: Loss:  0.4592, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.6084, Accuracy: 0.8750\n",
      "Batch number : 281, Training: Loss:  0.6106, Accuracy: 0.7969\n",
      "Batch number : 282, Training: Loss:  0.4060, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.5653, Accuracy: 0.8594\n",
      "Batch number : 284, Training: Loss:  0.5226, Accuracy: 0.8438\n",
      "Batch number : 285, Training: Loss:  0.5736, Accuracy: 0.8281\n",
      "Batch number : 286, Training: Loss:  0.3684, Accuracy: 0.8750\n",
      "Batch number : 287, Training: Loss:  0.5983, Accuracy: 0.8125\n",
      "Batch number : 288, Training: Loss:  0.4940, Accuracy: 0.8594\n",
      "Batch number : 289, Training: Loss:  0.4608, Accuracy: 0.8594\n",
      "Batch number : 290, Training: Loss:  0.4271, Accuracy: 0.8906\n",
      "Batch number : 291, Training: Loss:  0.4599, Accuracy: 0.8594\n",
      "Batch number : 292, Training: Loss:  0.4352, Accuracy: 0.8906\n",
      "Batch number : 293, Training: Loss:  0.4929, Accuracy: 0.8125\n",
      "Batch number : 294, Training: Loss:  0.5442, Accuracy: 0.8281\n",
      "Batch number : 295, Training: Loss:  0.3783, Accuracy: 0.9062\n",
      "Batch number : 296, Training: Loss:  0.6495, Accuracy: 0.7969\n",
      "Batch number : 297, Training: Loss:  0.4958, Accuracy: 0.8438\n",
      "Batch number : 298, Training: Loss:  0.2518, Accuracy: 0.9375\n",
      "Batch number : 299, Training: Loss:  0.4425, Accuracy: 0.8438\n",
      "Batch number : 300, Training: Loss:  0.3516, Accuracy: 0.9062\n",
      "Batch number : 301, Training: Loss:  0.5673, Accuracy: 0.8438\n",
      "Batch number : 302, Training: Loss:  0.5066, Accuracy: 0.8906\n",
      "Batch number : 303, Training: Loss:  0.4765, Accuracy: 0.8750\n",
      "Batch number : 304, Training: Loss:  0.2887, Accuracy: 0.9062\n",
      "Batch number : 305, Training: Loss:  0.2912, Accuracy: 0.9375\n",
      "Batch number : 306, Training: Loss:  0.5314, Accuracy: 0.8906\n",
      "Batch number : 307, Training: Loss:  0.9286, Accuracy: 0.7344\n",
      "Batch number : 308, Training: Loss:  0.3860, Accuracy: 0.9062\n",
      "Batch number : 309, Training: Loss:  0.5059, Accuracy: 0.8438\n",
      "Batch number : 310, Training: Loss:  0.5986, Accuracy: 0.8125\n",
      "Batch number : 311, Training: Loss:  0.5128, Accuracy: 0.8594\n",
      "Batch number : 312, Training: Loss:  0.4309, Accuracy: 0.8750\n",
      "Batch number : 313, Training: Loss:  0.2600, Accuracy: 0.9531\n",
      "Batch number : 314, Training: Loss:  0.5201, Accuracy: 0.8594\n",
      "Batch number : 315, Training: Loss:  0.5219, Accuracy: 0.8594\n",
      "Batch number : 316, Training: Loss:  0.5328, Accuracy: 0.8438\n",
      "Batch number : 317, Training: Loss:  0.5684, Accuracy: 0.8438\n",
      "Batch number : 318, Training: Loss:  0.4744, Accuracy: 0.8750\n",
      "Batch number : 319, Training: Loss:  0.4485, Accuracy: 0.8750\n",
      "Batch number : 320, Training: Loss:  0.3161, Accuracy: 0.9062\n",
      "Batch number : 321, Training: Loss:  0.3322, Accuracy: 0.9062\n",
      "Batch number : 322, Training: Loss:  0.5424, Accuracy: 0.8281\n",
      "Batch number : 323, Training: Loss:  0.3050, Accuracy: 0.9219\n",
      "Batch number : 324, Training: Loss:  0.5875, Accuracy: 0.8438\n",
      "Batch number : 325, Training: Loss:  0.4077, Accuracy: 0.8750\n",
      "Batch number : 326, Training: Loss:  0.5272, Accuracy: 0.8281\n",
      "Batch number : 327, Training: Loss:  0.4919, Accuracy: 0.8438\n",
      "Batch number : 328, Training: Loss:  0.6949, Accuracy: 0.7812\n",
      "Batch number : 329, Training: Loss:  0.5309, Accuracy: 0.8594\n",
      "Batch number : 330, Training: Loss:  0.4834, Accuracy: 0.8594\n",
      "Batch number : 331, Training: Loss:  0.5894, Accuracy: 0.7812\n",
      "Batch number : 332, Training: Loss:  0.4719, Accuracy: 0.8906\n",
      "Batch number : 333, Training: Loss:  0.4904, Accuracy: 0.8281\n",
      "Batch number : 334, Training: Loss:  0.4298, Accuracy: 0.8750\n",
      "Batch number : 335, Training: Loss:  0.4430, Accuracy: 0.8906\n",
      "Batch number : 336, Training: Loss:  0.4956, Accuracy: 0.8594\n",
      "Batch number : 337, Training: Loss:  0.6573, Accuracy: 0.7969\n",
      "Batch number : 338, Training: Loss:  0.3701, Accuracy: 0.8594\n",
      "Batch number : 339, Training: Loss:  0.6341, Accuracy: 0.7812\n",
      "Batch number : 340, Training: Loss:  0.4303, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.5491, Accuracy: 0.8281\n",
      "Batch number : 342, Training: Loss:  0.3251, Accuracy: 0.8906\n",
      "Batch number : 343, Training: Loss:  0.4407, Accuracy: 0.8750\n",
      "Batch number : 344, Training: Loss:  0.4598, Accuracy: 0.8594\n",
      "Batch number : 345, Training: Loss:  0.3817, Accuracy: 0.8750\n",
      "Batch number : 346, Training: Loss:  0.6322, Accuracy: 0.8750\n",
      "Batch number : 347, Training: Loss:  0.7524, Accuracy: 0.7969\n",
      "Batch number : 348, Training: Loss:  0.4599, Accuracy: 0.8281\n",
      "Batch number : 349, Training: Loss:  0.5726, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.4214, Accuracy: 0.8750\n",
      "Batch number : 351, Training: Loss:  0.4753, Accuracy: 0.8594\n",
      "Batch number : 352, Training: Loss:  0.4352, Accuracy: 0.8438\n",
      "Batch number : 353, Training: Loss:  0.6191, Accuracy: 0.8125\n",
      "Batch number : 354, Training: Loss:  0.5626, Accuracy: 0.8594\n",
      "Batch number : 355, Training: Loss:  0.3959, Accuracy: 0.8906\n",
      "Batch number : 356, Training: Loss:  0.6674, Accuracy: 0.7500\n",
      "Batch number : 357, Training: Loss:  0.5898, Accuracy: 0.7656\n",
      "Batch number : 358, Training: Loss:  0.3793, Accuracy: 0.9062\n",
      "Batch number : 359, Training: Loss:  0.4674, Accuracy: 0.8594\n",
      "Batch number : 360, Training: Loss:  0.6137, Accuracy: 0.8281\n",
      "Batch number : 361, Training: Loss:  0.4267, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.4023, Accuracy: 0.8750\n",
      "Batch number : 363, Training: Loss:  0.4564, Accuracy: 0.8750\n",
      "Batch number : 364, Training: Loss:  0.6189, Accuracy: 0.8281\n",
      "Batch number : 365, Training: Loss:  0.4294, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.3899, Accuracy: 0.9062\n",
      "Batch number : 367, Training: Loss:  0.3335, Accuracy: 0.9062\n",
      "Batch number : 368, Training: Loss:  0.4592, Accuracy: 0.9062\n",
      "Batch number : 369, Training: Loss:  0.5808, Accuracy: 0.8281\n",
      "Batch number : 370, Training: Loss:  0.7714, Accuracy: 0.7656\n",
      "Batch number : 371, Training: Loss:  0.3279, Accuracy: 0.9062\n",
      "Batch number : 372, Training: Loss:  0.5540, Accuracy: 0.8125\n",
      "Batch number : 373, Training: Loss:  0.3418, Accuracy: 0.9062\n",
      "Batch number : 374, Training: Loss:  0.6785, Accuracy: 0.7969\n",
      "Batch number : 375, Training: Loss:  0.5376, Accuracy: 0.8125\n",
      "Batch number : 376, Training: Loss:  0.5207, Accuracy: 0.8438\n",
      "Batch number : 377, Training: Loss:  0.4021, Accuracy: 0.9000\n",
      "Epoch: 6/20\n",
      "Batch number : 000, Training: Loss:  0.5767, Accuracy: 0.7812\n",
      "Batch number : 001, Training: Loss:  0.4044, Accuracy: 0.9062\n",
      "Batch number : 002, Training: Loss:  0.6123, Accuracy: 0.8125\n",
      "Batch number : 003, Training: Loss:  0.4758, Accuracy: 0.7969\n",
      "Batch number : 004, Training: Loss:  0.5084, Accuracy: 0.8750\n",
      "Batch number : 005, Training: Loss:  0.6615, Accuracy: 0.8125\n",
      "Batch number : 006, Training: Loss:  0.3363, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.4927, Accuracy: 0.8906\n",
      "Batch number : 008, Training: Loss:  0.2861, Accuracy: 0.9375\n",
      "Batch number : 009, Training: Loss:  0.5283, Accuracy: 0.8594\n",
      "Batch number : 010, Training: Loss:  0.6266, Accuracy: 0.8281\n",
      "Batch number : 011, Training: Loss:  0.3331, Accuracy: 0.9062\n",
      "Batch number : 012, Training: Loss:  0.3931, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.3679, Accuracy: 0.8906\n",
      "Batch number : 014, Training: Loss:  0.6921, Accuracy: 0.8281\n",
      "Batch number : 015, Training: Loss:  0.5201, Accuracy: 0.8594\n",
      "Batch number : 016, Training: Loss:  0.3570, Accuracy: 0.9062\n",
      "Batch number : 017, Training: Loss:  0.5398, Accuracy: 0.8281\n",
      "Batch number : 018, Training: Loss:  0.6496, Accuracy: 0.7812\n",
      "Batch number : 019, Training: Loss:  0.4538, Accuracy: 0.9062\n",
      "Batch number : 020, Training: Loss:  0.4323, Accuracy: 0.8906\n",
      "Batch number : 021, Training: Loss:  0.4557, Accuracy: 0.8750\n",
      "Batch number : 022, Training: Loss:  0.4168, Accuracy: 0.8750\n",
      "Batch number : 023, Training: Loss:  0.6504, Accuracy: 0.7969\n",
      "Batch number : 024, Training: Loss:  0.3709, Accuracy: 0.8750\n",
      "Batch number : 025, Training: Loss:  0.5393, Accuracy: 0.8281\n",
      "Batch number : 026, Training: Loss:  0.6274, Accuracy: 0.8281\n",
      "Batch number : 027, Training: Loss:  0.3545, Accuracy: 0.8906\n",
      "Batch number : 028, Training: Loss:  0.3507, Accuracy: 0.9062\n",
      "Batch number : 029, Training: Loss:  0.6671, Accuracy: 0.7969\n",
      "Batch number : 030, Training: Loss:  0.4555, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.3439, Accuracy: 0.9062\n",
      "Batch number : 032, Training: Loss:  0.4884, Accuracy: 0.8594\n",
      "Batch number : 033, Training: Loss:  0.5360, Accuracy: 0.8281\n",
      "Batch number : 034, Training: Loss:  0.4999, Accuracy: 0.8594\n",
      "Batch number : 035, Training: Loss:  0.3653, Accuracy: 0.8750\n",
      "Batch number : 036, Training: Loss:  0.5397, Accuracy: 0.8281\n",
      "Batch number : 037, Training: Loss:  0.3892, Accuracy: 0.8438\n",
      "Batch number : 038, Training: Loss:  0.3139, Accuracy: 0.9219\n",
      "Batch number : 039, Training: Loss:  0.6049, Accuracy: 0.8125\n",
      "Batch number : 040, Training: Loss:  0.4830, Accuracy: 0.8594\n",
      "Batch number : 041, Training: Loss:  0.5092, Accuracy: 0.8125\n",
      "Batch number : 042, Training: Loss:  0.4403, Accuracy: 0.8594\n",
      "Batch number : 043, Training: Loss:  0.3930, Accuracy: 0.8906\n",
      "Batch number : 044, Training: Loss:  0.4589, Accuracy: 0.8594\n",
      "Batch number : 045, Training: Loss:  0.3548, Accuracy: 0.9062\n",
      "Batch number : 046, Training: Loss:  0.7347, Accuracy: 0.7812\n",
      "Batch number : 047, Training: Loss:  0.4848, Accuracy: 0.8438\n",
      "Batch number : 048, Training: Loss:  0.5165, Accuracy: 0.8125\n",
      "Batch number : 049, Training: Loss:  0.6420, Accuracy: 0.7969\n",
      "Batch number : 050, Training: Loss:  0.3932, Accuracy: 0.8906\n",
      "Batch number : 051, Training: Loss:  0.3598, Accuracy: 0.9062\n",
      "Batch number : 052, Training: Loss:  0.3764, Accuracy: 0.8594\n",
      "Batch number : 053, Training: Loss:  0.5720, Accuracy: 0.8281\n",
      "Batch number : 054, Training: Loss:  0.4867, Accuracy: 0.8906\n",
      "Batch number : 055, Training: Loss:  0.4265, Accuracy: 0.8594\n",
      "Batch number : 056, Training: Loss:  0.6927, Accuracy: 0.8125\n",
      "Batch number : 057, Training: Loss:  0.3973, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.7330, Accuracy: 0.8125\n",
      "Batch number : 059, Training: Loss:  0.4868, Accuracy: 0.8750\n",
      "Batch number : 060, Training: Loss:  0.3830, Accuracy: 0.9062\n",
      "Batch number : 061, Training: Loss:  0.4031, Accuracy: 0.8750\n",
      "Batch number : 062, Training: Loss:  0.7079, Accuracy: 0.8281\n",
      "Batch number : 063, Training: Loss:  0.4424, Accuracy: 0.8594\n",
      "Batch number : 064, Training: Loss:  0.3721, Accuracy: 0.8750\n",
      "Batch number : 065, Training: Loss:  0.5081, Accuracy: 0.8906\n",
      "Batch number : 066, Training: Loss:  0.5373, Accuracy: 0.8594\n",
      "Batch number : 067, Training: Loss:  0.3624, Accuracy: 0.8906\n",
      "Batch number : 068, Training: Loss:  0.4051, Accuracy: 0.8750\n",
      "Batch number : 069, Training: Loss:  0.5279, Accuracy: 0.8594\n",
      "Batch number : 070, Training: Loss:  0.5479, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.4382, Accuracy: 0.8594\n",
      "Batch number : 072, Training: Loss:  0.4749, Accuracy: 0.8594\n",
      "Batch number : 073, Training: Loss:  0.2871, Accuracy: 0.9219\n",
      "Batch number : 074, Training: Loss:  0.3704, Accuracy: 0.8906\n",
      "Batch number : 075, Training: Loss:  0.5604, Accuracy: 0.8281\n",
      "Batch number : 076, Training: Loss:  0.3839, Accuracy: 0.8750\n",
      "Batch number : 077, Training: Loss:  0.4647, Accuracy: 0.8281\n",
      "Batch number : 078, Training: Loss:  0.5735, Accuracy: 0.8125\n",
      "Batch number : 079, Training: Loss:  0.4775, Accuracy: 0.8594\n",
      "Batch number : 080, Training: Loss:  0.4385, Accuracy: 0.8750\n",
      "Batch number : 081, Training: Loss:  0.3896, Accuracy: 0.9062\n",
      "Batch number : 082, Training: Loss:  0.3181, Accuracy: 0.9062\n",
      "Batch number : 083, Training: Loss:  0.6208, Accuracy: 0.7812\n",
      "Batch number : 084, Training: Loss:  0.4234, Accuracy: 0.8438\n",
      "Batch number : 085, Training: Loss:  0.5759, Accuracy: 0.8281\n",
      "Batch number : 086, Training: Loss:  0.3971, Accuracy: 0.8594\n",
      "Batch number : 087, Training: Loss:  0.6880, Accuracy: 0.8281\n",
      "Batch number : 088, Training: Loss:  0.4016, Accuracy: 0.8750\n",
      "Batch number : 089, Training: Loss:  0.6291, Accuracy: 0.7812\n",
      "Batch number : 090, Training: Loss:  0.4656, Accuracy: 0.8594\n",
      "Batch number : 091, Training: Loss:  0.6635, Accuracy: 0.7969\n",
      "Batch number : 092, Training: Loss:  0.6213, Accuracy: 0.8281\n",
      "Batch number : 093, Training: Loss:  0.5086, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.6981, Accuracy: 0.7656\n",
      "Batch number : 095, Training: Loss:  0.4584, Accuracy: 0.8594\n",
      "Batch number : 096, Training: Loss:  0.6056, Accuracy: 0.8125\n",
      "Batch number : 097, Training: Loss:  0.3040, Accuracy: 0.9219\n",
      "Batch number : 098, Training: Loss:  0.6947, Accuracy: 0.7812\n",
      "Batch number : 099, Training: Loss:  0.5178, Accuracy: 0.8125\n",
      "Batch number : 100, Training: Loss:  0.6435, Accuracy: 0.7969\n",
      "Batch number : 101, Training: Loss:  0.5485, Accuracy: 0.8281\n",
      "Batch number : 102, Training: Loss:  0.4833, Accuracy: 0.8438\n",
      "Batch number : 103, Training: Loss:  0.4078, Accuracy: 0.8906\n",
      "Batch number : 104, Training: Loss:  0.5333, Accuracy: 0.8594\n",
      "Batch number : 105, Training: Loss:  0.6529, Accuracy: 0.8438\n",
      "Batch number : 106, Training: Loss:  0.6241, Accuracy: 0.8281\n",
      "Batch number : 107, Training: Loss:  0.3478, Accuracy: 0.8906\n",
      "Batch number : 108, Training: Loss:  0.4481, Accuracy: 0.8281\n",
      "Batch number : 109, Training: Loss:  0.7436, Accuracy: 0.8125\n",
      "Batch number : 110, Training: Loss:  0.5717, Accuracy: 0.8125\n",
      "Batch number : 111, Training: Loss:  0.5319, Accuracy: 0.8750\n",
      "Batch number : 112, Training: Loss:  0.6184, Accuracy: 0.7969\n",
      "Batch number : 113, Training: Loss:  0.4040, Accuracy: 0.8750\n",
      "Batch number : 114, Training: Loss:  0.5082, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.5773, Accuracy: 0.8125\n",
      "Batch number : 116, Training: Loss:  0.3772, Accuracy: 0.9219\n",
      "Batch number : 117, Training: Loss:  0.3319, Accuracy: 0.9375\n",
      "Batch number : 118, Training: Loss:  0.6300, Accuracy: 0.7500\n",
      "Batch number : 119, Training: Loss:  0.3511, Accuracy: 0.8750\n",
      "Batch number : 120, Training: Loss:  0.8312, Accuracy: 0.7812\n",
      "Batch number : 121, Training: Loss:  0.4820, Accuracy: 0.8906\n",
      "Batch number : 122, Training: Loss:  0.4541, Accuracy: 0.8750\n",
      "Batch number : 123, Training: Loss:  0.7364, Accuracy: 0.7969\n",
      "Batch number : 124, Training: Loss:  0.5589, Accuracy: 0.8281\n",
      "Batch number : 125, Training: Loss:  0.5336, Accuracy: 0.8281\n",
      "Batch number : 126, Training: Loss:  0.5768, Accuracy: 0.8281\n",
      "Batch number : 127, Training: Loss:  0.7523, Accuracy: 0.7344\n",
      "Batch number : 128, Training: Loss:  0.3358, Accuracy: 0.9375\n",
      "Batch number : 129, Training: Loss:  0.3390, Accuracy: 0.9531\n",
      "Batch number : 130, Training: Loss:  0.4397, Accuracy: 0.9062\n",
      "Batch number : 131, Training: Loss:  0.4441, Accuracy: 0.8750\n",
      "Batch number : 132, Training: Loss:  0.5067, Accuracy: 0.8594\n",
      "Batch number : 133, Training: Loss:  0.4488, Accuracy: 0.8906\n",
      "Batch number : 134, Training: Loss:  0.3423, Accuracy: 0.8750\n",
      "Batch number : 135, Training: Loss:  0.3880, Accuracy: 0.9062\n",
      "Batch number : 136, Training: Loss:  0.5354, Accuracy: 0.8438\n",
      "Batch number : 137, Training: Loss:  0.3922, Accuracy: 0.8906\n",
      "Batch number : 138, Training: Loss:  0.4381, Accuracy: 0.8594\n",
      "Batch number : 139, Training: Loss:  0.6345, Accuracy: 0.8438\n",
      "Batch number : 140, Training: Loss:  0.6453, Accuracy: 0.8125\n",
      "Batch number : 141, Training: Loss:  0.5269, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.3992, Accuracy: 0.8906\n",
      "Batch number : 143, Training: Loss:  0.4470, Accuracy: 0.8594\n",
      "Batch number : 144, Training: Loss:  0.7368, Accuracy: 0.7656\n",
      "Batch number : 145, Training: Loss:  0.5990, Accuracy: 0.8438\n",
      "Batch number : 146, Training: Loss:  0.4646, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.4259, Accuracy: 0.9062\n",
      "Batch number : 148, Training: Loss:  0.6104, Accuracy: 0.7969\n",
      "Batch number : 149, Training: Loss:  0.4907, Accuracy: 0.8906\n",
      "Batch number : 150, Training: Loss:  0.3229, Accuracy: 0.9219\n",
      "Batch number : 151, Training: Loss:  0.3227, Accuracy: 0.9219\n",
      "Batch number : 152, Training: Loss:  0.3405, Accuracy: 0.8906\n",
      "Batch number : 153, Training: Loss:  0.4220, Accuracy: 0.8750\n",
      "Batch number : 154, Training: Loss:  0.4397, Accuracy: 0.8594\n",
      "Batch number : 155, Training: Loss:  0.4912, Accuracy: 0.8594\n",
      "Batch number : 156, Training: Loss:  0.5690, Accuracy: 0.8125\n",
      "Batch number : 157, Training: Loss:  0.5897, Accuracy: 0.8125\n",
      "Batch number : 158, Training: Loss:  0.3553, Accuracy: 0.8750\n",
      "Batch number : 159, Training: Loss:  0.1872, Accuracy: 0.9531\n",
      "Batch number : 160, Training: Loss:  0.4233, Accuracy: 0.8750\n",
      "Batch number : 161, Training: Loss:  0.4803, Accuracy: 0.8594\n",
      "Batch number : 162, Training: Loss:  0.3754, Accuracy: 0.8906\n",
      "Batch number : 163, Training: Loss:  0.5065, Accuracy: 0.8750\n",
      "Batch number : 164, Training: Loss:  0.2842, Accuracy: 0.9375\n",
      "Batch number : 165, Training: Loss:  0.3613, Accuracy: 0.8594\n",
      "Batch number : 166, Training: Loss:  0.8084, Accuracy: 0.8125\n",
      "Batch number : 167, Training: Loss:  0.4801, Accuracy: 0.8281\n",
      "Batch number : 168, Training: Loss:  0.4202, Accuracy: 0.8438\n",
      "Batch number : 169, Training: Loss:  0.5854, Accuracy: 0.8125\n",
      "Batch number : 170, Training: Loss:  0.6546, Accuracy: 0.7344\n",
      "Batch number : 171, Training: Loss:  0.5451, Accuracy: 0.8125\n",
      "Batch number : 172, Training: Loss:  0.7076, Accuracy: 0.7969\n",
      "Batch number : 173, Training: Loss:  0.3498, Accuracy: 0.9219\n",
      "Batch number : 174, Training: Loss:  0.3759, Accuracy: 0.8750\n",
      "Batch number : 175, Training: Loss:  0.3636, Accuracy: 0.8750\n",
      "Batch number : 176, Training: Loss:  0.6660, Accuracy: 0.7344\n",
      "Batch number : 177, Training: Loss:  0.6107, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.6714, Accuracy: 0.7344\n",
      "Batch number : 179, Training: Loss:  0.6338, Accuracy: 0.8281\n",
      "Batch number : 180, Training: Loss:  0.3820, Accuracy: 0.8750\n",
      "Batch number : 181, Training: Loss:  0.4382, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.4066, Accuracy: 0.8906\n",
      "Batch number : 183, Training: Loss:  0.7191, Accuracy: 0.7969\n",
      "Batch number : 184, Training: Loss:  0.3592, Accuracy: 0.9531\n",
      "Batch number : 185, Training: Loss:  0.4273, Accuracy: 0.8906\n",
      "Batch number : 186, Training: Loss:  0.4769, Accuracy: 0.8906\n",
      "Batch number : 187, Training: Loss:  0.3431, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.5828, Accuracy: 0.7969\n",
      "Batch number : 189, Training: Loss:  0.7894, Accuracy: 0.7344\n",
      "Batch number : 190, Training: Loss:  0.4556, Accuracy: 0.8594\n",
      "Batch number : 191, Training: Loss:  0.4025, Accuracy: 0.8906\n",
      "Batch number : 192, Training: Loss:  0.4242, Accuracy: 0.8594\n",
      "Batch number : 193, Training: Loss:  0.6192, Accuracy: 0.7656\n",
      "Batch number : 194, Training: Loss:  0.4339, Accuracy: 0.8594\n",
      "Batch number : 195, Training: Loss:  0.4480, Accuracy: 0.8750\n",
      "Batch number : 196, Training: Loss:  0.5691, Accuracy: 0.8438\n",
      "Batch number : 197, Training: Loss:  0.4663, Accuracy: 0.9062\n",
      "Batch number : 198, Training: Loss:  0.5067, Accuracy: 0.8594\n",
      "Batch number : 199, Training: Loss:  0.4561, Accuracy: 0.8750\n",
      "Batch number : 200, Training: Loss:  0.5323, Accuracy: 0.8125\n",
      "Batch number : 201, Training: Loss:  0.4892, Accuracy: 0.8438\n",
      "Batch number : 202, Training: Loss:  0.4190, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.5443, Accuracy: 0.8125\n",
      "Batch number : 204, Training: Loss:  0.3850, Accuracy: 0.8906\n",
      "Batch number : 205, Training: Loss:  0.6677, Accuracy: 0.7969\n",
      "Batch number : 206, Training: Loss:  0.4754, Accuracy: 0.8438\n",
      "Batch number : 207, Training: Loss:  0.4237, Accuracy: 0.8750\n",
      "Batch number : 208, Training: Loss:  0.4395, Accuracy: 0.9062\n",
      "Batch number : 209, Training: Loss:  0.2480, Accuracy: 0.9375\n",
      "Batch number : 210, Training: Loss:  0.2732, Accuracy: 0.9219\n",
      "Batch number : 211, Training: Loss:  0.6218, Accuracy: 0.8281\n",
      "Batch number : 212, Training: Loss:  0.5720, Accuracy: 0.8125\n",
      "Batch number : 213, Training: Loss:  0.4030, Accuracy: 0.8594\n",
      "Batch number : 214, Training: Loss:  0.4838, Accuracy: 0.8750\n",
      "Batch number : 215, Training: Loss:  0.3668, Accuracy: 0.9062\n",
      "Batch number : 216, Training: Loss:  0.7843, Accuracy: 0.7656\n",
      "Batch number : 217, Training: Loss:  0.4175, Accuracy: 0.8594\n",
      "Batch number : 218, Training: Loss:  0.3740, Accuracy: 0.8750\n",
      "Batch number : 219, Training: Loss:  0.4596, Accuracy: 0.8750\n",
      "Batch number : 220, Training: Loss:  0.3065, Accuracy: 0.9219\n",
      "Batch number : 221, Training: Loss:  0.3581, Accuracy: 0.8906\n",
      "Batch number : 222, Training: Loss:  0.5259, Accuracy: 0.8438\n",
      "Batch number : 223, Training: Loss:  0.6825, Accuracy: 0.7969\n",
      "Batch number : 224, Training: Loss:  0.5916, Accuracy: 0.7969\n",
      "Batch number : 225, Training: Loss:  0.5197, Accuracy: 0.8438\n",
      "Batch number : 226, Training: Loss:  0.5867, Accuracy: 0.8438\n",
      "Batch number : 227, Training: Loss:  0.3772, Accuracy: 0.9062\n",
      "Batch number : 228, Training: Loss:  0.3259, Accuracy: 0.9062\n",
      "Batch number : 229, Training: Loss:  0.4666, Accuracy: 0.8750\n",
      "Batch number : 230, Training: Loss:  0.3680, Accuracy: 0.9062\n",
      "Batch number : 231, Training: Loss:  0.6203, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.4051, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.4172, Accuracy: 0.8750\n",
      "Batch number : 234, Training: Loss:  0.2999, Accuracy: 0.9531\n",
      "Batch number : 235, Training: Loss:  0.6060, Accuracy: 0.8281\n",
      "Batch number : 236, Training: Loss:  0.4406, Accuracy: 0.8750\n",
      "Batch number : 237, Training: Loss:  0.3821, Accuracy: 0.8906\n",
      "Batch number : 238, Training: Loss:  0.5192, Accuracy: 0.8281\n",
      "Batch number : 239, Training: Loss:  0.5153, Accuracy: 0.8281\n",
      "Batch number : 240, Training: Loss:  0.4859, Accuracy: 0.8594\n",
      "Batch number : 241, Training: Loss:  0.3733, Accuracy: 0.9062\n",
      "Batch number : 242, Training: Loss:  0.3016, Accuracy: 0.9219\n",
      "Batch number : 243, Training: Loss:  0.4298, Accuracy: 0.8438\n",
      "Batch number : 244, Training: Loss:  0.2542, Accuracy: 0.9531\n",
      "Batch number : 245, Training: Loss:  0.5950, Accuracy: 0.7969\n",
      "Batch number : 246, Training: Loss:  0.5554, Accuracy: 0.7969\n",
      "Batch number : 247, Training: Loss:  0.5750, Accuracy: 0.8281\n",
      "Batch number : 248, Training: Loss:  0.5268, Accuracy: 0.7969\n",
      "Batch number : 249, Training: Loss:  0.3774, Accuracy: 0.8906\n",
      "Batch number : 250, Training: Loss:  0.5234, Accuracy: 0.8438\n",
      "Batch number : 251, Training: Loss:  0.5341, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.3222, Accuracy: 0.9062\n",
      "Batch number : 253, Training: Loss:  0.7142, Accuracy: 0.8281\n",
      "Batch number : 254, Training: Loss:  0.4635, Accuracy: 0.8594\n",
      "Batch number : 255, Training: Loss:  0.5929, Accuracy: 0.7812\n",
      "Batch number : 256, Training: Loss:  0.7146, Accuracy: 0.8125\n",
      "Batch number : 257, Training: Loss:  0.2950, Accuracy: 0.9688\n",
      "Batch number : 258, Training: Loss:  0.4392, Accuracy: 0.9062\n",
      "Batch number : 259, Training: Loss:  0.3287, Accuracy: 0.8750\n",
      "Batch number : 260, Training: Loss:  0.3519, Accuracy: 0.9062\n",
      "Batch number : 261, Training: Loss:  0.4098, Accuracy: 0.8750\n",
      "Batch number : 262, Training: Loss:  0.7146, Accuracy: 0.7969\n",
      "Batch number : 263, Training: Loss:  0.3496, Accuracy: 0.9062\n",
      "Batch number : 264, Training: Loss:  0.3065, Accuracy: 0.9062\n",
      "Batch number : 265, Training: Loss:  0.6272, Accuracy: 0.8125\n",
      "Batch number : 266, Training: Loss:  0.5595, Accuracy: 0.8438\n",
      "Batch number : 267, Training: Loss:  0.6497, Accuracy: 0.7812\n",
      "Batch number : 268, Training: Loss:  0.4073, Accuracy: 0.8750\n",
      "Batch number : 269, Training: Loss:  0.4321, Accuracy: 0.8594\n",
      "Batch number : 270, Training: Loss:  0.4874, Accuracy: 0.7969\n",
      "Batch number : 271, Training: Loss:  0.6170, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.5012, Accuracy: 0.8594\n",
      "Batch number : 273, Training: Loss:  0.4955, Accuracy: 0.8594\n",
      "Batch number : 274, Training: Loss:  0.5106, Accuracy: 0.8438\n",
      "Batch number : 275, Training: Loss:  0.5888, Accuracy: 0.7812\n",
      "Batch number : 276, Training: Loss:  0.5208, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.4455, Accuracy: 0.8750\n",
      "Batch number : 278, Training: Loss:  0.4065, Accuracy: 0.9219\n",
      "Batch number : 279, Training: Loss:  0.4832, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.4983, Accuracy: 0.8594\n",
      "Batch number : 281, Training: Loss:  0.5040, Accuracy: 0.8594\n",
      "Batch number : 282, Training: Loss:  0.3075, Accuracy: 0.9219\n",
      "Batch number : 283, Training: Loss:  0.4193, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.6431, Accuracy: 0.8594\n",
      "Batch number : 285, Training: Loss:  0.5761, Accuracy: 0.8438\n",
      "Batch number : 286, Training: Loss:  0.3584, Accuracy: 0.8906\n",
      "Batch number : 287, Training: Loss:  0.7361, Accuracy: 0.8125\n",
      "Batch number : 288, Training: Loss:  0.5508, Accuracy: 0.9062\n",
      "Batch number : 289, Training: Loss:  0.5844, Accuracy: 0.7969\n",
      "Batch number : 290, Training: Loss:  0.4340, Accuracy: 0.8750\n",
      "Batch number : 291, Training: Loss:  0.7307, Accuracy: 0.7656\n",
      "Batch number : 292, Training: Loss:  0.4440, Accuracy: 0.8750\n",
      "Batch number : 293, Training: Loss:  0.4695, Accuracy: 0.8750\n",
      "Batch number : 294, Training: Loss:  0.3847, Accuracy: 0.8906\n",
      "Batch number : 295, Training: Loss:  0.4553, Accuracy: 0.8750\n",
      "Batch number : 296, Training: Loss:  0.3681, Accuracy: 0.8906\n",
      "Batch number : 297, Training: Loss:  0.5223, Accuracy: 0.8906\n",
      "Batch number : 298, Training: Loss:  0.5466, Accuracy: 0.8438\n",
      "Batch number : 299, Training: Loss:  0.4062, Accuracy: 0.9062\n",
      "Batch number : 300, Training: Loss:  0.4123, Accuracy: 0.9062\n",
      "Batch number : 301, Training: Loss:  0.4195, Accuracy: 0.9062\n",
      "Batch number : 302, Training: Loss:  0.4151, Accuracy: 0.8594\n",
      "Batch number : 303, Training: Loss:  0.1953, Accuracy: 0.9688\n",
      "Batch number : 304, Training: Loss:  0.4286, Accuracy: 0.8594\n",
      "Batch number : 305, Training: Loss:  0.2794, Accuracy: 0.9219\n",
      "Batch number : 306, Training: Loss:  0.3461, Accuracy: 0.9219\n",
      "Batch number : 307, Training: Loss:  0.8196, Accuracy: 0.7656\n",
      "Batch number : 308, Training: Loss:  0.3129, Accuracy: 0.9219\n",
      "Batch number : 309, Training: Loss:  0.2941, Accuracy: 0.8750\n",
      "Batch number : 310, Training: Loss:  0.5092, Accuracy: 0.8438\n",
      "Batch number : 311, Training: Loss:  0.4721, Accuracy: 0.8750\n",
      "Batch number : 312, Training: Loss:  0.2851, Accuracy: 0.9062\n",
      "Batch number : 313, Training: Loss:  0.5478, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.6125, Accuracy: 0.8281\n",
      "Batch number : 315, Training: Loss:  0.4127, Accuracy: 0.8906\n",
      "Batch number : 316, Training: Loss:  0.4786, Accuracy: 0.8594\n",
      "Batch number : 317, Training: Loss:  0.5501, Accuracy: 0.8750\n",
      "Batch number : 318, Training: Loss:  0.7115, Accuracy: 0.7969\n",
      "Batch number : 319, Training: Loss:  0.4020, Accuracy: 0.8750\n",
      "Batch number : 320, Training: Loss:  0.3918, Accuracy: 0.9219\n",
      "Batch number : 321, Training: Loss:  0.4769, Accuracy: 0.8438\n",
      "Batch number : 322, Training: Loss:  0.4472, Accuracy: 0.8906\n",
      "Batch number : 323, Training: Loss:  0.7396, Accuracy: 0.7812\n",
      "Batch number : 324, Training: Loss:  0.7184, Accuracy: 0.7969\n",
      "Batch number : 325, Training: Loss:  0.3930, Accuracy: 0.8438\n",
      "Batch number : 326, Training: Loss:  0.5832, Accuracy: 0.8281\n",
      "Batch number : 327, Training: Loss:  0.3890, Accuracy: 0.8906\n",
      "Batch number : 328, Training: Loss:  0.3205, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.5003, Accuracy: 0.8125\n",
      "Batch number : 330, Training: Loss:  0.4494, Accuracy: 0.8281\n",
      "Batch number : 331, Training: Loss:  0.4444, Accuracy: 0.8750\n",
      "Batch number : 332, Training: Loss:  0.3405, Accuracy: 0.8906\n",
      "Batch number : 333, Training: Loss:  0.4251, Accuracy: 0.8750\n",
      "Batch number : 334, Training: Loss:  0.3077, Accuracy: 0.9062\n",
      "Batch number : 335, Training: Loss:  0.4371, Accuracy: 0.8594\n",
      "Batch number : 336, Training: Loss:  0.4399, Accuracy: 0.8438\n",
      "Batch number : 337, Training: Loss:  0.3522, Accuracy: 0.8594\n",
      "Batch number : 338, Training: Loss:  0.6223, Accuracy: 0.8438\n",
      "Batch number : 339, Training: Loss:  0.4648, Accuracy: 0.8750\n",
      "Batch number : 340, Training: Loss:  0.3326, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.4058, Accuracy: 0.9062\n",
      "Batch number : 342, Training: Loss:  0.4103, Accuracy: 0.9219\n",
      "Batch number : 343, Training: Loss:  0.3868, Accuracy: 0.8906\n",
      "Batch number : 344, Training: Loss:  0.6342, Accuracy: 0.8438\n",
      "Batch number : 345, Training: Loss:  0.5439, Accuracy: 0.8438\n",
      "Batch number : 346, Training: Loss:  0.5758, Accuracy: 0.8125\n",
      "Batch number : 347, Training: Loss:  0.4496, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.5562, Accuracy: 0.8281\n",
      "Batch number : 349, Training: Loss:  0.4277, Accuracy: 0.9219\n",
      "Batch number : 350, Training: Loss:  0.3974, Accuracy: 0.8750\n",
      "Batch number : 351, Training: Loss:  0.6125, Accuracy: 0.7969\n",
      "Batch number : 352, Training: Loss:  0.6237, Accuracy: 0.7969\n",
      "Batch number : 353, Training: Loss:  0.5584, Accuracy: 0.8281\n",
      "Batch number : 354, Training: Loss:  0.4271, Accuracy: 0.8906\n",
      "Batch number : 355, Training: Loss:  0.5341, Accuracy: 0.8125\n",
      "Batch number : 356, Training: Loss:  0.5774, Accuracy: 0.7969\n",
      "Batch number : 357, Training: Loss:  0.5666, Accuracy: 0.8125\n",
      "Batch number : 358, Training: Loss:  0.5177, Accuracy: 0.8438\n",
      "Batch number : 359, Training: Loss:  0.4730, Accuracy: 0.9062\n",
      "Batch number : 360, Training: Loss:  0.5529, Accuracy: 0.7812\n",
      "Batch number : 361, Training: Loss:  0.3550, Accuracy: 0.9219\n",
      "Batch number : 362, Training: Loss:  0.4426, Accuracy: 0.8906\n",
      "Batch number : 363, Training: Loss:  0.5702, Accuracy: 0.8281\n",
      "Batch number : 364, Training: Loss:  0.5252, Accuracy: 0.8438\n",
      "Batch number : 365, Training: Loss:  0.4131, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.7428, Accuracy: 0.7812\n",
      "Batch number : 367, Training: Loss:  0.4805, Accuracy: 0.8594\n",
      "Batch number : 368, Training: Loss:  0.4886, Accuracy: 0.8438\n",
      "Batch number : 369, Training: Loss:  0.3740, Accuracy: 0.8750\n",
      "Batch number : 370, Training: Loss:  0.4858, Accuracy: 0.9062\n",
      "Batch number : 371, Training: Loss:  0.6227, Accuracy: 0.8125\n",
      "Batch number : 372, Training: Loss:  0.6030, Accuracy: 0.8125\n",
      "Batch number : 373, Training: Loss:  0.3736, Accuracy: 0.9062\n",
      "Batch number : 374, Training: Loss:  0.5583, Accuracy: 0.8750\n",
      "Batch number : 375, Training: Loss:  0.4404, Accuracy: 0.8906\n",
      "Batch number : 376, Training: Loss:  0.4895, Accuracy: 0.8125\n",
      "Batch number : 377, Training: Loss:  0.4445, Accuracy: 0.8500\n",
      "Epoch: 7/20\n",
      "Batch number : 000, Training: Loss:  0.8078, Accuracy: 0.8125\n",
      "Batch number : 001, Training: Loss:  0.6374, Accuracy: 0.8438\n",
      "Batch number : 002, Training: Loss:  0.2909, Accuracy: 0.9375\n",
      "Batch number : 003, Training: Loss:  0.7513, Accuracy: 0.7500\n",
      "Batch number : 004, Training: Loss:  0.4180, Accuracy: 0.9219\n",
      "Batch number : 005, Training: Loss:  0.3177, Accuracy: 0.9219\n",
      "Batch number : 006, Training: Loss:  0.4094, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.3908, Accuracy: 0.8750\n",
      "Batch number : 008, Training: Loss:  0.4807, Accuracy: 0.8594\n",
      "Batch number : 009, Training: Loss:  0.7534, Accuracy: 0.7969\n",
      "Batch number : 010, Training: Loss:  0.3248, Accuracy: 0.9062\n",
      "Batch number : 011, Training: Loss:  0.4680, Accuracy: 0.7969\n",
      "Batch number : 012, Training: Loss:  0.4251, Accuracy: 0.8750\n",
      "Batch number : 013, Training: Loss:  0.3623, Accuracy: 0.8906\n",
      "Batch number : 014, Training: Loss:  0.4742, Accuracy: 0.9062\n",
      "Batch number : 015, Training: Loss:  0.6780, Accuracy: 0.8438\n",
      "Batch number : 016, Training: Loss:  0.4658, Accuracy: 0.8125\n",
      "Batch number : 017, Training: Loss:  0.6600, Accuracy: 0.8281\n",
      "Batch number : 018, Training: Loss:  0.4668, Accuracy: 0.8438\n",
      "Batch number : 019, Training: Loss:  0.4173, Accuracy: 0.9062\n",
      "Batch number : 020, Training: Loss:  0.3487, Accuracy: 0.9375\n",
      "Batch number : 021, Training: Loss:  0.3645, Accuracy: 0.8750\n",
      "Batch number : 022, Training: Loss:  0.6127, Accuracy: 0.8125\n",
      "Batch number : 023, Training: Loss:  0.5688, Accuracy: 0.8281\n",
      "Batch number : 024, Training: Loss:  0.5689, Accuracy: 0.8281\n",
      "Batch number : 025, Training: Loss:  0.3410, Accuracy: 0.8906\n",
      "Batch number : 026, Training: Loss:  0.3054, Accuracy: 0.9062\n",
      "Batch number : 027, Training: Loss:  0.4567, Accuracy: 0.8750\n",
      "Batch number : 028, Training: Loss:  0.6332, Accuracy: 0.7969\n",
      "Batch number : 029, Training: Loss:  0.5488, Accuracy: 0.7969\n",
      "Batch number : 030, Training: Loss:  0.6042, Accuracy: 0.8125\n",
      "Batch number : 031, Training: Loss:  0.6336, Accuracy: 0.8125\n",
      "Batch number : 032, Training: Loss:  0.4760, Accuracy: 0.9219\n",
      "Batch number : 033, Training: Loss:  0.4763, Accuracy: 0.8438\n",
      "Batch number : 034, Training: Loss:  0.4264, Accuracy: 0.9062\n",
      "Batch number : 035, Training: Loss:  0.3712, Accuracy: 0.9219\n",
      "Batch number : 036, Training: Loss:  0.5959, Accuracy: 0.7969\n",
      "Batch number : 037, Training: Loss:  0.3672, Accuracy: 0.8906\n",
      "Batch number : 038, Training: Loss:  0.4692, Accuracy: 0.8750\n",
      "Batch number : 039, Training: Loss:  0.5331, Accuracy: 0.8594\n",
      "Batch number : 040, Training: Loss:  0.5600, Accuracy: 0.7969\n",
      "Batch number : 041, Training: Loss:  0.6803, Accuracy: 0.8594\n",
      "Batch number : 042, Training: Loss:  0.3549, Accuracy: 0.9062\n",
      "Batch number : 043, Training: Loss:  0.3194, Accuracy: 0.8906\n",
      "Batch number : 044, Training: Loss:  0.3109, Accuracy: 0.9219\n",
      "Batch number : 045, Training: Loss:  0.5157, Accuracy: 0.8750\n",
      "Batch number : 046, Training: Loss:  0.3218, Accuracy: 0.9062\n",
      "Batch number : 047, Training: Loss:  0.6165, Accuracy: 0.8438\n",
      "Batch number : 048, Training: Loss:  0.2497, Accuracy: 0.9375\n",
      "Batch number : 049, Training: Loss:  0.3762, Accuracy: 0.9062\n",
      "Batch number : 050, Training: Loss:  0.6319, Accuracy: 0.8125\n",
      "Batch number : 051, Training: Loss:  0.3304, Accuracy: 0.8906\n",
      "Batch number : 052, Training: Loss:  0.5440, Accuracy: 0.8281\n",
      "Batch number : 053, Training: Loss:  0.4738, Accuracy: 0.8438\n",
      "Batch number : 054, Training: Loss:  0.6257, Accuracy: 0.7969\n",
      "Batch number : 055, Training: Loss:  0.4950, Accuracy: 0.8438\n",
      "Batch number : 056, Training: Loss:  0.4734, Accuracy: 0.8594\n",
      "Batch number : 057, Training: Loss:  0.5438, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.7294, Accuracy: 0.7812\n",
      "Batch number : 059, Training: Loss:  0.4984, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.7035, Accuracy: 0.7969\n",
      "Batch number : 061, Training: Loss:  0.4847, Accuracy: 0.8594\n",
      "Batch number : 062, Training: Loss:  0.5381, Accuracy: 0.8438\n",
      "Batch number : 063, Training: Loss:  0.3951, Accuracy: 0.9062\n",
      "Batch number : 064, Training: Loss:  0.4606, Accuracy: 0.8594\n",
      "Batch number : 065, Training: Loss:  0.5706, Accuracy: 0.8125\n",
      "Batch number : 066, Training: Loss:  0.6519, Accuracy: 0.7812\n",
      "Batch number : 067, Training: Loss:  0.4285, Accuracy: 0.8438\n",
      "Batch number : 068, Training: Loss:  0.5251, Accuracy: 0.8125\n",
      "Batch number : 069, Training: Loss:  0.3632, Accuracy: 0.9062\n",
      "Batch number : 070, Training: Loss:  0.3839, Accuracy: 0.8906\n",
      "Batch number : 071, Training: Loss:  0.6213, Accuracy: 0.7812\n",
      "Batch number : 072, Training: Loss:  0.4895, Accuracy: 0.8438\n",
      "Batch number : 073, Training: Loss:  0.7100, Accuracy: 0.7969\n",
      "Batch number : 074, Training: Loss:  0.5247, Accuracy: 0.8438\n",
      "Batch number : 075, Training: Loss:  0.5895, Accuracy: 0.8125\n",
      "Batch number : 076, Training: Loss:  0.4609, Accuracy: 0.8750\n",
      "Batch number : 077, Training: Loss:  0.2896, Accuracy: 0.9375\n",
      "Batch number : 078, Training: Loss:  0.5588, Accuracy: 0.8438\n",
      "Batch number : 079, Training: Loss:  0.5181, Accuracy: 0.8594\n",
      "Batch number : 080, Training: Loss:  0.6501, Accuracy: 0.7969\n",
      "Batch number : 081, Training: Loss:  0.2712, Accuracy: 0.9375\n",
      "Batch number : 082, Training: Loss:  0.4096, Accuracy: 0.8750\n",
      "Batch number : 083, Training: Loss:  0.5678, Accuracy: 0.8125\n",
      "Batch number : 084, Training: Loss:  0.6375, Accuracy: 0.8281\n",
      "Batch number : 085, Training: Loss:  0.5180, Accuracy: 0.8281\n",
      "Batch number : 086, Training: Loss:  0.3593, Accuracy: 0.8906\n",
      "Batch number : 087, Training: Loss:  0.4578, Accuracy: 0.8750\n",
      "Batch number : 088, Training: Loss:  0.4806, Accuracy: 0.8438\n",
      "Batch number : 089, Training: Loss:  0.2607, Accuracy: 0.9531\n",
      "Batch number : 090, Training: Loss:  0.7177, Accuracy: 0.7812\n",
      "Batch number : 091, Training: Loss:  0.4165, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.6863, Accuracy: 0.8438\n",
      "Batch number : 093, Training: Loss:  0.3997, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.3766, Accuracy: 0.8594\n",
      "Batch number : 095, Training: Loss:  0.4705, Accuracy: 0.8750\n",
      "Batch number : 096, Training: Loss:  0.4590, Accuracy: 0.8438\n",
      "Batch number : 097, Training: Loss:  0.5875, Accuracy: 0.8281\n",
      "Batch number : 098, Training: Loss:  0.6137, Accuracy: 0.7969\n",
      "Batch number : 099, Training: Loss:  0.2846, Accuracy: 0.9219\n",
      "Batch number : 100, Training: Loss:  0.4773, Accuracy: 0.9062\n",
      "Batch number : 101, Training: Loss:  0.4605, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.7000, Accuracy: 0.8281\n",
      "Batch number : 103, Training: Loss:  0.5205, Accuracy: 0.8750\n",
      "Batch number : 104, Training: Loss:  0.3919, Accuracy: 0.8906\n",
      "Batch number : 105, Training: Loss:  0.4665, Accuracy: 0.8594\n",
      "Batch number : 106, Training: Loss:  0.2967, Accuracy: 0.9062\n",
      "Batch number : 107, Training: Loss:  0.3602, Accuracy: 0.8906\n",
      "Batch number : 108, Training: Loss:  0.4866, Accuracy: 0.8750\n",
      "Batch number : 109, Training: Loss:  0.3468, Accuracy: 0.9062\n",
      "Batch number : 110, Training: Loss:  0.8836, Accuracy: 0.7500\n",
      "Batch number : 111, Training: Loss:  0.5667, Accuracy: 0.8125\n",
      "Batch number : 112, Training: Loss:  0.6713, Accuracy: 0.7969\n",
      "Batch number : 113, Training: Loss:  0.4137, Accuracy: 0.8906\n",
      "Batch number : 114, Training: Loss:  0.7095, Accuracy: 0.7344\n",
      "Batch number : 115, Training: Loss:  0.4833, Accuracy: 0.8594\n",
      "Batch number : 116, Training: Loss:  0.6397, Accuracy: 0.7812\n",
      "Batch number : 117, Training: Loss:  0.4321, Accuracy: 0.9062\n",
      "Batch number : 118, Training: Loss:  0.4413, Accuracy: 0.9375\n",
      "Batch number : 119, Training: Loss:  0.5007, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.4230, Accuracy: 0.8906\n",
      "Batch number : 121, Training: Loss:  0.4736, Accuracy: 0.8594\n",
      "Batch number : 122, Training: Loss:  0.5205, Accuracy: 0.8594\n",
      "Batch number : 123, Training: Loss:  0.6898, Accuracy: 0.8438\n",
      "Batch number : 124, Training: Loss:  0.7101, Accuracy: 0.8125\n",
      "Batch number : 125, Training: Loss:  0.4809, Accuracy: 0.8594\n",
      "Batch number : 126, Training: Loss:  0.6279, Accuracy: 0.8438\n",
      "Batch number : 127, Training: Loss:  0.3888, Accuracy: 0.8906\n",
      "Batch number : 128, Training: Loss:  0.4599, Accuracy: 0.8594\n",
      "Batch number : 129, Training: Loss:  0.3297, Accuracy: 0.9375\n",
      "Batch number : 130, Training: Loss:  0.4472, Accuracy: 0.8594\n",
      "Batch number : 131, Training: Loss:  0.5353, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.3085, Accuracy: 0.9375\n",
      "Batch number : 133, Training: Loss:  0.4689, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.4967, Accuracy: 0.8281\n",
      "Batch number : 135, Training: Loss:  0.4447, Accuracy: 0.9219\n",
      "Batch number : 136, Training: Loss:  0.5560, Accuracy: 0.8125\n",
      "Batch number : 137, Training: Loss:  0.4366, Accuracy: 0.8750\n",
      "Batch number : 138, Training: Loss:  0.7447, Accuracy: 0.7969\n",
      "Batch number : 139, Training: Loss:  0.4917, Accuracy: 0.8906\n",
      "Batch number : 140, Training: Loss:  0.4414, Accuracy: 0.8750\n",
      "Batch number : 141, Training: Loss:  0.3697, Accuracy: 0.8750\n",
      "Batch number : 142, Training: Loss:  0.6968, Accuracy: 0.7812\n",
      "Batch number : 143, Training: Loss:  0.5563, Accuracy: 0.8281\n",
      "Batch number : 144, Training: Loss:  0.5298, Accuracy: 0.8281\n",
      "Batch number : 145, Training: Loss:  0.4029, Accuracy: 0.9062\n",
      "Batch number : 146, Training: Loss:  0.4705, Accuracy: 0.9062\n",
      "Batch number : 147, Training: Loss:  0.4634, Accuracy: 0.8594\n",
      "Batch number : 148, Training: Loss:  0.4449, Accuracy: 0.8594\n",
      "Batch number : 149, Training: Loss:  0.3513, Accuracy: 0.8906\n",
      "Batch number : 150, Training: Loss:  0.4099, Accuracy: 0.8438\n",
      "Batch number : 151, Training: Loss:  0.3261, Accuracy: 0.9062\n",
      "Batch number : 152, Training: Loss:  0.2512, Accuracy: 0.9375\n",
      "Batch number : 153, Training: Loss:  0.2484, Accuracy: 0.9375\n",
      "Batch number : 154, Training: Loss:  0.5810, Accuracy: 0.8438\n",
      "Batch number : 155, Training: Loss:  0.3207, Accuracy: 0.8906\n",
      "Batch number : 156, Training: Loss:  0.4701, Accuracy: 0.8750\n",
      "Batch number : 157, Training: Loss:  1.0393, Accuracy: 0.7188\n",
      "Batch number : 158, Training: Loss:  0.3928, Accuracy: 0.9062\n",
      "Batch number : 159, Training: Loss:  0.5324, Accuracy: 0.8594\n",
      "Batch number : 160, Training: Loss:  0.7117, Accuracy: 0.7812\n",
      "Batch number : 161, Training: Loss:  0.4639, Accuracy: 0.8906\n",
      "Batch number : 162, Training: Loss:  0.4461, Accuracy: 0.8594\n",
      "Batch number : 163, Training: Loss:  0.4606, Accuracy: 0.8750\n",
      "Batch number : 164, Training: Loss:  0.5939, Accuracy: 0.7500\n",
      "Batch number : 165, Training: Loss:  0.4492, Accuracy: 0.8594\n",
      "Batch number : 166, Training: Loss:  0.6056, Accuracy: 0.7969\n",
      "Batch number : 167, Training: Loss:  0.6347, Accuracy: 0.7969\n",
      "Batch number : 168, Training: Loss:  0.5372, Accuracy: 0.7969\n",
      "Batch number : 169, Training: Loss:  0.4702, Accuracy: 0.8281\n",
      "Batch number : 170, Training: Loss:  0.4940, Accuracy: 0.8750\n",
      "Batch number : 171, Training: Loss:  0.3810, Accuracy: 0.8906\n",
      "Batch number : 172, Training: Loss:  0.4567, Accuracy: 0.8906\n",
      "Batch number : 173, Training: Loss:  0.4718, Accuracy: 0.8594\n",
      "Batch number : 174, Training: Loss:  0.6452, Accuracy: 0.8125\n",
      "Batch number : 175, Training: Loss:  0.4218, Accuracy: 0.8750\n",
      "Batch number : 176, Training: Loss:  0.5034, Accuracy: 0.8125\n",
      "Batch number : 177, Training: Loss:  0.3913, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.2897, Accuracy: 0.9375\n",
      "Batch number : 179, Training: Loss:  0.4514, Accuracy: 0.8906\n",
      "Batch number : 180, Training: Loss:  0.2450, Accuracy: 0.9375\n",
      "Batch number : 181, Training: Loss:  0.6319, Accuracy: 0.7969\n",
      "Batch number : 182, Training: Loss:  0.4808, Accuracy: 0.8438\n",
      "Batch number : 183, Training: Loss:  0.4541, Accuracy: 0.8750\n",
      "Batch number : 184, Training: Loss:  0.5099, Accuracy: 0.8281\n",
      "Batch number : 185, Training: Loss:  0.6301, Accuracy: 0.8438\n",
      "Batch number : 186, Training: Loss:  0.5560, Accuracy: 0.8281\n",
      "Batch number : 187, Training: Loss:  0.4968, Accuracy: 0.8594\n",
      "Batch number : 188, Training: Loss:  0.6100, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.5454, Accuracy: 0.7969\n",
      "Batch number : 190, Training: Loss:  0.4681, Accuracy: 0.8281\n",
      "Batch number : 191, Training: Loss:  0.6836, Accuracy: 0.7812\n",
      "Batch number : 192, Training: Loss:  0.4661, Accuracy: 0.8750\n",
      "Batch number : 193, Training: Loss:  0.3498, Accuracy: 0.9062\n",
      "Batch number : 194, Training: Loss:  0.6549, Accuracy: 0.8125\n",
      "Batch number : 195, Training: Loss:  0.3429, Accuracy: 0.8906\n",
      "Batch number : 196, Training: Loss:  0.4616, Accuracy: 0.8594\n",
      "Batch number : 197, Training: Loss:  0.6402, Accuracy: 0.8125\n",
      "Batch number : 198, Training: Loss:  0.6974, Accuracy: 0.7812\n",
      "Batch number : 199, Training: Loss:  0.6404, Accuracy: 0.7969\n",
      "Batch number : 200, Training: Loss:  0.4094, Accuracy: 0.8906\n",
      "Batch number : 201, Training: Loss:  0.4322, Accuracy: 0.8750\n",
      "Batch number : 202, Training: Loss:  0.4051, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.3926, Accuracy: 0.8906\n",
      "Batch number : 204, Training: Loss:  0.5343, Accuracy: 0.8438\n",
      "Batch number : 205, Training: Loss:  0.5786, Accuracy: 0.8281\n",
      "Batch number : 206, Training: Loss:  0.2560, Accuracy: 0.9375\n",
      "Batch number : 207, Training: Loss:  0.5131, Accuracy: 0.8594\n",
      "Batch number : 208, Training: Loss:  0.7798, Accuracy: 0.7344\n",
      "Batch number : 209, Training: Loss:  0.3247, Accuracy: 0.8906\n",
      "Batch number : 210, Training: Loss:  0.5418, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.4043, Accuracy: 0.8750\n",
      "Batch number : 212, Training: Loss:  0.3669, Accuracy: 0.8906\n",
      "Batch number : 213, Training: Loss:  0.5596, Accuracy: 0.8125\n",
      "Batch number : 214, Training: Loss:  0.5754, Accuracy: 0.8438\n",
      "Batch number : 215, Training: Loss:  0.3930, Accuracy: 0.8906\n",
      "Batch number : 216, Training: Loss:  0.3224, Accuracy: 0.9219\n",
      "Batch number : 217, Training: Loss:  0.5359, Accuracy: 0.8594\n",
      "Batch number : 218, Training: Loss:  0.5568, Accuracy: 0.8438\n",
      "Batch number : 219, Training: Loss:  0.2713, Accuracy: 0.9219\n",
      "Batch number : 220, Training: Loss:  0.4737, Accuracy: 0.8906\n",
      "Batch number : 221, Training: Loss:  0.6574, Accuracy: 0.7969\n",
      "Batch number : 222, Training: Loss:  0.6194, Accuracy: 0.7812\n",
      "Batch number : 223, Training: Loss:  0.3843, Accuracy: 0.8750\n",
      "Batch number : 224, Training: Loss:  0.4688, Accuracy: 0.8594\n",
      "Batch number : 225, Training: Loss:  0.4789, Accuracy: 0.8281\n",
      "Batch number : 226, Training: Loss:  0.5668, Accuracy: 0.8438\n",
      "Batch number : 227, Training: Loss:  0.3585, Accuracy: 0.9531\n",
      "Batch number : 228, Training: Loss:  0.5704, Accuracy: 0.8125\n",
      "Batch number : 229, Training: Loss:  0.4275, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.5449, Accuracy: 0.8438\n",
      "Batch number : 231, Training: Loss:  0.5295, Accuracy: 0.8594\n",
      "Batch number : 232, Training: Loss:  0.3999, Accuracy: 0.8438\n",
      "Batch number : 233, Training: Loss:  0.5083, Accuracy: 0.8594\n",
      "Batch number : 234, Training: Loss:  0.5258, Accuracy: 0.8125\n",
      "Batch number : 235, Training: Loss:  0.3906, Accuracy: 0.8906\n",
      "Batch number : 236, Training: Loss:  0.3906, Accuracy: 0.8906\n",
      "Batch number : 237, Training: Loss:  0.4656, Accuracy: 0.8281\n",
      "Batch number : 238, Training: Loss:  0.6391, Accuracy: 0.8438\n",
      "Batch number : 239, Training: Loss:  0.5785, Accuracy: 0.8594\n",
      "Batch number : 240, Training: Loss:  0.6992, Accuracy: 0.8125\n",
      "Batch number : 241, Training: Loss:  0.6288, Accuracy: 0.8281\n",
      "Batch number : 242, Training: Loss:  0.6122, Accuracy: 0.7656\n",
      "Batch number : 243, Training: Loss:  0.5933, Accuracy: 0.8438\n",
      "Batch number : 244, Training: Loss:  0.5009, Accuracy: 0.8125\n",
      "Batch number : 245, Training: Loss:  0.4031, Accuracy: 0.8750\n",
      "Batch number : 246, Training: Loss:  0.4496, Accuracy: 0.8750\n",
      "Batch number : 247, Training: Loss:  0.4185, Accuracy: 0.8594\n",
      "Batch number : 248, Training: Loss:  0.3802, Accuracy: 0.8594\n",
      "Batch number : 249, Training: Loss:  0.4619, Accuracy: 0.8594\n",
      "Batch number : 250, Training: Loss:  0.3982, Accuracy: 0.8906\n",
      "Batch number : 251, Training: Loss:  0.3232, Accuracy: 0.8906\n",
      "Batch number : 252, Training: Loss:  0.3705, Accuracy: 0.8906\n",
      "Batch number : 253, Training: Loss:  0.4786, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.5177, Accuracy: 0.8438\n",
      "Batch number : 255, Training: Loss:  0.2811, Accuracy: 0.9062\n",
      "Batch number : 256, Training: Loss:  0.9126, Accuracy: 0.7656\n",
      "Batch number : 257, Training: Loss:  0.4025, Accuracy: 0.8438\n",
      "Batch number : 258, Training: Loss:  0.5119, Accuracy: 0.8438\n",
      "Batch number : 259, Training: Loss:  0.7490, Accuracy: 0.7812\n",
      "Batch number : 260, Training: Loss:  0.5976, Accuracy: 0.8281\n",
      "Batch number : 261, Training: Loss:  0.4483, Accuracy: 0.8750\n",
      "Batch number : 262, Training: Loss:  0.4168, Accuracy: 0.8750\n",
      "Batch number : 263, Training: Loss:  0.4685, Accuracy: 0.8750\n",
      "Batch number : 264, Training: Loss:  0.4901, Accuracy: 0.7969\n",
      "Batch number : 265, Training: Loss:  0.5918, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.2653, Accuracy: 0.9531\n",
      "Batch number : 267, Training: Loss:  0.5049, Accuracy: 0.8438\n",
      "Batch number : 268, Training: Loss:  0.4566, Accuracy: 0.8438\n",
      "Batch number : 269, Training: Loss:  0.4564, Accuracy: 0.8281\n",
      "Batch number : 270, Training: Loss:  0.1666, Accuracy: 0.9531\n",
      "Batch number : 271, Training: Loss:  0.6595, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.4629, Accuracy: 0.8750\n",
      "Batch number : 273, Training: Loss:  0.3109, Accuracy: 0.8906\n",
      "Batch number : 274, Training: Loss:  0.3458, Accuracy: 0.8906\n",
      "Batch number : 275, Training: Loss:  0.4610, Accuracy: 0.9062\n",
      "Batch number : 276, Training: Loss:  0.6032, Accuracy: 0.8125\n",
      "Batch number : 277, Training: Loss:  0.5373, Accuracy: 0.8594\n",
      "Batch number : 278, Training: Loss:  0.4301, Accuracy: 0.8281\n",
      "Batch number : 279, Training: Loss:  0.6236, Accuracy: 0.8281\n",
      "Batch number : 280, Training: Loss:  0.3011, Accuracy: 0.9219\n",
      "Batch number : 281, Training: Loss:  0.4768, Accuracy: 0.8750\n",
      "Batch number : 282, Training: Loss:  0.6510, Accuracy: 0.7969\n",
      "Batch number : 283, Training: Loss:  0.3246, Accuracy: 0.9531\n",
      "Batch number : 284, Training: Loss:  0.4927, Accuracy: 0.8594\n",
      "Batch number : 285, Training: Loss:  0.4147, Accuracy: 0.8594\n",
      "Batch number : 286, Training: Loss:  0.3381, Accuracy: 0.8906\n",
      "Batch number : 287, Training: Loss:  0.5603, Accuracy: 0.8594\n",
      "Batch number : 288, Training: Loss:  0.4100, Accuracy: 0.8750\n",
      "Batch number : 289, Training: Loss:  0.6954, Accuracy: 0.7969\n",
      "Batch number : 290, Training: Loss:  0.3252, Accuracy: 0.9219\n",
      "Batch number : 291, Training: Loss:  0.5814, Accuracy: 0.7969\n",
      "Batch number : 292, Training: Loss:  0.7363, Accuracy: 0.7969\n",
      "Batch number : 293, Training: Loss:  0.4840, Accuracy: 0.8594\n",
      "Batch number : 294, Training: Loss:  0.3005, Accuracy: 0.9062\n",
      "Batch number : 295, Training: Loss:  0.5297, Accuracy: 0.8281\n",
      "Batch number : 296, Training: Loss:  0.3719, Accuracy: 0.9062\n",
      "Batch number : 297, Training: Loss:  0.4085, Accuracy: 0.9219\n",
      "Batch number : 298, Training: Loss:  0.3889, Accuracy: 0.9219\n",
      "Batch number : 299, Training: Loss:  0.5382, Accuracy: 0.8438\n",
      "Batch number : 300, Training: Loss:  0.4721, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.4818, Accuracy: 0.8438\n",
      "Batch number : 302, Training: Loss:  0.4398, Accuracy: 0.8750\n",
      "Batch number : 303, Training: Loss:  0.2664, Accuracy: 0.9531\n",
      "Batch number : 304, Training: Loss:  0.5501, Accuracy: 0.8594\n",
      "Batch number : 305, Training: Loss:  0.3319, Accuracy: 0.8906\n",
      "Batch number : 306, Training: Loss:  0.3795, Accuracy: 0.8750\n",
      "Batch number : 307, Training: Loss:  0.5116, Accuracy: 0.8750\n",
      "Batch number : 308, Training: Loss:  0.3171, Accuracy: 0.8906\n",
      "Batch number : 309, Training: Loss:  0.4413, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.5910, Accuracy: 0.8438\n",
      "Batch number : 311, Training: Loss:  0.5689, Accuracy: 0.8281\n",
      "Batch number : 312, Training: Loss:  0.4980, Accuracy: 0.8750\n",
      "Batch number : 313, Training: Loss:  0.5478, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.4234, Accuracy: 0.8906\n",
      "Batch number : 315, Training: Loss:  0.4681, Accuracy: 0.8438\n",
      "Batch number : 316, Training: Loss:  0.5411, Accuracy: 0.8438\n",
      "Batch number : 317, Training: Loss:  0.4936, Accuracy: 0.8438\n",
      "Batch number : 318, Training: Loss:  0.4147, Accuracy: 0.8906\n",
      "Batch number : 319, Training: Loss:  0.4027, Accuracy: 0.8906\n",
      "Batch number : 320, Training: Loss:  0.6665, Accuracy: 0.7969\n",
      "Batch number : 321, Training: Loss:  0.4342, Accuracy: 0.8750\n",
      "Batch number : 322, Training: Loss:  0.5287, Accuracy: 0.8750\n",
      "Batch number : 323, Training: Loss:  0.5617, Accuracy: 0.8906\n",
      "Batch number : 324, Training: Loss:  0.4409, Accuracy: 0.8906\n",
      "Batch number : 325, Training: Loss:  0.4169, Accuracy: 0.8750\n",
      "Batch number : 326, Training: Loss:  0.4092, Accuracy: 0.8750\n",
      "Batch number : 327, Training: Loss:  0.4906, Accuracy: 0.8438\n",
      "Batch number : 328, Training: Loss:  0.2620, Accuracy: 0.9375\n",
      "Batch number : 329, Training: Loss:  0.4129, Accuracy: 0.8594\n",
      "Batch number : 330, Training: Loss:  0.5707, Accuracy: 0.8281\n",
      "Batch number : 331, Training: Loss:  0.6027, Accuracy: 0.7969\n",
      "Batch number : 332, Training: Loss:  0.4670, Accuracy: 0.8594\n",
      "Batch number : 333, Training: Loss:  0.3551, Accuracy: 0.8906\n",
      "Batch number : 334, Training: Loss:  0.5894, Accuracy: 0.8281\n",
      "Batch number : 335, Training: Loss:  0.4093, Accuracy: 0.9062\n",
      "Batch number : 336, Training: Loss:  0.6796, Accuracy: 0.7656\n",
      "Batch number : 337, Training: Loss:  0.5044, Accuracy: 0.8438\n",
      "Batch number : 338, Training: Loss:  0.5582, Accuracy: 0.8125\n",
      "Batch number : 339, Training: Loss:  0.4963, Accuracy: 0.8594\n",
      "Batch number : 340, Training: Loss:  0.5648, Accuracy: 0.8594\n",
      "Batch number : 341, Training: Loss:  0.3885, Accuracy: 0.8906\n",
      "Batch number : 342, Training: Loss:  0.5857, Accuracy: 0.8281\n",
      "Batch number : 343, Training: Loss:  0.5444, Accuracy: 0.8438\n",
      "Batch number : 344, Training: Loss:  0.4374, Accuracy: 0.9062\n",
      "Batch number : 345, Training: Loss:  0.3933, Accuracy: 0.9219\n",
      "Batch number : 346, Training: Loss:  0.5326, Accuracy: 0.8438\n",
      "Batch number : 347, Training: Loss:  0.3906, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.4826, Accuracy: 0.8750\n",
      "Batch number : 349, Training: Loss:  0.4913, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.6538, Accuracy: 0.8438\n",
      "Batch number : 351, Training: Loss:  0.3903, Accuracy: 0.8438\n",
      "Batch number : 352, Training: Loss:  0.5141, Accuracy: 0.8438\n",
      "Batch number : 353, Training: Loss:  0.5231, Accuracy: 0.8438\n",
      "Batch number : 354, Training: Loss:  0.6027, Accuracy: 0.8281\n",
      "Batch number : 355, Training: Loss:  0.5261, Accuracy: 0.8281\n",
      "Batch number : 356, Training: Loss:  0.5276, Accuracy: 0.8281\n",
      "Batch number : 357, Training: Loss:  0.7021, Accuracy: 0.7812\n",
      "Batch number : 358, Training: Loss:  0.3902, Accuracy: 0.9219\n",
      "Batch number : 359, Training: Loss:  0.3740, Accuracy: 0.9062\n",
      "Batch number : 360, Training: Loss:  0.6718, Accuracy: 0.8281\n",
      "Batch number : 361, Training: Loss:  0.4196, Accuracy: 0.8594\n",
      "Batch number : 362, Training: Loss:  0.4393, Accuracy: 0.8906\n",
      "Batch number : 363, Training: Loss:  0.3418, Accuracy: 0.9219\n",
      "Batch number : 364, Training: Loss:  0.5745, Accuracy: 0.8125\n",
      "Batch number : 365, Training: Loss:  0.4336, Accuracy: 0.8750\n",
      "Batch number : 366, Training: Loss:  0.5228, Accuracy: 0.8281\n",
      "Batch number : 367, Training: Loss:  0.5087, Accuracy: 0.8281\n",
      "Batch number : 368, Training: Loss:  0.2696, Accuracy: 0.9375\n",
      "Batch number : 369, Training: Loss:  0.4956, Accuracy: 0.8906\n",
      "Batch number : 370, Training: Loss:  0.4384, Accuracy: 0.8750\n",
      "Batch number : 371, Training: Loss:  0.5859, Accuracy: 0.8594\n",
      "Batch number : 372, Training: Loss:  0.5117, Accuracy: 0.8438\n",
      "Batch number : 373, Training: Loss:  0.6513, Accuracy: 0.8281\n",
      "Batch number : 374, Training: Loss:  0.5557, Accuracy: 0.7969\n",
      "Batch number : 375, Training: Loss:  0.6560, Accuracy: 0.8281\n",
      "Batch number : 376, Training: Loss:  0.5869, Accuracy: 0.7969\n",
      "Batch number : 377, Training: Loss:  0.5122, Accuracy: 0.8000\n",
      "Epoch: 8/20\n",
      "Batch number : 000, Training: Loss:  0.4804, Accuracy: 0.8438\n",
      "Batch number : 001, Training: Loss:  0.3804, Accuracy: 0.9062\n",
      "Batch number : 002, Training: Loss:  0.5447, Accuracy: 0.8281\n",
      "Batch number : 003, Training: Loss:  0.5503, Accuracy: 0.8281\n",
      "Batch number : 004, Training: Loss:  0.5496, Accuracy: 0.8125\n",
      "Batch number : 005, Training: Loss:  0.4369, Accuracy: 0.8594\n",
      "Batch number : 006, Training: Loss:  0.5107, Accuracy: 0.8281\n",
      "Batch number : 007, Training: Loss:  0.4870, Accuracy: 0.8281\n",
      "Batch number : 008, Training: Loss:  0.4168, Accuracy: 0.8594\n",
      "Batch number : 009, Training: Loss:  0.5006, Accuracy: 0.8594\n",
      "Batch number : 010, Training: Loss:  0.5976, Accuracy: 0.8438\n",
      "Batch number : 011, Training: Loss:  0.4821, Accuracy: 0.8281\n",
      "Batch number : 012, Training: Loss:  0.6821, Accuracy: 0.7969\n",
      "Batch number : 013, Training: Loss:  0.3977, Accuracy: 0.8750\n",
      "Batch number : 014, Training: Loss:  0.3071, Accuracy: 0.9062\n",
      "Batch number : 015, Training: Loss:  0.3859, Accuracy: 0.8906\n",
      "Batch number : 016, Training: Loss:  0.4841, Accuracy: 0.8281\n",
      "Batch number : 017, Training: Loss:  0.4293, Accuracy: 0.8594\n",
      "Batch number : 018, Training: Loss:  0.6225, Accuracy: 0.8281\n",
      "Batch number : 019, Training: Loss:  0.5455, Accuracy: 0.8438\n",
      "Batch number : 020, Training: Loss:  0.5005, Accuracy: 0.8594\n",
      "Batch number : 021, Training: Loss:  0.4469, Accuracy: 0.8594\n",
      "Batch number : 022, Training: Loss:  0.6451, Accuracy: 0.8125\n",
      "Batch number : 023, Training: Loss:  0.2766, Accuracy: 0.9219\n",
      "Batch number : 024, Training: Loss:  0.6796, Accuracy: 0.8281\n",
      "Batch number : 025, Training: Loss:  0.2697, Accuracy: 0.9062\n",
      "Batch number : 026, Training: Loss:  0.4571, Accuracy: 0.8438\n",
      "Batch number : 027, Training: Loss:  0.5869, Accuracy: 0.7969\n",
      "Batch number : 028, Training: Loss:  0.5587, Accuracy: 0.8281\n",
      "Batch number : 029, Training: Loss:  0.5799, Accuracy: 0.8125\n",
      "Batch number : 030, Training: Loss:  0.5400, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.2365, Accuracy: 0.9688\n",
      "Batch number : 032, Training: Loss:  0.5625, Accuracy: 0.8281\n",
      "Batch number : 033, Training: Loss:  0.4716, Accuracy: 0.8594\n",
      "Batch number : 034, Training: Loss:  0.3517, Accuracy: 0.9062\n",
      "Batch number : 035, Training: Loss:  0.4717, Accuracy: 0.8438\n",
      "Batch number : 036, Training: Loss:  0.4695, Accuracy: 0.8750\n",
      "Batch number : 037, Training: Loss:  0.8118, Accuracy: 0.7500\n",
      "Batch number : 038, Training: Loss:  0.4126, Accuracy: 0.9062\n",
      "Batch number : 039, Training: Loss:  0.5035, Accuracy: 0.8438\n",
      "Batch number : 040, Training: Loss:  0.4778, Accuracy: 0.8594\n",
      "Batch number : 041, Training: Loss:  0.5254, Accuracy: 0.8125\n",
      "Batch number : 042, Training: Loss:  0.5093, Accuracy: 0.8125\n",
      "Batch number : 043, Training: Loss:  0.5572, Accuracy: 0.8125\n",
      "Batch number : 044, Training: Loss:  0.5019, Accuracy: 0.8281\n",
      "Batch number : 045, Training: Loss:  0.5583, Accuracy: 0.8438\n",
      "Batch number : 046, Training: Loss:  0.3857, Accuracy: 0.8750\n",
      "Batch number : 047, Training: Loss:  0.4229, Accuracy: 0.9062\n",
      "Batch number : 048, Training: Loss:  0.3222, Accuracy: 0.9062\n",
      "Batch number : 049, Training: Loss:  0.6180, Accuracy: 0.8281\n",
      "Batch number : 050, Training: Loss:  0.6031, Accuracy: 0.8438\n",
      "Batch number : 051, Training: Loss:  0.4276, Accuracy: 0.8906\n",
      "Batch number : 052, Training: Loss:  0.5176, Accuracy: 0.8438\n",
      "Batch number : 053, Training: Loss:  0.4705, Accuracy: 0.8750\n",
      "Batch number : 054, Training: Loss:  0.3291, Accuracy: 0.9219\n",
      "Batch number : 055, Training: Loss:  0.2123, Accuracy: 0.9531\n",
      "Batch number : 056, Training: Loss:  0.3529, Accuracy: 0.9219\n",
      "Batch number : 057, Training: Loss:  0.4488, Accuracy: 0.8594\n",
      "Batch number : 058, Training: Loss:  0.5013, Accuracy: 0.8438\n",
      "Batch number : 059, Training: Loss:  0.5433, Accuracy: 0.8438\n",
      "Batch number : 060, Training: Loss:  0.9402, Accuracy: 0.7031\n",
      "Batch number : 061, Training: Loss:  0.3422, Accuracy: 0.9062\n",
      "Batch number : 062, Training: Loss:  0.5374, Accuracy: 0.8438\n",
      "Batch number : 063, Training: Loss:  0.4911, Accuracy: 0.8438\n",
      "Batch number : 064, Training: Loss:  0.4037, Accuracy: 0.9062\n",
      "Batch number : 065, Training: Loss:  0.6702, Accuracy: 0.7812\n",
      "Batch number : 066, Training: Loss:  0.4151, Accuracy: 0.8750\n",
      "Batch number : 067, Training: Loss:  0.6662, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.6807, Accuracy: 0.7969\n",
      "Batch number : 069, Training: Loss:  0.5796, Accuracy: 0.8125\n",
      "Batch number : 070, Training: Loss:  0.5163, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.5101, Accuracy: 0.8125\n",
      "Batch number : 072, Training: Loss:  0.4935, Accuracy: 0.8906\n",
      "Batch number : 073, Training: Loss:  0.3993, Accuracy: 0.8594\n",
      "Batch number : 074, Training: Loss:  0.2379, Accuracy: 0.9688\n",
      "Batch number : 075, Training: Loss:  0.3556, Accuracy: 0.9062\n",
      "Batch number : 076, Training: Loss:  0.3415, Accuracy: 0.9219\n",
      "Batch number : 077, Training: Loss:  0.5212, Accuracy: 0.8750\n",
      "Batch number : 078, Training: Loss:  0.7366, Accuracy: 0.7656\n",
      "Batch number : 079, Training: Loss:  0.3360, Accuracy: 0.8906\n",
      "Batch number : 080, Training: Loss:  0.7374, Accuracy: 0.7188\n",
      "Batch number : 081, Training: Loss:  0.5309, Accuracy: 0.8750\n",
      "Batch number : 082, Training: Loss:  0.4057, Accuracy: 0.8906\n",
      "Batch number : 083, Training: Loss:  0.5784, Accuracy: 0.8594\n",
      "Batch number : 084, Training: Loss:  0.5094, Accuracy: 0.8438\n",
      "Batch number : 085, Training: Loss:  0.3843, Accuracy: 0.8750\n",
      "Batch number : 086, Training: Loss:  0.3810, Accuracy: 0.9219\n",
      "Batch number : 087, Training: Loss:  0.5330, Accuracy: 0.8438\n",
      "Batch number : 088, Training: Loss:  0.4151, Accuracy: 0.9062\n",
      "Batch number : 089, Training: Loss:  0.4740, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.4873, Accuracy: 0.8438\n",
      "Batch number : 091, Training: Loss:  0.4248, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.4182, Accuracy: 0.8906\n",
      "Batch number : 093, Training: Loss:  0.3993, Accuracy: 0.9062\n",
      "Batch number : 094, Training: Loss:  0.8781, Accuracy: 0.7500\n",
      "Batch number : 095, Training: Loss:  0.6278, Accuracy: 0.7969\n",
      "Batch number : 096, Training: Loss:  0.7153, Accuracy: 0.7812\n",
      "Batch number : 097, Training: Loss:  0.5361, Accuracy: 0.8281\n",
      "Batch number : 098, Training: Loss:  0.5921, Accuracy: 0.7812\n",
      "Batch number : 099, Training: Loss:  0.3574, Accuracy: 0.9062\n",
      "Batch number : 100, Training: Loss:  0.4365, Accuracy: 0.8906\n",
      "Batch number : 101, Training: Loss:  0.5637, Accuracy: 0.8594\n",
      "Batch number : 102, Training: Loss:  0.6214, Accuracy: 0.8438\n",
      "Batch number : 103, Training: Loss:  0.4783, Accuracy: 0.8438\n",
      "Batch number : 104, Training: Loss:  0.4492, Accuracy: 0.8906\n",
      "Batch number : 105, Training: Loss:  0.5866, Accuracy: 0.7656\n",
      "Batch number : 106, Training: Loss:  0.5938, Accuracy: 0.7812\n",
      "Batch number : 107, Training: Loss:  0.4646, Accuracy: 0.8750\n",
      "Batch number : 108, Training: Loss:  0.7698, Accuracy: 0.7500\n",
      "Batch number : 109, Training: Loss:  0.5681, Accuracy: 0.8125\n",
      "Batch number : 110, Training: Loss:  0.5024, Accuracy: 0.8438\n",
      "Batch number : 111, Training: Loss:  0.2961, Accuracy: 0.9375\n",
      "Batch number : 112, Training: Loss:  0.3235, Accuracy: 0.9375\n",
      "Batch number : 113, Training: Loss:  0.4830, Accuracy: 0.8750\n",
      "Batch number : 114, Training: Loss:  0.4787, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.5627, Accuracy: 0.7969\n",
      "Batch number : 116, Training: Loss:  0.6156, Accuracy: 0.8594\n",
      "Batch number : 117, Training: Loss:  0.5029, Accuracy: 0.8750\n",
      "Batch number : 118, Training: Loss:  0.5012, Accuracy: 0.8438\n",
      "Batch number : 119, Training: Loss:  0.3219, Accuracy: 0.9062\n",
      "Batch number : 120, Training: Loss:  0.3246, Accuracy: 0.9062\n",
      "Batch number : 121, Training: Loss:  0.4054, Accuracy: 0.8750\n",
      "Batch number : 122, Training: Loss:  0.4690, Accuracy: 0.8438\n",
      "Batch number : 123, Training: Loss:  0.4633, Accuracy: 0.8750\n",
      "Batch number : 124, Training: Loss:  0.6083, Accuracy: 0.7969\n",
      "Batch number : 125, Training: Loss:  0.5466, Accuracy: 0.8125\n",
      "Batch number : 126, Training: Loss:  0.3777, Accuracy: 0.8438\n",
      "Batch number : 127, Training: Loss:  0.4059, Accuracy: 0.8906\n",
      "Batch number : 128, Training: Loss:  0.3483, Accuracy: 0.8750\n",
      "Batch number : 129, Training: Loss:  0.4519, Accuracy: 0.8594\n",
      "Batch number : 130, Training: Loss:  0.3636, Accuracy: 0.9062\n",
      "Batch number : 131, Training: Loss:  0.3185, Accuracy: 0.9062\n",
      "Batch number : 132, Training: Loss:  0.5703, Accuracy: 0.8594\n",
      "Batch number : 133, Training: Loss:  0.4605, Accuracy: 0.8438\n",
      "Batch number : 134, Training: Loss:  0.2356, Accuracy: 0.9219\n",
      "Batch number : 135, Training: Loss:  0.3475, Accuracy: 0.8906\n",
      "Batch number : 136, Training: Loss:  0.6925, Accuracy: 0.8281\n",
      "Batch number : 137, Training: Loss:  0.2788, Accuracy: 0.9062\n",
      "Batch number : 138, Training: Loss:  0.4057, Accuracy: 0.9062\n",
      "Batch number : 139, Training: Loss:  0.5654, Accuracy: 0.8125\n",
      "Batch number : 140, Training: Loss:  0.6903, Accuracy: 0.8438\n",
      "Batch number : 141, Training: Loss:  0.4468, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.3750, Accuracy: 0.8594\n",
      "Batch number : 143, Training: Loss:  0.7307, Accuracy: 0.7656\n",
      "Batch number : 144, Training: Loss:  0.5136, Accuracy: 0.8281\n",
      "Batch number : 145, Training: Loss:  0.3826, Accuracy: 0.8750\n",
      "Batch number : 146, Training: Loss:  0.5409, Accuracy: 0.8438\n",
      "Batch number : 147, Training: Loss:  0.4830, Accuracy: 0.8594\n",
      "Batch number : 148, Training: Loss:  0.5571, Accuracy: 0.7812\n",
      "Batch number : 149, Training: Loss:  0.4845, Accuracy: 0.8594\n",
      "Batch number : 150, Training: Loss:  0.3167, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.7020, Accuracy: 0.7656\n",
      "Batch number : 152, Training: Loss:  0.4117, Accuracy: 0.8594\n",
      "Batch number : 153, Training: Loss:  0.2709, Accuracy: 0.9375\n",
      "Batch number : 154, Training: Loss:  0.4106, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.5391, Accuracy: 0.8750\n",
      "Batch number : 156, Training: Loss:  0.7129, Accuracy: 0.8125\n",
      "Batch number : 157, Training: Loss:  0.7966, Accuracy: 0.8125\n",
      "Batch number : 158, Training: Loss:  0.6002, Accuracy: 0.8438\n",
      "Batch number : 159, Training: Loss:  0.7658, Accuracy: 0.7500\n",
      "Batch number : 160, Training: Loss:  0.2729, Accuracy: 0.8906\n",
      "Batch number : 161, Training: Loss:  0.4767, Accuracy: 0.8750\n",
      "Batch number : 162, Training: Loss:  0.4747, Accuracy: 0.8594\n",
      "Batch number : 163, Training: Loss:  0.3043, Accuracy: 0.9531\n",
      "Batch number : 164, Training: Loss:  0.4048, Accuracy: 0.8594\n",
      "Batch number : 165, Training: Loss:  0.3210, Accuracy: 0.9375\n",
      "Batch number : 166, Training: Loss:  0.4406, Accuracy: 0.8750\n",
      "Batch number : 167, Training: Loss:  0.4514, Accuracy: 0.8438\n",
      "Batch number : 168, Training: Loss:  0.4310, Accuracy: 0.8906\n",
      "Batch number : 169, Training: Loss:  0.6345, Accuracy: 0.8438\n",
      "Batch number : 170, Training: Loss:  0.6865, Accuracy: 0.7969\n",
      "Batch number : 171, Training: Loss:  0.3855, Accuracy: 0.8906\n",
      "Batch number : 172, Training: Loss:  0.4988, Accuracy: 0.8281\n",
      "Batch number : 173, Training: Loss:  0.3542, Accuracy: 0.8906\n",
      "Batch number : 174, Training: Loss:  0.4544, Accuracy: 0.8906\n",
      "Batch number : 175, Training: Loss:  0.3679, Accuracy: 0.8750\n",
      "Batch number : 176, Training: Loss:  0.5596, Accuracy: 0.8281\n",
      "Batch number : 177, Training: Loss:  0.4218, Accuracy: 0.9219\n",
      "Batch number : 178, Training: Loss:  0.7177, Accuracy: 0.7812\n",
      "Batch number : 179, Training: Loss:  0.2941, Accuracy: 0.9219\n",
      "Batch number : 180, Training: Loss:  0.4047, Accuracy: 0.9062\n",
      "Batch number : 181, Training: Loss:  0.5922, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.4361, Accuracy: 0.8750\n",
      "Batch number : 183, Training: Loss:  0.3840, Accuracy: 0.8906\n",
      "Batch number : 184, Training: Loss:  0.6861, Accuracy: 0.7656\n",
      "Batch number : 185, Training: Loss:  0.4068, Accuracy: 0.8906\n",
      "Batch number : 186, Training: Loss:  0.5380, Accuracy: 0.8594\n",
      "Batch number : 187, Training: Loss:  0.6120, Accuracy: 0.8438\n",
      "Batch number : 188, Training: Loss:  0.6050, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.5106, Accuracy: 0.8750\n",
      "Batch number : 190, Training: Loss:  0.3923, Accuracy: 0.9219\n",
      "Batch number : 191, Training: Loss:  0.5801, Accuracy: 0.7969\n",
      "Batch number : 192, Training: Loss:  0.4128, Accuracy: 0.8906\n",
      "Batch number : 193, Training: Loss:  0.3469, Accuracy: 0.8906\n",
      "Batch number : 194, Training: Loss:  0.6253, Accuracy: 0.8125\n",
      "Batch number : 195, Training: Loss:  0.5080, Accuracy: 0.8594\n",
      "Batch number : 196, Training: Loss:  0.6983, Accuracy: 0.7812\n",
      "Batch number : 197, Training: Loss:  0.3517, Accuracy: 0.8906\n",
      "Batch number : 198, Training: Loss:  0.3646, Accuracy: 0.9375\n",
      "Batch number : 199, Training: Loss:  0.4110, Accuracy: 0.8438\n",
      "Batch number : 200, Training: Loss:  0.4266, Accuracy: 0.8750\n",
      "Batch number : 201, Training: Loss:  0.3385, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.3647, Accuracy: 0.8906\n",
      "Batch number : 203, Training: Loss:  0.5552, Accuracy: 0.8750\n",
      "Batch number : 204, Training: Loss:  0.4363, Accuracy: 0.8438\n",
      "Batch number : 205, Training: Loss:  0.4629, Accuracy: 0.8906\n",
      "Batch number : 206, Training: Loss:  0.3555, Accuracy: 0.9062\n",
      "Batch number : 207, Training: Loss:  0.3832, Accuracy: 0.8594\n",
      "Batch number : 208, Training: Loss:  0.5312, Accuracy: 0.8438\n",
      "Batch number : 209, Training: Loss:  0.5417, Accuracy: 0.8438\n",
      "Batch number : 210, Training: Loss:  0.3586, Accuracy: 0.8906\n",
      "Batch number : 211, Training: Loss:  0.5884, Accuracy: 0.8281\n",
      "Batch number : 212, Training: Loss:  0.6401, Accuracy: 0.7500\n",
      "Batch number : 213, Training: Loss:  0.6268, Accuracy: 0.7656\n",
      "Batch number : 214, Training: Loss:  0.5402, Accuracy: 0.8125\n",
      "Batch number : 215, Training: Loss:  0.5430, Accuracy: 0.8438\n",
      "Batch number : 216, Training: Loss:  0.6832, Accuracy: 0.7969\n",
      "Batch number : 217, Training: Loss:  0.5153, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.5592, Accuracy: 0.8594\n",
      "Batch number : 219, Training: Loss:  0.3605, Accuracy: 0.9219\n",
      "Batch number : 220, Training: Loss:  0.4949, Accuracy: 0.8750\n",
      "Batch number : 221, Training: Loss:  0.5363, Accuracy: 0.8438\n",
      "Batch number : 222, Training: Loss:  0.4724, Accuracy: 0.8594\n",
      "Batch number : 223, Training: Loss:  0.5396, Accuracy: 0.8438\n",
      "Batch number : 224, Training: Loss:  0.2779, Accuracy: 0.9219\n",
      "Batch number : 225, Training: Loss:  0.2446, Accuracy: 0.9375\n",
      "Batch number : 226, Training: Loss:  0.5820, Accuracy: 0.8281\n",
      "Batch number : 227, Training: Loss:  0.4335, Accuracy: 0.8906\n",
      "Batch number : 228, Training: Loss:  0.6808, Accuracy: 0.8438\n",
      "Batch number : 229, Training: Loss:  0.4142, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.2706, Accuracy: 0.9219\n",
      "Batch number : 231, Training: Loss:  0.4638, Accuracy: 0.8750\n",
      "Batch number : 232, Training: Loss:  0.5903, Accuracy: 0.8281\n",
      "Batch number : 233, Training: Loss:  0.6229, Accuracy: 0.7812\n",
      "Batch number : 234, Training: Loss:  0.6736, Accuracy: 0.7812\n",
      "Batch number : 235, Training: Loss:  0.5934, Accuracy: 0.8281\n",
      "Batch number : 236, Training: Loss:  0.4074, Accuracy: 0.8906\n",
      "Batch number : 237, Training: Loss:  0.4456, Accuracy: 0.8750\n",
      "Batch number : 238, Training: Loss:  0.5511, Accuracy: 0.8594\n",
      "Batch number : 239, Training: Loss:  0.4812, Accuracy: 0.8594\n",
      "Batch number : 240, Training: Loss:  0.3816, Accuracy: 0.9375\n",
      "Batch number : 241, Training: Loss:  0.5207, Accuracy: 0.8125\n",
      "Batch number : 242, Training: Loss:  0.5154, Accuracy: 0.8594\n",
      "Batch number : 243, Training: Loss:  0.3532, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.5761, Accuracy: 0.8125\n",
      "Batch number : 245, Training: Loss:  0.5090, Accuracy: 0.8594\n",
      "Batch number : 246, Training: Loss:  0.5271, Accuracy: 0.8438\n",
      "Batch number : 247, Training: Loss:  0.5274, Accuracy: 0.8438\n",
      "Batch number : 248, Training: Loss:  0.6791, Accuracy: 0.7812\n",
      "Batch number : 249, Training: Loss:  0.4915, Accuracy: 0.8281\n",
      "Batch number : 250, Training: Loss:  0.4819, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.5315, Accuracy: 0.8438\n",
      "Batch number : 252, Training: Loss:  0.5299, Accuracy: 0.8750\n",
      "Batch number : 253, Training: Loss:  0.4542, Accuracy: 0.8906\n",
      "Batch number : 254, Training: Loss:  0.4255, Accuracy: 0.8906\n",
      "Batch number : 255, Training: Loss:  0.3965, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.2849, Accuracy: 0.9219\n",
      "Batch number : 257, Training: Loss:  0.5742, Accuracy: 0.8438\n",
      "Batch number : 258, Training: Loss:  0.6969, Accuracy: 0.8281\n",
      "Batch number : 259, Training: Loss:  0.2994, Accuracy: 0.9219\n",
      "Batch number : 260, Training: Loss:  0.6428, Accuracy: 0.7969\n",
      "Batch number : 261, Training: Loss:  0.4934, Accuracy: 0.8594\n",
      "Batch number : 262, Training: Loss:  0.5452, Accuracy: 0.8594\n",
      "Batch number : 263, Training: Loss:  0.4209, Accuracy: 0.8750\n",
      "Batch number : 264, Training: Loss:  0.7079, Accuracy: 0.7812\n",
      "Batch number : 265, Training: Loss:  0.4574, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.5181, Accuracy: 0.8438\n",
      "Batch number : 267, Training: Loss:  0.3659, Accuracy: 0.9219\n",
      "Batch number : 268, Training: Loss:  0.5857, Accuracy: 0.8594\n",
      "Batch number : 269, Training: Loss:  0.6813, Accuracy: 0.8125\n",
      "Batch number : 270, Training: Loss:  0.3998, Accuracy: 0.8906\n",
      "Batch number : 271, Training: Loss:  0.6081, Accuracy: 0.8125\n",
      "Batch number : 272, Training: Loss:  0.4579, Accuracy: 0.8125\n",
      "Batch number : 273, Training: Loss:  0.3521, Accuracy: 0.9062\n",
      "Batch number : 274, Training: Loss:  0.4557, Accuracy: 0.8438\n",
      "Batch number : 275, Training: Loss:  0.4975, Accuracy: 0.8594\n",
      "Batch number : 276, Training: Loss:  0.5608, Accuracy: 0.8125\n",
      "Batch number : 277, Training: Loss:  0.6229, Accuracy: 0.7656\n",
      "Batch number : 278, Training: Loss:  0.7623, Accuracy: 0.7656\n",
      "Batch number : 279, Training: Loss:  0.4243, Accuracy: 0.8750\n",
      "Batch number : 280, Training: Loss:  0.5847, Accuracy: 0.7812\n",
      "Batch number : 281, Training: Loss:  0.3820, Accuracy: 0.9062\n",
      "Batch number : 282, Training: Loss:  0.4847, Accuracy: 0.8594\n",
      "Batch number : 283, Training: Loss:  0.4259, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.6064, Accuracy: 0.8281\n",
      "Batch number : 285, Training: Loss:  0.4723, Accuracy: 0.8906\n",
      "Batch number : 286, Training: Loss:  0.4804, Accuracy: 0.8438\n",
      "Batch number : 287, Training: Loss:  0.4982, Accuracy: 0.8750\n",
      "Batch number : 288, Training: Loss:  0.3229, Accuracy: 0.9375\n",
      "Batch number : 289, Training: Loss:  0.5316, Accuracy: 0.8125\n",
      "Batch number : 290, Training: Loss:  0.5860, Accuracy: 0.8438\n",
      "Batch number : 291, Training: Loss:  0.4857, Accuracy: 0.8750\n",
      "Batch number : 292, Training: Loss:  0.6306, Accuracy: 0.8125\n",
      "Batch number : 293, Training: Loss:  0.6014, Accuracy: 0.8125\n",
      "Batch number : 294, Training: Loss:  0.5846, Accuracy: 0.8281\n",
      "Batch number : 295, Training: Loss:  0.3648, Accuracy: 0.9219\n",
      "Batch number : 296, Training: Loss:  0.5554, Accuracy: 0.8281\n",
      "Batch number : 297, Training: Loss:  0.3372, Accuracy: 0.8906\n",
      "Batch number : 298, Training: Loss:  0.5969, Accuracy: 0.8125\n",
      "Batch number : 299, Training: Loss:  0.5102, Accuracy: 0.8594\n",
      "Batch number : 300, Training: Loss:  0.5971, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.3838, Accuracy: 0.8906\n",
      "Batch number : 302, Training: Loss:  0.5105, Accuracy: 0.8281\n",
      "Batch number : 303, Training: Loss:  0.4070, Accuracy: 0.9062\n",
      "Batch number : 304, Training: Loss:  0.4553, Accuracy: 0.8750\n",
      "Batch number : 305, Training: Loss:  0.4416, Accuracy: 0.8750\n",
      "Batch number : 306, Training: Loss:  0.3973, Accuracy: 0.8594\n",
      "Batch number : 307, Training: Loss:  0.4420, Accuracy: 0.8125\n",
      "Batch number : 308, Training: Loss:  0.2435, Accuracy: 0.9531\n",
      "Batch number : 309, Training: Loss:  0.4462, Accuracy: 0.8906\n",
      "Batch number : 310, Training: Loss:  0.3998, Accuracy: 0.9062\n",
      "Batch number : 311, Training: Loss:  0.4061, Accuracy: 0.9062\n",
      "Batch number : 312, Training: Loss:  0.4727, Accuracy: 0.8438\n",
      "Batch number : 313, Training: Loss:  0.7808, Accuracy: 0.7812\n",
      "Batch number : 314, Training: Loss:  0.4377, Accuracy: 0.8594\n",
      "Batch number : 315, Training: Loss:  0.6112, Accuracy: 0.8281\n",
      "Batch number : 316, Training: Loss:  0.4084, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.3544, Accuracy: 0.8906\n",
      "Batch number : 318, Training: Loss:  0.2963, Accuracy: 0.9219\n",
      "Batch number : 319, Training: Loss:  0.4218, Accuracy: 0.8594\n",
      "Batch number : 320, Training: Loss:  0.4404, Accuracy: 0.8906\n",
      "Batch number : 321, Training: Loss:  0.3440, Accuracy: 0.9062\n",
      "Batch number : 322, Training: Loss:  0.5195, Accuracy: 0.8438\n",
      "Batch number : 323, Training: Loss:  0.3914, Accuracy: 0.8594\n",
      "Batch number : 324, Training: Loss:  0.5729, Accuracy: 0.8281\n",
      "Batch number : 325, Training: Loss:  0.6656, Accuracy: 0.8125\n",
      "Batch number : 326, Training: Loss:  0.3917, Accuracy: 0.8594\n",
      "Batch number : 327, Training: Loss:  0.5665, Accuracy: 0.7812\n",
      "Batch number : 328, Training: Loss:  0.4172, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.4891, Accuracy: 0.8594\n",
      "Batch number : 330, Training: Loss:  0.4837, Accuracy: 0.8438\n",
      "Batch number : 331, Training: Loss:  0.5235, Accuracy: 0.8125\n",
      "Batch number : 332, Training: Loss:  0.4849, Accuracy: 0.8438\n",
      "Batch number : 333, Training: Loss:  0.5307, Accuracy: 0.8438\n",
      "Batch number : 334, Training: Loss:  0.5150, Accuracy: 0.8281\n",
      "Batch number : 335, Training: Loss:  0.4380, Accuracy: 0.8594\n",
      "Batch number : 336, Training: Loss:  0.5232, Accuracy: 0.8594\n",
      "Batch number : 337, Training: Loss:  0.4502, Accuracy: 0.8438\n",
      "Batch number : 338, Training: Loss:  0.6674, Accuracy: 0.7500\n",
      "Batch number : 339, Training: Loss:  0.5382, Accuracy: 0.8438\n",
      "Batch number : 340, Training: Loss:  0.5001, Accuracy: 0.8594\n",
      "Batch number : 341, Training: Loss:  0.5090, Accuracy: 0.8594\n",
      "Batch number : 342, Training: Loss:  0.5269, Accuracy: 0.8750\n",
      "Batch number : 343, Training: Loss:  0.4027, Accuracy: 0.8750\n",
      "Batch number : 344, Training: Loss:  0.3222, Accuracy: 0.8750\n",
      "Batch number : 345, Training: Loss:  0.2904, Accuracy: 0.9375\n",
      "Batch number : 346, Training: Loss:  0.4771, Accuracy: 0.8438\n",
      "Batch number : 347, Training: Loss:  0.1297, Accuracy: 0.9844\n",
      "Batch number : 348, Training: Loss:  0.5616, Accuracy: 0.8438\n",
      "Batch number : 349, Training: Loss:  0.6005, Accuracy: 0.8125\n",
      "Batch number : 350, Training: Loss:  0.5074, Accuracy: 0.8594\n",
      "Batch number : 351, Training: Loss:  0.5553, Accuracy: 0.8125\n",
      "Batch number : 352, Training: Loss:  0.4803, Accuracy: 0.9062\n",
      "Batch number : 353, Training: Loss:  0.3844, Accuracy: 0.8906\n",
      "Batch number : 354, Training: Loss:  0.3535, Accuracy: 0.8750\n",
      "Batch number : 355, Training: Loss:  0.3486, Accuracy: 0.9062\n",
      "Batch number : 356, Training: Loss:  0.3580, Accuracy: 0.9062\n",
      "Batch number : 357, Training: Loss:  0.5136, Accuracy: 0.8281\n",
      "Batch number : 358, Training: Loss:  0.3092, Accuracy: 0.9062\n",
      "Batch number : 359, Training: Loss:  0.5892, Accuracy: 0.8281\n",
      "Batch number : 360, Training: Loss:  0.3542, Accuracy: 0.8906\n",
      "Batch number : 361, Training: Loss:  0.4045, Accuracy: 0.8594\n",
      "Batch number : 362, Training: Loss:  0.3113, Accuracy: 0.9219\n",
      "Batch number : 363, Training: Loss:  0.4781, Accuracy: 0.8594\n",
      "Batch number : 364, Training: Loss:  0.4004, Accuracy: 0.8906\n",
      "Batch number : 365, Training: Loss:  0.5527, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.6579, Accuracy: 0.8281\n",
      "Batch number : 367, Training: Loss:  0.6103, Accuracy: 0.8438\n",
      "Batch number : 368, Training: Loss:  0.3979, Accuracy: 0.8906\n",
      "Batch number : 369, Training: Loss:  0.4397, Accuracy: 0.8906\n",
      "Batch number : 370, Training: Loss:  0.6279, Accuracy: 0.8281\n",
      "Batch number : 371, Training: Loss:  0.2980, Accuracy: 0.9375\n",
      "Batch number : 372, Training: Loss:  0.3622, Accuracy: 0.8906\n",
      "Batch number : 373, Training: Loss:  0.4852, Accuracy: 0.8438\n",
      "Batch number : 374, Training: Loss:  0.4625, Accuracy: 0.8438\n",
      "Batch number : 375, Training: Loss:  0.5644, Accuracy: 0.8281\n",
      "Batch number : 376, Training: Loss:  0.5059, Accuracy: 0.8438\n",
      "Batch number : 377, Training: Loss:  0.2259, Accuracy: 0.9750\n",
      "Epoch: 9/20\n",
      "Batch number : 000, Training: Loss:  0.5936, Accuracy: 0.7812\n",
      "Batch number : 001, Training: Loss:  0.3294, Accuracy: 0.8906\n",
      "Batch number : 002, Training: Loss:  0.4054, Accuracy: 0.8438\n",
      "Batch number : 003, Training: Loss:  0.3632, Accuracy: 0.8906\n",
      "Batch number : 004, Training: Loss:  0.5044, Accuracy: 0.8438\n",
      "Batch number : 005, Training: Loss:  0.2563, Accuracy: 0.9375\n",
      "Batch number : 006, Training: Loss:  0.4114, Accuracy: 0.8594\n",
      "Batch number : 007, Training: Loss:  0.5054, Accuracy: 0.8594\n",
      "Batch number : 008, Training: Loss:  0.4439, Accuracy: 0.8438\n",
      "Batch number : 009, Training: Loss:  0.7549, Accuracy: 0.7969\n",
      "Batch number : 010, Training: Loss:  0.4088, Accuracy: 0.9062\n",
      "Batch number : 011, Training: Loss:  0.4211, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.4206, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.6152, Accuracy: 0.7812\n",
      "Batch number : 014, Training: Loss:  0.5913, Accuracy: 0.7812\n",
      "Batch number : 015, Training: Loss:  0.4350, Accuracy: 0.8750\n",
      "Batch number : 016, Training: Loss:  0.5129, Accuracy: 0.8125\n",
      "Batch number : 017, Training: Loss:  0.4206, Accuracy: 0.8750\n",
      "Batch number : 018, Training: Loss:  0.7353, Accuracy: 0.7812\n",
      "Batch number : 019, Training: Loss:  0.3387, Accuracy: 0.9375\n",
      "Batch number : 020, Training: Loss:  0.3166, Accuracy: 0.9219\n",
      "Batch number : 021, Training: Loss:  0.2780, Accuracy: 0.9375\n",
      "Batch number : 022, Training: Loss:  0.5816, Accuracy: 0.8438\n",
      "Batch number : 023, Training: Loss:  0.4470, Accuracy: 0.8750\n",
      "Batch number : 024, Training: Loss:  0.5049, Accuracy: 0.8438\n",
      "Batch number : 025, Training: Loss:  0.3501, Accuracy: 0.9062\n",
      "Batch number : 026, Training: Loss:  0.3750, Accuracy: 0.8906\n",
      "Batch number : 027, Training: Loss:  0.5903, Accuracy: 0.8281\n",
      "Batch number : 028, Training: Loss:  0.4561, Accuracy: 0.8281\n",
      "Batch number : 029, Training: Loss:  0.3426, Accuracy: 0.8750\n",
      "Batch number : 030, Training: Loss:  0.3945, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.5274, Accuracy: 0.8125\n",
      "Batch number : 032, Training: Loss:  0.5915, Accuracy: 0.8438\n",
      "Batch number : 033, Training: Loss:  0.2909, Accuracy: 0.9062\n",
      "Batch number : 034, Training: Loss:  0.5284, Accuracy: 0.8281\n",
      "Batch number : 035, Training: Loss:  0.4669, Accuracy: 0.8438\n",
      "Batch number : 036, Training: Loss:  0.3269, Accuracy: 0.9219\n",
      "Batch number : 037, Training: Loss:  0.3399, Accuracy: 0.9062\n",
      "Batch number : 038, Training: Loss:  0.4488, Accuracy: 0.8125\n",
      "Batch number : 039, Training: Loss:  0.4950, Accuracy: 0.8125\n",
      "Batch number : 040, Training: Loss:  0.4173, Accuracy: 0.8438\n",
      "Batch number : 041, Training: Loss:  0.6566, Accuracy: 0.8750\n",
      "Batch number : 042, Training: Loss:  0.6036, Accuracy: 0.8281\n",
      "Batch number : 043, Training: Loss:  0.6179, Accuracy: 0.7969\n",
      "Batch number : 044, Training: Loss:  0.4314, Accuracy: 0.8438\n",
      "Batch number : 045, Training: Loss:  0.5191, Accuracy: 0.8438\n",
      "Batch number : 046, Training: Loss:  0.2596, Accuracy: 0.9375\n",
      "Batch number : 047, Training: Loss:  0.3236, Accuracy: 0.9219\n",
      "Batch number : 048, Training: Loss:  0.4855, Accuracy: 0.8125\n",
      "Batch number : 049, Training: Loss:  0.6353, Accuracy: 0.8281\n",
      "Batch number : 050, Training: Loss:  0.4846, Accuracy: 0.8281\n",
      "Batch number : 051, Training: Loss:  0.5102, Accuracy: 0.8594\n",
      "Batch number : 052, Training: Loss:  0.3391, Accuracy: 0.9219\n",
      "Batch number : 053, Training: Loss:  0.4420, Accuracy: 0.8906\n",
      "Batch number : 054, Training: Loss:  0.5913, Accuracy: 0.8281\n",
      "Batch number : 055, Training: Loss:  0.6379, Accuracy: 0.8125\n",
      "Batch number : 056, Training: Loss:  0.4089, Accuracy: 0.9062\n",
      "Batch number : 057, Training: Loss:  0.3156, Accuracy: 0.9219\n",
      "Batch number : 058, Training: Loss:  0.3743, Accuracy: 0.8906\n",
      "Batch number : 059, Training: Loss:  0.5472, Accuracy: 0.8438\n",
      "Batch number : 060, Training: Loss:  0.3454, Accuracy: 0.8906\n",
      "Batch number : 061, Training: Loss:  0.3796, Accuracy: 0.8906\n",
      "Batch number : 062, Training: Loss:  0.3838, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.3444, Accuracy: 0.9062\n",
      "Batch number : 064, Training: Loss:  0.2159, Accuracy: 0.9531\n",
      "Batch number : 065, Training: Loss:  0.5028, Accuracy: 0.8750\n",
      "Batch number : 066, Training: Loss:  0.5008, Accuracy: 0.8438\n",
      "Batch number : 067, Training: Loss:  0.5631, Accuracy: 0.8438\n",
      "Batch number : 068, Training: Loss:  0.6804, Accuracy: 0.8125\n",
      "Batch number : 069, Training: Loss:  0.6739, Accuracy: 0.7969\n",
      "Batch number : 070, Training: Loss:  0.5414, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.4190, Accuracy: 0.8750\n",
      "Batch number : 072, Training: Loss:  0.6119, Accuracy: 0.8281\n",
      "Batch number : 073, Training: Loss:  0.4351, Accuracy: 0.8438\n",
      "Batch number : 074, Training: Loss:  0.3917, Accuracy: 0.9219\n",
      "Batch number : 075, Training: Loss:  0.5488, Accuracy: 0.8281\n",
      "Batch number : 076, Training: Loss:  0.4814, Accuracy: 0.8906\n",
      "Batch number : 077, Training: Loss:  0.5388, Accuracy: 0.8125\n",
      "Batch number : 078, Training: Loss:  0.4420, Accuracy: 0.8281\n",
      "Batch number : 079, Training: Loss:  0.4293, Accuracy: 0.8750\n",
      "Batch number : 080, Training: Loss:  0.5821, Accuracy: 0.8125\n",
      "Batch number : 081, Training: Loss:  0.5075, Accuracy: 0.8594\n",
      "Batch number : 082, Training: Loss:  0.4587, Accuracy: 0.8125\n",
      "Batch number : 083, Training: Loss:  0.3769, Accuracy: 0.9062\n",
      "Batch number : 084, Training: Loss:  0.4016, Accuracy: 0.8750\n",
      "Batch number : 085, Training: Loss:  0.5187, Accuracy: 0.8438\n",
      "Batch number : 086, Training: Loss:  0.6426, Accuracy: 0.8281\n",
      "Batch number : 087, Training: Loss:  0.6440, Accuracy: 0.7969\n",
      "Batch number : 088, Training: Loss:  0.3758, Accuracy: 0.8906\n",
      "Batch number : 089, Training: Loss:  0.4883, Accuracy: 0.8281\n",
      "Batch number : 090, Training: Loss:  0.6445, Accuracy: 0.7812\n",
      "Batch number : 091, Training: Loss:  0.5868, Accuracy: 0.8125\n",
      "Batch number : 092, Training: Loss:  0.4187, Accuracy: 0.8438\n",
      "Batch number : 093, Training: Loss:  0.4320, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.3220, Accuracy: 0.9375\n",
      "Batch number : 095, Training: Loss:  0.2968, Accuracy: 0.9219\n",
      "Batch number : 096, Training: Loss:  0.5937, Accuracy: 0.8125\n",
      "Batch number : 097, Training: Loss:  0.5209, Accuracy: 0.8438\n",
      "Batch number : 098, Training: Loss:  0.6425, Accuracy: 0.8281\n",
      "Batch number : 099, Training: Loss:  0.4372, Accuracy: 0.8750\n",
      "Batch number : 100, Training: Loss:  0.4411, Accuracy: 0.8906\n",
      "Batch number : 101, Training: Loss:  0.3539, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.5240, Accuracy: 0.8281\n",
      "Batch number : 103, Training: Loss:  0.5901, Accuracy: 0.8125\n",
      "Batch number : 104, Training: Loss:  0.6758, Accuracy: 0.8281\n",
      "Batch number : 105, Training: Loss:  0.3848, Accuracy: 0.9219\n",
      "Batch number : 106, Training: Loss:  0.3324, Accuracy: 0.8906\n",
      "Batch number : 107, Training: Loss:  0.3211, Accuracy: 0.9062\n",
      "Batch number : 108, Training: Loss:  0.4703, Accuracy: 0.8594\n",
      "Batch number : 109, Training: Loss:  0.5862, Accuracy: 0.8438\n",
      "Batch number : 110, Training: Loss:  0.3396, Accuracy: 0.9219\n",
      "Batch number : 111, Training: Loss:  0.5548, Accuracy: 0.8281\n",
      "Batch number : 112, Training: Loss:  0.3883, Accuracy: 0.9219\n",
      "Batch number : 113, Training: Loss:  0.3011, Accuracy: 0.9219\n",
      "Batch number : 114, Training: Loss:  0.4091, Accuracy: 0.8750\n",
      "Batch number : 115, Training: Loss:  0.5609, Accuracy: 0.8594\n",
      "Batch number : 116, Training: Loss:  0.5277, Accuracy: 0.8438\n",
      "Batch number : 117, Training: Loss:  0.4062, Accuracy: 0.8750\n",
      "Batch number : 118, Training: Loss:  0.4013, Accuracy: 0.9062\n",
      "Batch number : 119, Training: Loss:  0.4158, Accuracy: 0.9062\n",
      "Batch number : 120, Training: Loss:  0.3392, Accuracy: 0.9062\n",
      "Batch number : 121, Training: Loss:  0.6167, Accuracy: 0.8125\n",
      "Batch number : 122, Training: Loss:  0.4202, Accuracy: 0.8750\n",
      "Batch number : 123, Training: Loss:  0.3773, Accuracy: 0.8750\n",
      "Batch number : 124, Training: Loss:  0.4788, Accuracy: 0.8594\n",
      "Batch number : 125, Training: Loss:  0.4648, Accuracy: 0.8594\n",
      "Batch number : 126, Training: Loss:  0.4766, Accuracy: 0.8281\n",
      "Batch number : 127, Training: Loss:  0.2829, Accuracy: 0.9062\n",
      "Batch number : 128, Training: Loss:  0.6112, Accuracy: 0.8281\n",
      "Batch number : 129, Training: Loss:  0.3962, Accuracy: 0.8906\n",
      "Batch number : 130, Training: Loss:  0.7193, Accuracy: 0.8125\n",
      "Batch number : 131, Training: Loss:  0.6549, Accuracy: 0.7969\n",
      "Batch number : 132, Training: Loss:  0.4929, Accuracy: 0.7969\n",
      "Batch number : 133, Training: Loss:  0.5299, Accuracy: 0.8125\n",
      "Batch number : 134, Training: Loss:  0.5004, Accuracy: 0.8125\n",
      "Batch number : 135, Training: Loss:  0.5475, Accuracy: 0.7969\n",
      "Batch number : 136, Training: Loss:  0.4013, Accuracy: 0.8594\n",
      "Batch number : 137, Training: Loss:  0.5794, Accuracy: 0.8125\n",
      "Batch number : 138, Training: Loss:  0.5098, Accuracy: 0.8438\n",
      "Batch number : 139, Training: Loss:  0.5435, Accuracy: 0.8438\n",
      "Batch number : 140, Training: Loss:  0.5022, Accuracy: 0.8438\n",
      "Batch number : 141, Training: Loss:  0.5559, Accuracy: 0.8281\n",
      "Batch number : 142, Training: Loss:  0.3516, Accuracy: 0.8906\n",
      "Batch number : 143, Training: Loss:  0.4226, Accuracy: 0.8906\n",
      "Batch number : 144, Training: Loss:  0.4254, Accuracy: 0.9062\n",
      "Batch number : 145, Training: Loss:  0.3652, Accuracy: 0.8906\n",
      "Batch number : 146, Training: Loss:  0.3713, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.5922, Accuracy: 0.8281\n",
      "Batch number : 148, Training: Loss:  0.4796, Accuracy: 0.8750\n",
      "Batch number : 149, Training: Loss:  0.3738, Accuracy: 0.8906\n",
      "Batch number : 150, Training: Loss:  0.5542, Accuracy: 0.8594\n",
      "Batch number : 151, Training: Loss:  0.3653, Accuracy: 0.8906\n",
      "Batch number : 152, Training: Loss:  0.3450, Accuracy: 0.8906\n",
      "Batch number : 153, Training: Loss:  0.3546, Accuracy: 0.9062\n",
      "Batch number : 154, Training: Loss:  0.4502, Accuracy: 0.8750\n",
      "Batch number : 155, Training: Loss:  0.4824, Accuracy: 0.8594\n",
      "Batch number : 156, Training: Loss:  0.3677, Accuracy: 0.8906\n",
      "Batch number : 157, Training: Loss:  0.6391, Accuracy: 0.7969\n",
      "Batch number : 158, Training: Loss:  0.3907, Accuracy: 0.8438\n",
      "Batch number : 159, Training: Loss:  0.5452, Accuracy: 0.8125\n",
      "Batch number : 160, Training: Loss:  0.6283, Accuracy: 0.8281\n",
      "Batch number : 161, Training: Loss:  0.5864, Accuracy: 0.7969\n",
      "Batch number : 162, Training: Loss:  0.4344, Accuracy: 0.8750\n",
      "Batch number : 163, Training: Loss:  0.4915, Accuracy: 0.8438\n",
      "Batch number : 164, Training: Loss:  0.3804, Accuracy: 0.9062\n",
      "Batch number : 165, Training: Loss:  0.4722, Accuracy: 0.8438\n",
      "Batch number : 166, Training: Loss:  0.6456, Accuracy: 0.7812\n",
      "Batch number : 167, Training: Loss:  0.5889, Accuracy: 0.8125\n",
      "Batch number : 168, Training: Loss:  0.4782, Accuracy: 0.8438\n",
      "Batch number : 169, Training: Loss:  0.5663, Accuracy: 0.8125\n",
      "Batch number : 170, Training: Loss:  0.4918, Accuracy: 0.8438\n",
      "Batch number : 171, Training: Loss:  0.4790, Accuracy: 0.8438\n",
      "Batch number : 172, Training: Loss:  0.4176, Accuracy: 0.9219\n",
      "Batch number : 173, Training: Loss:  0.5609, Accuracy: 0.8438\n",
      "Batch number : 174, Training: Loss:  0.4469, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.4831, Accuracy: 0.8125\n",
      "Batch number : 176, Training: Loss:  0.4006, Accuracy: 0.9062\n",
      "Batch number : 177, Training: Loss:  0.4752, Accuracy: 0.8750\n",
      "Batch number : 178, Training: Loss:  0.4642, Accuracy: 0.8125\n",
      "Batch number : 179, Training: Loss:  0.3650, Accuracy: 0.9062\n",
      "Batch number : 180, Training: Loss:  0.3174, Accuracy: 0.9062\n",
      "Batch number : 181, Training: Loss:  0.5289, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.3755, Accuracy: 0.9062\n",
      "Batch number : 183, Training: Loss:  0.5405, Accuracy: 0.8125\n",
      "Batch number : 184, Training: Loss:  0.5085, Accuracy: 0.8438\n",
      "Batch number : 185, Training: Loss:  0.4089, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.3909, Accuracy: 0.8906\n",
      "Batch number : 187, Training: Loss:  0.4755, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.5552, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.6717, Accuracy: 0.8125\n",
      "Batch number : 190, Training: Loss:  0.2767, Accuracy: 0.9375\n",
      "Batch number : 191, Training: Loss:  0.3587, Accuracy: 0.9062\n",
      "Batch number : 192, Training: Loss:  0.5442, Accuracy: 0.8281\n",
      "Batch number : 193, Training: Loss:  0.7604, Accuracy: 0.7812\n",
      "Batch number : 194, Training: Loss:  0.7522, Accuracy: 0.8750\n",
      "Batch number : 195, Training: Loss:  0.5869, Accuracy: 0.7969\n",
      "Batch number : 196, Training: Loss:  0.3386, Accuracy: 0.8906\n",
      "Batch number : 197, Training: Loss:  0.7816, Accuracy: 0.7656\n",
      "Batch number : 198, Training: Loss:  0.3749, Accuracy: 0.9219\n",
      "Batch number : 199, Training: Loss:  0.5410, Accuracy: 0.7812\n",
      "Batch number : 200, Training: Loss:  0.5768, Accuracy: 0.7969\n",
      "Batch number : 201, Training: Loss:  0.4014, Accuracy: 0.8906\n",
      "Batch number : 202, Training: Loss:  0.6235, Accuracy: 0.8125\n",
      "Batch number : 203, Training: Loss:  0.5238, Accuracy: 0.8281\n",
      "Batch number : 204, Training: Loss:  0.6757, Accuracy: 0.8438\n",
      "Batch number : 205, Training: Loss:  0.3737, Accuracy: 0.9062\n",
      "Batch number : 206, Training: Loss:  0.5083, Accuracy: 0.8438\n",
      "Batch number : 207, Training: Loss:  0.4047, Accuracy: 0.8750\n",
      "Batch number : 208, Training: Loss:  0.4998, Accuracy: 0.8281\n",
      "Batch number : 209, Training: Loss:  0.4459, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.4716, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.5262, Accuracy: 0.8438\n",
      "Batch number : 212, Training: Loss:  0.3975, Accuracy: 0.9062\n",
      "Batch number : 213, Training: Loss:  0.4936, Accuracy: 0.8438\n",
      "Batch number : 214, Training: Loss:  0.5228, Accuracy: 0.8906\n",
      "Batch number : 215, Training: Loss:  0.5076, Accuracy: 0.8750\n",
      "Batch number : 216, Training: Loss:  0.7895, Accuracy: 0.7656\n",
      "Batch number : 217, Training: Loss:  0.4073, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.5226, Accuracy: 0.8438\n",
      "Batch number : 219, Training: Loss:  0.4512, Accuracy: 0.8281\n",
      "Batch number : 220, Training: Loss:  0.4047, Accuracy: 0.9062\n",
      "Batch number : 221, Training: Loss:  0.4499, Accuracy: 0.9062\n",
      "Batch number : 222, Training: Loss:  0.4472, Accuracy: 0.8750\n",
      "Batch number : 223, Training: Loss:  0.4276, Accuracy: 0.8750\n",
      "Batch number : 224, Training: Loss:  0.4085, Accuracy: 0.8750\n",
      "Batch number : 225, Training: Loss:  0.4861, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.7540, Accuracy: 0.8125\n",
      "Batch number : 227, Training: Loss:  0.3948, Accuracy: 0.8594\n",
      "Batch number : 228, Training: Loss:  0.3476, Accuracy: 0.9062\n",
      "Batch number : 229, Training: Loss:  0.5447, Accuracy: 0.8281\n",
      "Batch number : 230, Training: Loss:  0.4394, Accuracy: 0.8750\n",
      "Batch number : 231, Training: Loss:  0.6158, Accuracy: 0.7969\n",
      "Batch number : 232, Training: Loss:  0.5095, Accuracy: 0.8594\n",
      "Batch number : 233, Training: Loss:  0.5873, Accuracy: 0.8750\n",
      "Batch number : 234, Training: Loss:  0.4914, Accuracy: 0.8438\n",
      "Batch number : 235, Training: Loss:  0.5261, Accuracy: 0.8594\n",
      "Batch number : 236, Training: Loss:  0.4887, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.4969, Accuracy: 0.8750\n",
      "Batch number : 238, Training: Loss:  0.4861, Accuracy: 0.8594\n",
      "Batch number : 239, Training: Loss:  0.6547, Accuracy: 0.7969\n",
      "Batch number : 240, Training: Loss:  0.4303, Accuracy: 0.8594\n",
      "Batch number : 241, Training: Loss:  0.5161, Accuracy: 0.8594\n",
      "Batch number : 242, Training: Loss:  0.3572, Accuracy: 0.9062\n",
      "Batch number : 243, Training: Loss:  0.3360, Accuracy: 0.9062\n",
      "Batch number : 244, Training: Loss:  0.5537, Accuracy: 0.8594\n",
      "Batch number : 245, Training: Loss:  0.5149, Accuracy: 0.8281\n",
      "Batch number : 246, Training: Loss:  0.3543, Accuracy: 0.9219\n",
      "Batch number : 247, Training: Loss:  0.3713, Accuracy: 0.9062\n",
      "Batch number : 248, Training: Loss:  0.5051, Accuracy: 0.8594\n",
      "Batch number : 249, Training: Loss:  0.4618, Accuracy: 0.8750\n",
      "Batch number : 250, Training: Loss:  0.6767, Accuracy: 0.7812\n",
      "Batch number : 251, Training: Loss:  0.3767, Accuracy: 0.8906\n",
      "Batch number : 252, Training: Loss:  0.3785, Accuracy: 0.8906\n",
      "Batch number : 253, Training: Loss:  0.5368, Accuracy: 0.8438\n",
      "Batch number : 254, Training: Loss:  0.4293, Accuracy: 0.8906\n",
      "Batch number : 255, Training: Loss:  0.4853, Accuracy: 0.8281\n",
      "Batch number : 256, Training: Loss:  0.4594, Accuracy: 0.8906\n",
      "Batch number : 257, Training: Loss:  0.6514, Accuracy: 0.8281\n",
      "Batch number : 258, Training: Loss:  0.4938, Accuracy: 0.8281\n",
      "Batch number : 259, Training: Loss:  0.5742, Accuracy: 0.8438\n",
      "Batch number : 260, Training: Loss:  0.3969, Accuracy: 0.8750\n",
      "Batch number : 261, Training: Loss:  0.3268, Accuracy: 0.9062\n",
      "Batch number : 262, Training: Loss:  0.3694, Accuracy: 0.9062\n",
      "Batch number : 263, Training: Loss:  0.3994, Accuracy: 0.8906\n",
      "Batch number : 264, Training: Loss:  0.5360, Accuracy: 0.8438\n",
      "Batch number : 265, Training: Loss:  0.3305, Accuracy: 0.9062\n",
      "Batch number : 266, Training: Loss:  0.4583, Accuracy: 0.8750\n",
      "Batch number : 267, Training: Loss:  0.4240, Accuracy: 0.8594\n",
      "Batch number : 268, Training: Loss:  0.5512, Accuracy: 0.8594\n",
      "Batch number : 269, Training: Loss:  0.8255, Accuracy: 0.7656\n",
      "Batch number : 270, Training: Loss:  0.3394, Accuracy: 0.9062\n",
      "Batch number : 271, Training: Loss:  0.3905, Accuracy: 0.8906\n",
      "Batch number : 272, Training: Loss:  0.6810, Accuracy: 0.7500\n",
      "Batch number : 273, Training: Loss:  0.4048, Accuracy: 0.8438\n",
      "Batch number : 274, Training: Loss:  0.5045, Accuracy: 0.8438\n",
      "Batch number : 275, Training: Loss:  0.5076, Accuracy: 0.8438\n",
      "Batch number : 276, Training: Loss:  0.6808, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.6365, Accuracy: 0.7812\n",
      "Batch number : 278, Training: Loss:  0.8265, Accuracy: 0.7812\n",
      "Batch number : 279, Training: Loss:  0.7222, Accuracy: 0.7969\n",
      "Batch number : 280, Training: Loss:  0.3596, Accuracy: 0.9062\n",
      "Batch number : 281, Training: Loss:  0.3830, Accuracy: 0.8906\n",
      "Batch number : 282, Training: Loss:  0.2843, Accuracy: 0.9375\n",
      "Batch number : 283, Training: Loss:  0.5652, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.3476, Accuracy: 0.9062\n",
      "Batch number : 285, Training: Loss:  0.4293, Accuracy: 0.8438\n",
      "Batch number : 286, Training: Loss:  0.6247, Accuracy: 0.8281\n",
      "Batch number : 287, Training: Loss:  0.3721, Accuracy: 0.8906\n",
      "Batch number : 288, Training: Loss:  0.4731, Accuracy: 0.8906\n",
      "Batch number : 289, Training: Loss:  0.7597, Accuracy: 0.7969\n",
      "Batch number : 290, Training: Loss:  0.3547, Accuracy: 0.8906\n",
      "Batch number : 291, Training: Loss:  0.3946, Accuracy: 0.8906\n",
      "Batch number : 292, Training: Loss:  0.5789, Accuracy: 0.8594\n",
      "Batch number : 293, Training: Loss:  0.6283, Accuracy: 0.7812\n",
      "Batch number : 294, Training: Loss:  0.6559, Accuracy: 0.8125\n",
      "Batch number : 295, Training: Loss:  0.5546, Accuracy: 0.8125\n",
      "Batch number : 296, Training: Loss:  0.4276, Accuracy: 0.9062\n",
      "Batch number : 297, Training: Loss:  0.5810, Accuracy: 0.8438\n",
      "Batch number : 298, Training: Loss:  0.6222, Accuracy: 0.7969\n",
      "Batch number : 299, Training: Loss:  0.4432, Accuracy: 0.8750\n",
      "Batch number : 300, Training: Loss:  0.4106, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.5834, Accuracy: 0.8125\n",
      "Batch number : 302, Training: Loss:  0.5172, Accuracy: 0.8125\n",
      "Batch number : 303, Training: Loss:  0.4046, Accuracy: 0.9062\n",
      "Batch number : 304, Training: Loss:  0.3500, Accuracy: 0.8906\n",
      "Batch number : 305, Training: Loss:  0.4230, Accuracy: 0.8594\n",
      "Batch number : 306, Training: Loss:  0.4519, Accuracy: 0.8594\n",
      "Batch number : 307, Training: Loss:  0.4226, Accuracy: 0.8438\n",
      "Batch number : 308, Training: Loss:  0.4808, Accuracy: 0.8594\n",
      "Batch number : 309, Training: Loss:  0.4259, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.5571, Accuracy: 0.8750\n",
      "Batch number : 311, Training: Loss:  0.4830, Accuracy: 0.8750\n",
      "Batch number : 312, Training: Loss:  0.3280, Accuracy: 0.8906\n",
      "Batch number : 313, Training: Loss:  0.3771, Accuracy: 0.9062\n",
      "Batch number : 314, Training: Loss:  0.4226, Accuracy: 0.8750\n",
      "Batch number : 315, Training: Loss:  0.5876, Accuracy: 0.8594\n",
      "Batch number : 316, Training: Loss:  0.4406, Accuracy: 0.8906\n",
      "Batch number : 317, Training: Loss:  0.5007, Accuracy: 0.8594\n",
      "Batch number : 318, Training: Loss:  0.5791, Accuracy: 0.7969\n",
      "Batch number : 319, Training: Loss:  0.4327, Accuracy: 0.8750\n",
      "Batch number : 320, Training: Loss:  0.3884, Accuracy: 0.8750\n",
      "Batch number : 321, Training: Loss:  0.2622, Accuracy: 0.9531\n",
      "Batch number : 322, Training: Loss:  0.5579, Accuracy: 0.7969\n",
      "Batch number : 323, Training: Loss:  0.5582, Accuracy: 0.8594\n",
      "Batch number : 324, Training: Loss:  0.3343, Accuracy: 0.9062\n",
      "Batch number : 325, Training: Loss:  0.2736, Accuracy: 0.9531\n",
      "Batch number : 326, Training: Loss:  0.3440, Accuracy: 0.9219\n",
      "Batch number : 327, Training: Loss:  0.5535, Accuracy: 0.8594\n",
      "Batch number : 328, Training: Loss:  0.5732, Accuracy: 0.8281\n",
      "Batch number : 329, Training: Loss:  0.4486, Accuracy: 0.8906\n",
      "Batch number : 330, Training: Loss:  0.4571, Accuracy: 0.8594\n",
      "Batch number : 331, Training: Loss:  0.5080, Accuracy: 0.8438\n",
      "Batch number : 332, Training: Loss:  0.5378, Accuracy: 0.8594\n",
      "Batch number : 333, Training: Loss:  0.6731, Accuracy: 0.7500\n",
      "Batch number : 334, Training: Loss:  0.3955, Accuracy: 0.8750\n",
      "Batch number : 335, Training: Loss:  0.4070, Accuracy: 0.8750\n",
      "Batch number : 336, Training: Loss:  0.3811, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.5025, Accuracy: 0.8750\n",
      "Batch number : 338, Training: Loss:  0.4830, Accuracy: 0.8594\n",
      "Batch number : 339, Training: Loss:  0.5964, Accuracy: 0.8594\n",
      "Batch number : 340, Training: Loss:  0.6771, Accuracy: 0.8125\n",
      "Batch number : 341, Training: Loss:  0.5969, Accuracy: 0.8125\n",
      "Batch number : 342, Training: Loss:  0.4153, Accuracy: 0.8750\n",
      "Batch number : 343, Training: Loss:  0.3646, Accuracy: 0.9219\n",
      "Batch number : 344, Training: Loss:  0.3680, Accuracy: 0.8594\n",
      "Batch number : 345, Training: Loss:  0.6248, Accuracy: 0.8281\n",
      "Batch number : 346, Training: Loss:  0.6562, Accuracy: 0.8438\n",
      "Batch number : 347, Training: Loss:  0.4537, Accuracy: 0.8594\n",
      "Batch number : 348, Training: Loss:  0.3811, Accuracy: 0.9062\n",
      "Batch number : 349, Training: Loss:  0.4832, Accuracy: 0.8438\n",
      "Batch number : 350, Training: Loss:  0.2588, Accuracy: 0.9531\n",
      "Batch number : 351, Training: Loss:  0.4652, Accuracy: 0.8594\n",
      "Batch number : 352, Training: Loss:  0.3273, Accuracy: 0.8906\n",
      "Batch number : 353, Training: Loss:  0.5561, Accuracy: 0.8125\n",
      "Batch number : 354, Training: Loss:  0.3306, Accuracy: 0.9219\n",
      "Batch number : 355, Training: Loss:  0.8339, Accuracy: 0.7188\n",
      "Batch number : 356, Training: Loss:  0.3753, Accuracy: 0.9219\n",
      "Batch number : 357, Training: Loss:  0.4832, Accuracy: 0.8438\n",
      "Batch number : 358, Training: Loss:  0.6563, Accuracy: 0.8125\n",
      "Batch number : 359, Training: Loss:  0.4951, Accuracy: 0.8438\n",
      "Batch number : 360, Training: Loss:  0.5996, Accuracy: 0.8281\n",
      "Batch number : 361, Training: Loss:  0.4313, Accuracy: 0.8906\n",
      "Batch number : 362, Training: Loss:  0.4818, Accuracy: 0.8906\n",
      "Batch number : 363, Training: Loss:  0.5049, Accuracy: 0.8438\n",
      "Batch number : 364, Training: Loss:  0.4635, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.4547, Accuracy: 0.8438\n",
      "Batch number : 366, Training: Loss:  0.4478, Accuracy: 0.8750\n",
      "Batch number : 367, Training: Loss:  0.5122, Accuracy: 0.8281\n",
      "Batch number : 368, Training: Loss:  0.5414, Accuracy: 0.8281\n",
      "Batch number : 369, Training: Loss:  0.6206, Accuracy: 0.8438\n",
      "Batch number : 370, Training: Loss:  0.4968, Accuracy: 0.8281\n",
      "Batch number : 371, Training: Loss:  0.6834, Accuracy: 0.8281\n",
      "Batch number : 372, Training: Loss:  0.5206, Accuracy: 0.8438\n",
      "Batch number : 373, Training: Loss:  0.4172, Accuracy: 0.9062\n",
      "Batch number : 374, Training: Loss:  0.5723, Accuracy: 0.7969\n",
      "Batch number : 375, Training: Loss:  0.5856, Accuracy: 0.8125\n",
      "Batch number : 376, Training: Loss:  0.3677, Accuracy: 0.8906\n",
      "Batch number : 377, Training: Loss:  0.3954, Accuracy: 0.9000\n",
      "Epoch: 10/20\n",
      "Batch number : 000, Training: Loss:  0.4252, Accuracy: 0.8750\n",
      "Batch number : 001, Training: Loss:  0.6897, Accuracy: 0.8594\n",
      "Batch number : 002, Training: Loss:  0.6175, Accuracy: 0.7969\n",
      "Batch number : 003, Training: Loss:  0.3977, Accuracy: 0.8750\n",
      "Batch number : 004, Training: Loss:  0.4533, Accuracy: 0.8438\n",
      "Batch number : 005, Training: Loss:  0.4365, Accuracy: 0.8594\n",
      "Batch number : 006, Training: Loss:  0.5034, Accuracy: 0.8438\n",
      "Batch number : 007, Training: Loss:  0.3185, Accuracy: 0.8906\n",
      "Batch number : 008, Training: Loss:  0.3749, Accuracy: 0.9062\n",
      "Batch number : 009, Training: Loss:  0.4773, Accuracy: 0.8594\n",
      "Batch number : 010, Training: Loss:  0.6252, Accuracy: 0.8438\n",
      "Batch number : 011, Training: Loss:  0.3813, Accuracy: 0.8906\n",
      "Batch number : 012, Training: Loss:  0.5451, Accuracy: 0.8438\n",
      "Batch number : 013, Training: Loss:  0.6088, Accuracy: 0.8125\n",
      "Batch number : 014, Training: Loss:  0.6744, Accuracy: 0.7500\n",
      "Batch number : 015, Training: Loss:  0.3504, Accuracy: 0.8906\n",
      "Batch number : 016, Training: Loss:  0.3695, Accuracy: 0.8906\n",
      "Batch number : 017, Training: Loss:  0.3777, Accuracy: 0.9062\n",
      "Batch number : 018, Training: Loss:  0.5808, Accuracy: 0.8594\n",
      "Batch number : 019, Training: Loss:  0.3409, Accuracy: 0.8594\n",
      "Batch number : 020, Training: Loss:  0.5559, Accuracy: 0.8438\n",
      "Batch number : 021, Training: Loss:  0.4275, Accuracy: 0.8281\n",
      "Batch number : 022, Training: Loss:  0.3291, Accuracy: 0.9219\n",
      "Batch number : 023, Training: Loss:  0.2949, Accuracy: 0.9219\n",
      "Batch number : 024, Training: Loss:  0.4109, Accuracy: 0.8906\n",
      "Batch number : 025, Training: Loss:  0.3879, Accuracy: 0.8750\n",
      "Batch number : 026, Training: Loss:  0.5016, Accuracy: 0.8594\n",
      "Batch number : 027, Training: Loss:  0.7161, Accuracy: 0.7812\n",
      "Batch number : 028, Training: Loss:  0.4642, Accuracy: 0.8906\n",
      "Batch number : 029, Training: Loss:  0.5242, Accuracy: 0.8750\n",
      "Batch number : 030, Training: Loss:  0.4762, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.5041, Accuracy: 0.8125\n",
      "Batch number : 032, Training: Loss:  0.4048, Accuracy: 0.8594\n",
      "Batch number : 033, Training: Loss:  0.3343, Accuracy: 0.8906\n",
      "Batch number : 034, Training: Loss:  0.5583, Accuracy: 0.7812\n",
      "Batch number : 035, Training: Loss:  0.4671, Accuracy: 0.8906\n",
      "Batch number : 036, Training: Loss:  0.4023, Accuracy: 0.8594\n",
      "Batch number : 037, Training: Loss:  0.5476, Accuracy: 0.8594\n",
      "Batch number : 038, Training: Loss:  0.4888, Accuracy: 0.8594\n",
      "Batch number : 039, Training: Loss:  0.4949, Accuracy: 0.8438\n",
      "Batch number : 040, Training: Loss:  0.5094, Accuracy: 0.8594\n",
      "Batch number : 041, Training: Loss:  0.6646, Accuracy: 0.8281\n",
      "Batch number : 042, Training: Loss:  0.3376, Accuracy: 0.9219\n",
      "Batch number : 043, Training: Loss:  0.4287, Accuracy: 0.8594\n",
      "Batch number : 044, Training: Loss:  0.6413, Accuracy: 0.8125\n",
      "Batch number : 045, Training: Loss:  0.5859, Accuracy: 0.8281\n",
      "Batch number : 046, Training: Loss:  0.6322, Accuracy: 0.8281\n",
      "Batch number : 047, Training: Loss:  0.5256, Accuracy: 0.8281\n",
      "Batch number : 048, Training: Loss:  0.5149, Accuracy: 0.8281\n",
      "Batch number : 049, Training: Loss:  0.6251, Accuracy: 0.8281\n",
      "Batch number : 050, Training: Loss:  0.4552, Accuracy: 0.8906\n",
      "Batch number : 051, Training: Loss:  0.4337, Accuracy: 0.8750\n",
      "Batch number : 052, Training: Loss:  0.4686, Accuracy: 0.8750\n",
      "Batch number : 053, Training: Loss:  0.5715, Accuracy: 0.8125\n",
      "Batch number : 054, Training: Loss:  0.3202, Accuracy: 0.9375\n",
      "Batch number : 055, Training: Loss:  0.3733, Accuracy: 0.9062\n",
      "Batch number : 056, Training: Loss:  0.6671, Accuracy: 0.8125\n",
      "Batch number : 057, Training: Loss:  0.5320, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.2497, Accuracy: 0.9219\n",
      "Batch number : 059, Training: Loss:  0.6198, Accuracy: 0.7812\n",
      "Batch number : 060, Training: Loss:  0.6705, Accuracy: 0.8281\n",
      "Batch number : 061, Training: Loss:  0.3872, Accuracy: 0.8750\n",
      "Batch number : 062, Training: Loss:  0.2497, Accuracy: 0.9219\n",
      "Batch number : 063, Training: Loss:  0.4156, Accuracy: 0.8594\n",
      "Batch number : 064, Training: Loss:  0.2271, Accuracy: 0.9375\n",
      "Batch number : 065, Training: Loss:  0.4645, Accuracy: 0.8594\n",
      "Batch number : 066, Training: Loss:  0.6593, Accuracy: 0.7969\n",
      "Batch number : 067, Training: Loss:  0.4456, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.4941, Accuracy: 0.8594\n",
      "Batch number : 069, Training: Loss:  0.3750, Accuracy: 0.8906\n",
      "Batch number : 070, Training: Loss:  0.5671, Accuracy: 0.8438\n",
      "Batch number : 071, Training: Loss:  0.3737, Accuracy: 0.9219\n",
      "Batch number : 072, Training: Loss:  0.6832, Accuracy: 0.7969\n",
      "Batch number : 073, Training: Loss:  0.4849, Accuracy: 0.8594\n",
      "Batch number : 074, Training: Loss:  0.3884, Accuracy: 0.8750\n",
      "Batch number : 075, Training: Loss:  0.4520, Accuracy: 0.8594\n",
      "Batch number : 076, Training: Loss:  0.5469, Accuracy: 0.8281\n",
      "Batch number : 077, Training: Loss:  0.5452, Accuracy: 0.8438\n",
      "Batch number : 078, Training: Loss:  0.4585, Accuracy: 0.8750\n",
      "Batch number : 079, Training: Loss:  0.4523, Accuracy: 0.8594\n",
      "Batch number : 080, Training: Loss:  0.5334, Accuracy: 0.8594\n",
      "Batch number : 081, Training: Loss:  0.7705, Accuracy: 0.7812\n",
      "Batch number : 082, Training: Loss:  0.6009, Accuracy: 0.8281\n",
      "Batch number : 083, Training: Loss:  0.4230, Accuracy: 0.8438\n",
      "Batch number : 084, Training: Loss:  0.3520, Accuracy: 0.8906\n",
      "Batch number : 085, Training: Loss:  0.4250, Accuracy: 0.8750\n",
      "Batch number : 086, Training: Loss:  0.6202, Accuracy: 0.8125\n",
      "Batch number : 087, Training: Loss:  0.4205, Accuracy: 0.8906\n",
      "Batch number : 088, Training: Loss:  0.6149, Accuracy: 0.8281\n",
      "Batch number : 089, Training: Loss:  0.4881, Accuracy: 0.8438\n",
      "Batch number : 090, Training: Loss:  0.5071, Accuracy: 0.8281\n",
      "Batch number : 091, Training: Loss:  0.3213, Accuracy: 0.9062\n",
      "Batch number : 092, Training: Loss:  0.4068, Accuracy: 0.8750\n",
      "Batch number : 093, Training: Loss:  0.5253, Accuracy: 0.8281\n",
      "Batch number : 094, Training: Loss:  0.3934, Accuracy: 0.8594\n",
      "Batch number : 095, Training: Loss:  0.4719, Accuracy: 0.8438\n",
      "Batch number : 096, Training: Loss:  0.5338, Accuracy: 0.8281\n",
      "Batch number : 097, Training: Loss:  0.5725, Accuracy: 0.8281\n",
      "Batch number : 098, Training: Loss:  0.6328, Accuracy: 0.8281\n",
      "Batch number : 099, Training: Loss:  0.5039, Accuracy: 0.8906\n",
      "Batch number : 100, Training: Loss:  0.3448, Accuracy: 0.9062\n",
      "Batch number : 101, Training: Loss:  0.6194, Accuracy: 0.8125\n",
      "Batch number : 102, Training: Loss:  0.4962, Accuracy: 0.8906\n",
      "Batch number : 103, Training: Loss:  0.5463, Accuracy: 0.8125\n",
      "Batch number : 104, Training: Loss:  0.4623, Accuracy: 0.8438\n",
      "Batch number : 105, Training: Loss:  0.2677, Accuracy: 0.9531\n",
      "Batch number : 106, Training: Loss:  0.5471, Accuracy: 0.8438\n",
      "Batch number : 107, Training: Loss:  0.4179, Accuracy: 0.8594\n",
      "Batch number : 108, Training: Loss:  0.4512, Accuracy: 0.8438\n",
      "Batch number : 109, Training: Loss:  0.4474, Accuracy: 0.8438\n",
      "Batch number : 110, Training: Loss:  0.3736, Accuracy: 0.9062\n",
      "Batch number : 111, Training: Loss:  0.4741, Accuracy: 0.8594\n",
      "Batch number : 112, Training: Loss:  0.6079, Accuracy: 0.8281\n",
      "Batch number : 113, Training: Loss:  0.3979, Accuracy: 0.8438\n",
      "Batch number : 114, Training: Loss:  0.4193, Accuracy: 0.8906\n",
      "Batch number : 115, Training: Loss:  0.6368, Accuracy: 0.8125\n",
      "Batch number : 116, Training: Loss:  0.4142, Accuracy: 0.8906\n",
      "Batch number : 117, Training: Loss:  0.4599, Accuracy: 0.8750\n",
      "Batch number : 118, Training: Loss:  0.3882, Accuracy: 0.8750\n",
      "Batch number : 119, Training: Loss:  0.3597, Accuracy: 0.8906\n",
      "Batch number : 120, Training: Loss:  0.5495, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.5517, Accuracy: 0.8281\n",
      "Batch number : 122, Training: Loss:  0.2388, Accuracy: 0.9531\n",
      "Batch number : 123, Training: Loss:  0.3849, Accuracy: 0.8906\n",
      "Batch number : 124, Training: Loss:  0.5031, Accuracy: 0.8438\n",
      "Batch number : 125, Training: Loss:  0.5856, Accuracy: 0.8281\n",
      "Batch number : 126, Training: Loss:  0.4799, Accuracy: 0.8594\n",
      "Batch number : 127, Training: Loss:  0.7431, Accuracy: 0.7656\n",
      "Batch number : 128, Training: Loss:  0.3818, Accuracy: 0.9375\n",
      "Batch number : 129, Training: Loss:  0.4814, Accuracy: 0.8281\n",
      "Batch number : 130, Training: Loss:  0.2910, Accuracy: 0.9375\n",
      "Batch number : 131, Training: Loss:  0.4855, Accuracy: 0.7969\n",
      "Batch number : 132, Training: Loss:  0.4901, Accuracy: 0.8281\n",
      "Batch number : 133, Training: Loss:  0.4074, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.6932, Accuracy: 0.8438\n",
      "Batch number : 135, Training: Loss:  0.5464, Accuracy: 0.8125\n",
      "Batch number : 136, Training: Loss:  0.3838, Accuracy: 0.9062\n",
      "Batch number : 137, Training: Loss:  0.6511, Accuracy: 0.8281\n",
      "Batch number : 138, Training: Loss:  0.5618, Accuracy: 0.8281\n",
      "Batch number : 139, Training: Loss:  0.4727, Accuracy: 0.8281\n",
      "Batch number : 140, Training: Loss:  0.5178, Accuracy: 0.8906\n",
      "Batch number : 141, Training: Loss:  0.5317, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.3995, Accuracy: 0.8906\n",
      "Batch number : 143, Training: Loss:  0.4454, Accuracy: 0.8906\n",
      "Batch number : 144, Training: Loss:  0.3007, Accuracy: 0.9062\n",
      "Batch number : 145, Training: Loss:  0.5445, Accuracy: 0.8438\n",
      "Batch number : 146, Training: Loss:  0.4481, Accuracy: 0.8750\n",
      "Batch number : 147, Training: Loss:  0.5289, Accuracy: 0.8594\n",
      "Batch number : 148, Training: Loss:  0.4586, Accuracy: 0.8281\n",
      "Batch number : 149, Training: Loss:  0.6078, Accuracy: 0.8594\n",
      "Batch number : 150, Training: Loss:  0.5344, Accuracy: 0.8125\n",
      "Batch number : 151, Training: Loss:  0.5202, Accuracy: 0.8750\n",
      "Batch number : 152, Training: Loss:  0.5715, Accuracy: 0.8125\n",
      "Batch number : 153, Training: Loss:  0.3490, Accuracy: 0.9375\n",
      "Batch number : 154, Training: Loss:  0.4895, Accuracy: 0.9062\n",
      "Batch number : 155, Training: Loss:  0.4257, Accuracy: 0.8906\n",
      "Batch number : 156, Training: Loss:  0.5302, Accuracy: 0.8281\n",
      "Batch number : 157, Training: Loss:  0.4553, Accuracy: 0.8906\n",
      "Batch number : 158, Training: Loss:  0.4040, Accuracy: 0.8906\n",
      "Batch number : 159, Training: Loss:  0.4636, Accuracy: 0.8594\n",
      "Batch number : 160, Training: Loss:  0.6661, Accuracy: 0.7969\n",
      "Batch number : 161, Training: Loss:  0.4082, Accuracy: 0.8750\n",
      "Batch number : 162, Training: Loss:  0.4699, Accuracy: 0.8125\n",
      "Batch number : 163, Training: Loss:  0.2420, Accuracy: 0.9219\n",
      "Batch number : 164, Training: Loss:  0.4985, Accuracy: 0.8594\n",
      "Batch number : 165, Training: Loss:  0.2612, Accuracy: 0.9375\n",
      "Batch number : 166, Training: Loss:  0.4601, Accuracy: 0.8594\n",
      "Batch number : 167, Training: Loss:  0.3373, Accuracy: 0.9062\n",
      "Batch number : 168, Training: Loss:  0.4699, Accuracy: 0.8438\n",
      "Batch number : 169, Training: Loss:  0.7619, Accuracy: 0.7344\n",
      "Batch number : 170, Training: Loss:  0.5254, Accuracy: 0.8281\n",
      "Batch number : 171, Training: Loss:  0.5547, Accuracy: 0.8125\n",
      "Batch number : 172, Training: Loss:  0.4522, Accuracy: 0.8438\n",
      "Batch number : 173, Training: Loss:  0.4540, Accuracy: 0.8281\n",
      "Batch number : 174, Training: Loss:  0.4837, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.5941, Accuracy: 0.7812\n",
      "Batch number : 176, Training: Loss:  0.6721, Accuracy: 0.7656\n",
      "Batch number : 177, Training: Loss:  0.6691, Accuracy: 0.7656\n",
      "Batch number : 178, Training: Loss:  0.4375, Accuracy: 0.8906\n",
      "Batch number : 179, Training: Loss:  0.3920, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.6059, Accuracy: 0.8281\n",
      "Batch number : 181, Training: Loss:  0.7052, Accuracy: 0.7656\n",
      "Batch number : 182, Training: Loss:  0.7231, Accuracy: 0.7812\n",
      "Batch number : 183, Training: Loss:  0.6946, Accuracy: 0.8125\n",
      "Batch number : 184, Training: Loss:  0.4610, Accuracy: 0.8594\n",
      "Batch number : 185, Training: Loss:  0.4864, Accuracy: 0.8750\n",
      "Batch number : 186, Training: Loss:  0.4527, Accuracy: 0.8594\n",
      "Batch number : 187, Training: Loss:  0.4171, Accuracy: 0.8906\n",
      "Batch number : 188, Training: Loss:  0.2745, Accuracy: 0.9375\n",
      "Batch number : 189, Training: Loss:  0.4237, Accuracy: 0.8750\n",
      "Batch number : 190, Training: Loss:  0.5112, Accuracy: 0.8281\n",
      "Batch number : 191, Training: Loss:  0.4870, Accuracy: 0.9062\n",
      "Batch number : 192, Training: Loss:  0.5032, Accuracy: 0.8750\n",
      "Batch number : 193, Training: Loss:  0.7490, Accuracy: 0.8281\n",
      "Batch number : 194, Training: Loss:  0.4542, Accuracy: 0.8594\n",
      "Batch number : 195, Training: Loss:  0.5757, Accuracy: 0.8281\n",
      "Batch number : 196, Training: Loss:  0.4277, Accuracy: 0.8438\n",
      "Batch number : 197, Training: Loss:  0.5628, Accuracy: 0.8438\n",
      "Batch number : 198, Training: Loss:  0.4790, Accuracy: 0.8750\n",
      "Batch number : 199, Training: Loss:  0.4999, Accuracy: 0.8594\n",
      "Batch number : 200, Training: Loss:  0.3341, Accuracy: 0.8906\n",
      "Batch number : 201, Training: Loss:  0.3977, Accuracy: 0.8750\n",
      "Batch number : 202, Training: Loss:  0.7255, Accuracy: 0.7812\n",
      "Batch number : 203, Training: Loss:  0.5472, Accuracy: 0.8594\n",
      "Batch number : 204, Training: Loss:  0.6484, Accuracy: 0.8281\n",
      "Batch number : 205, Training: Loss:  0.4498, Accuracy: 0.8906\n",
      "Batch number : 206, Training: Loss:  0.7005, Accuracy: 0.8281\n",
      "Batch number : 207, Training: Loss:  0.5111, Accuracy: 0.8438\n",
      "Batch number : 208, Training: Loss:  0.3715, Accuracy: 0.9062\n",
      "Batch number : 209, Training: Loss:  0.5237, Accuracy: 0.8438\n",
      "Batch number : 210, Training: Loss:  0.4634, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.3925, Accuracy: 0.8906\n",
      "Batch number : 212, Training: Loss:  0.5705, Accuracy: 0.8281\n",
      "Batch number : 213, Training: Loss:  0.5001, Accuracy: 0.8594\n",
      "Batch number : 214, Training: Loss:  0.4968, Accuracy: 0.8438\n",
      "Batch number : 215, Training: Loss:  0.3778, Accuracy: 0.9062\n",
      "Batch number : 216, Training: Loss:  0.3675, Accuracy: 0.9062\n",
      "Batch number : 217, Training: Loss:  0.4436, Accuracy: 0.8750\n",
      "Batch number : 218, Training: Loss:  0.5964, Accuracy: 0.7969\n",
      "Batch number : 219, Training: Loss:  0.4311, Accuracy: 0.8750\n",
      "Batch number : 220, Training: Loss:  0.6669, Accuracy: 0.7969\n",
      "Batch number : 221, Training: Loss:  0.2602, Accuracy: 0.9531\n",
      "Batch number : 222, Training: Loss:  0.5274, Accuracy: 0.8594\n",
      "Batch number : 223, Training: Loss:  0.4017, Accuracy: 0.8438\n",
      "Batch number : 224, Training: Loss:  0.4149, Accuracy: 0.8906\n",
      "Batch number : 225, Training: Loss:  0.5735, Accuracy: 0.8125\n",
      "Batch number : 226, Training: Loss:  0.4096, Accuracy: 0.8750\n",
      "Batch number : 227, Training: Loss:  0.3495, Accuracy: 0.9062\n",
      "Batch number : 228, Training: Loss:  0.3755, Accuracy: 0.9062\n",
      "Batch number : 229, Training: Loss:  0.3946, Accuracy: 0.8906\n",
      "Batch number : 230, Training: Loss:  0.7008, Accuracy: 0.8438\n",
      "Batch number : 231, Training: Loss:  0.5418, Accuracy: 0.8125\n",
      "Batch number : 232, Training: Loss:  0.4102, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.5291, Accuracy: 0.8125\n",
      "Batch number : 234, Training: Loss:  0.3561, Accuracy: 0.9219\n",
      "Batch number : 235, Training: Loss:  0.4531, Accuracy: 0.8438\n",
      "Batch number : 236, Training: Loss:  0.7201, Accuracy: 0.8125\n",
      "Batch number : 237, Training: Loss:  0.3966, Accuracy: 0.8750\n",
      "Batch number : 238, Training: Loss:  0.5746, Accuracy: 0.8281\n",
      "Batch number : 239, Training: Loss:  0.4430, Accuracy: 0.8594\n",
      "Batch number : 240, Training: Loss:  0.4298, Accuracy: 0.8594\n",
      "Batch number : 241, Training: Loss:  0.4109, Accuracy: 0.8594\n",
      "Batch number : 242, Training: Loss:  0.5367, Accuracy: 0.8125\n",
      "Batch number : 243, Training: Loss:  0.5983, Accuracy: 0.8438\n",
      "Batch number : 244, Training: Loss:  0.4543, Accuracy: 0.8594\n",
      "Batch number : 245, Training: Loss:  0.4311, Accuracy: 0.9062\n",
      "Batch number : 246, Training: Loss:  0.3532, Accuracy: 0.9375\n",
      "Batch number : 247, Training: Loss:  0.6483, Accuracy: 0.7969\n",
      "Batch number : 248, Training: Loss:  0.6375, Accuracy: 0.8281\n",
      "Batch number : 249, Training: Loss:  0.5361, Accuracy: 0.8281\n",
      "Batch number : 250, Training: Loss:  0.2536, Accuracy: 0.9531\n",
      "Batch number : 251, Training: Loss:  0.5508, Accuracy: 0.8125\n",
      "Batch number : 252, Training: Loss:  0.4619, Accuracy: 0.8594\n",
      "Batch number : 253, Training: Loss:  0.3202, Accuracy: 0.9375\n",
      "Batch number : 254, Training: Loss:  0.2818, Accuracy: 0.9375\n",
      "Batch number : 255, Training: Loss:  0.5456, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.3574, Accuracy: 0.8906\n",
      "Batch number : 257, Training: Loss:  0.4533, Accuracy: 0.8438\n",
      "Batch number : 258, Training: Loss:  0.3843, Accuracy: 0.8750\n",
      "Batch number : 259, Training: Loss:  0.3633, Accuracy: 0.9219\n",
      "Batch number : 260, Training: Loss:  0.6871, Accuracy: 0.7812\n",
      "Batch number : 261, Training: Loss:  0.4934, Accuracy: 0.8594\n",
      "Batch number : 262, Training: Loss:  0.5152, Accuracy: 0.8281\n",
      "Batch number : 263, Training: Loss:  0.3799, Accuracy: 0.8906\n",
      "Batch number : 264, Training: Loss:  0.2428, Accuracy: 0.9531\n",
      "Batch number : 265, Training: Loss:  0.5392, Accuracy: 0.8125\n",
      "Batch number : 266, Training: Loss:  0.4267, Accuracy: 0.9219\n",
      "Batch number : 267, Training: Loss:  0.3817, Accuracy: 0.8906\n",
      "Batch number : 268, Training: Loss:  0.5597, Accuracy: 0.8281\n",
      "Batch number : 269, Training: Loss:  0.3274, Accuracy: 0.9219\n",
      "Batch number : 270, Training: Loss:  0.6955, Accuracy: 0.7812\n",
      "Batch number : 271, Training: Loss:  0.7336, Accuracy: 0.8281\n",
      "Batch number : 272, Training: Loss:  0.5567, Accuracy: 0.8594\n",
      "Batch number : 273, Training: Loss:  0.7762, Accuracy: 0.8125\n",
      "Batch number : 274, Training: Loss:  0.4949, Accuracy: 0.8125\n",
      "Batch number : 275, Training: Loss:  0.4407, Accuracy: 0.8750\n",
      "Batch number : 276, Training: Loss:  0.5915, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.5354, Accuracy: 0.8438\n",
      "Batch number : 278, Training: Loss:  0.6490, Accuracy: 0.7969\n",
      "Batch number : 279, Training: Loss:  0.4664, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.2887, Accuracy: 0.9219\n",
      "Batch number : 281, Training: Loss:  0.4679, Accuracy: 0.8750\n",
      "Batch number : 282, Training: Loss:  0.4792, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.4568, Accuracy: 0.8906\n",
      "Batch number : 284, Training: Loss:  0.3752, Accuracy: 0.8750\n",
      "Batch number : 285, Training: Loss:  0.3444, Accuracy: 0.9219\n",
      "Batch number : 286, Training: Loss:  0.5748, Accuracy: 0.8594\n",
      "Batch number : 287, Training: Loss:  0.3732, Accuracy: 0.8906\n",
      "Batch number : 288, Training: Loss:  0.4526, Accuracy: 0.8594\n",
      "Batch number : 289, Training: Loss:  0.5969, Accuracy: 0.8281\n",
      "Batch number : 290, Training: Loss:  0.3022, Accuracy: 0.9062\n",
      "Batch number : 291, Training: Loss:  0.2973, Accuracy: 0.9219\n",
      "Batch number : 292, Training: Loss:  0.6171, Accuracy: 0.7969\n",
      "Batch number : 293, Training: Loss:  0.4000, Accuracy: 0.9062\n",
      "Batch number : 294, Training: Loss:  0.5313, Accuracy: 0.8281\n",
      "Batch number : 295, Training: Loss:  0.6356, Accuracy: 0.8125\n",
      "Batch number : 296, Training: Loss:  0.6919, Accuracy: 0.7656\n",
      "Batch number : 297, Training: Loss:  0.4649, Accuracy: 0.9062\n",
      "Batch number : 298, Training: Loss:  0.3490, Accuracy: 0.9062\n",
      "Batch number : 299, Training: Loss:  0.4247, Accuracy: 0.8906\n",
      "Batch number : 300, Training: Loss:  0.4191, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.4563, Accuracy: 0.8594\n",
      "Batch number : 302, Training: Loss:  0.5453, Accuracy: 0.8125\n",
      "Batch number : 303, Training: Loss:  0.5363, Accuracy: 0.8438\n",
      "Batch number : 304, Training: Loss:  0.4074, Accuracy: 0.8906\n",
      "Batch number : 305, Training: Loss:  0.5127, Accuracy: 0.8750\n",
      "Batch number : 306, Training: Loss:  0.5448, Accuracy: 0.8281\n",
      "Batch number : 307, Training: Loss:  0.4951, Accuracy: 0.8438\n",
      "Batch number : 308, Training: Loss:  0.5667, Accuracy: 0.8281\n",
      "Batch number : 309, Training: Loss:  0.5288, Accuracy: 0.8438\n",
      "Batch number : 310, Training: Loss:  0.2542, Accuracy: 0.9375\n",
      "Batch number : 311, Training: Loss:  0.5921, Accuracy: 0.7656\n",
      "Batch number : 312, Training: Loss:  0.4352, Accuracy: 0.8750\n",
      "Batch number : 313, Training: Loss:  0.4166, Accuracy: 0.8750\n",
      "Batch number : 314, Training: Loss:  0.5772, Accuracy: 0.8125\n",
      "Batch number : 315, Training: Loss:  0.5775, Accuracy: 0.7969\n",
      "Batch number : 316, Training: Loss:  0.5234, Accuracy: 0.7969\n",
      "Batch number : 317, Training: Loss:  0.5830, Accuracy: 0.8438\n",
      "Batch number : 318, Training: Loss:  0.5976, Accuracy: 0.7969\n",
      "Batch number : 319, Training: Loss:  0.3966, Accuracy: 0.9062\n",
      "Batch number : 320, Training: Loss:  0.3120, Accuracy: 0.9219\n",
      "Batch number : 321, Training: Loss:  0.3912, Accuracy: 0.9062\n",
      "Batch number : 322, Training: Loss:  0.3143, Accuracy: 0.9375\n",
      "Batch number : 323, Training: Loss:  0.5351, Accuracy: 0.8750\n",
      "Batch number : 324, Training: Loss:  0.5345, Accuracy: 0.8594\n",
      "Batch number : 325, Training: Loss:  0.2251, Accuracy: 0.9531\n",
      "Batch number : 326, Training: Loss:  0.5496, Accuracy: 0.7969\n",
      "Batch number : 327, Training: Loss:  0.7232, Accuracy: 0.7969\n",
      "Batch number : 328, Training: Loss:  0.4256, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.4426, Accuracy: 0.8125\n",
      "Batch number : 330, Training: Loss:  0.4856, Accuracy: 0.8281\n",
      "Batch number : 331, Training: Loss:  0.4189, Accuracy: 0.8438\n",
      "Batch number : 332, Training: Loss:  0.4738, Accuracy: 0.8281\n",
      "Batch number : 333, Training: Loss:  0.3420, Accuracy: 0.9062\n",
      "Batch number : 334, Training: Loss:  0.5457, Accuracy: 0.8281\n",
      "Batch number : 335, Training: Loss:  0.4650, Accuracy: 0.9062\n",
      "Batch number : 336, Training: Loss:  0.3517, Accuracy: 0.9375\n",
      "Batch number : 337, Training: Loss:  0.3558, Accuracy: 0.8906\n",
      "Batch number : 338, Training: Loss:  0.4523, Accuracy: 0.8594\n",
      "Batch number : 339, Training: Loss:  0.4245, Accuracy: 0.8750\n",
      "Batch number : 340, Training: Loss:  0.4724, Accuracy: 0.8438\n",
      "Batch number : 341, Training: Loss:  0.6696, Accuracy: 0.8281\n",
      "Batch number : 342, Training: Loss:  0.3142, Accuracy: 0.9219\n",
      "Batch number : 343, Training: Loss:  0.2923, Accuracy: 0.9062\n",
      "Batch number : 344, Training: Loss:  0.4361, Accuracy: 0.8438\n",
      "Batch number : 345, Training: Loss:  0.7655, Accuracy: 0.7656\n",
      "Batch number : 346, Training: Loss:  0.4222, Accuracy: 0.8594\n",
      "Batch number : 347, Training: Loss:  0.4433, Accuracy: 0.8906\n",
      "Batch number : 348, Training: Loss:  0.5366, Accuracy: 0.8281\n",
      "Batch number : 349, Training: Loss:  0.5693, Accuracy: 0.8750\n",
      "Batch number : 350, Training: Loss:  0.4548, Accuracy: 0.8750\n",
      "Batch number : 351, Training: Loss:  0.4066, Accuracy: 0.8906\n",
      "Batch number : 352, Training: Loss:  0.4317, Accuracy: 0.8750\n",
      "Batch number : 353, Training: Loss:  0.4169, Accuracy: 0.8906\n",
      "Batch number : 354, Training: Loss:  0.5387, Accuracy: 0.8281\n",
      "Batch number : 355, Training: Loss:  0.5181, Accuracy: 0.8594\n",
      "Batch number : 356, Training: Loss:  0.4135, Accuracy: 0.9219\n",
      "Batch number : 357, Training: Loss:  0.4241, Accuracy: 0.9219\n",
      "Batch number : 358, Training: Loss:  0.5662, Accuracy: 0.8594\n",
      "Batch number : 359, Training: Loss:  0.5484, Accuracy: 0.8281\n",
      "Batch number : 360, Training: Loss:  0.4190, Accuracy: 0.8750\n",
      "Batch number : 361, Training: Loss:  0.3689, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.6808, Accuracy: 0.8125\n",
      "Batch number : 363, Training: Loss:  0.4744, Accuracy: 0.8750\n",
      "Batch number : 364, Training: Loss:  0.4855, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.6241, Accuracy: 0.8281\n",
      "Batch number : 366, Training: Loss:  0.6644, Accuracy: 0.7656\n",
      "Batch number : 367, Training: Loss:  0.6658, Accuracy: 0.7812\n",
      "Batch number : 368, Training: Loss:  0.5452, Accuracy: 0.8438\n",
      "Batch number : 369, Training: Loss:  0.2998, Accuracy: 0.9688\n",
      "Batch number : 370, Training: Loss:  0.4337, Accuracy: 0.8750\n",
      "Batch number : 371, Training: Loss:  0.7092, Accuracy: 0.7344\n",
      "Batch number : 372, Training: Loss:  0.5337, Accuracy: 0.8281\n",
      "Batch number : 373, Training: Loss:  0.3676, Accuracy: 0.8906\n",
      "Batch number : 374, Training: Loss:  0.5566, Accuracy: 0.8594\n",
      "Batch number : 375, Training: Loss:  0.6405, Accuracy: 0.7969\n",
      "Batch number : 376, Training: Loss:  0.6274, Accuracy: 0.8281\n",
      "Batch number : 377, Training: Loss:  0.5532, Accuracy: 0.8250\n",
      "Epoch: 11/20\n",
      "Batch number : 000, Training: Loss:  0.3583, Accuracy: 0.8906\n",
      "Batch number : 001, Training: Loss:  0.5152, Accuracy: 0.8438\n",
      "Batch number : 002, Training: Loss:  0.5423, Accuracy: 0.8594\n",
      "Batch number : 003, Training: Loss:  0.4366, Accuracy: 0.9062\n",
      "Batch number : 004, Training: Loss:  0.5541, Accuracy: 0.8906\n",
      "Batch number : 005, Training: Loss:  0.5485, Accuracy: 0.8438\n",
      "Batch number : 006, Training: Loss:  0.4556, Accuracy: 0.8594\n",
      "Batch number : 007, Training: Loss:  0.4543, Accuracy: 0.9062\n",
      "Batch number : 008, Training: Loss:  0.5839, Accuracy: 0.8438\n",
      "Batch number : 009, Training: Loss:  0.5363, Accuracy: 0.7969\n",
      "Batch number : 010, Training: Loss:  0.3432, Accuracy: 0.9375\n",
      "Batch number : 011, Training: Loss:  0.5434, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.4355, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.4649, Accuracy: 0.8906\n",
      "Batch number : 014, Training: Loss:  0.4702, Accuracy: 0.8594\n",
      "Batch number : 015, Training: Loss:  0.4470, Accuracy: 0.8594\n",
      "Batch number : 016, Training: Loss:  0.5452, Accuracy: 0.8125\n",
      "Batch number : 017, Training: Loss:  0.4403, Accuracy: 0.8750\n",
      "Batch number : 018, Training: Loss:  0.5503, Accuracy: 0.8125\n",
      "Batch number : 019, Training: Loss:  0.4959, Accuracy: 0.8594\n",
      "Batch number : 020, Training: Loss:  0.6052, Accuracy: 0.8281\n",
      "Batch number : 021, Training: Loss:  0.4206, Accuracy: 0.8906\n",
      "Batch number : 022, Training: Loss:  0.5439, Accuracy: 0.8594\n",
      "Batch number : 023, Training: Loss:  0.3784, Accuracy: 0.8906\n",
      "Batch number : 024, Training: Loss:  0.3235, Accuracy: 0.9219\n",
      "Batch number : 025, Training: Loss:  0.5070, Accuracy: 0.8594\n",
      "Batch number : 026, Training: Loss:  0.5818, Accuracy: 0.8281\n",
      "Batch number : 027, Training: Loss:  0.4225, Accuracy: 0.9062\n",
      "Batch number : 028, Training: Loss:  0.4352, Accuracy: 0.8438\n",
      "Batch number : 029, Training: Loss:  0.2466, Accuracy: 0.9375\n",
      "Batch number : 030, Training: Loss:  0.5876, Accuracy: 0.8125\n",
      "Batch number : 031, Training: Loss:  0.4627, Accuracy: 0.8594\n",
      "Batch number : 032, Training: Loss:  0.4480, Accuracy: 0.8906\n",
      "Batch number : 033, Training: Loss:  0.2756, Accuracy: 0.9219\n",
      "Batch number : 034, Training: Loss:  0.4360, Accuracy: 0.8750\n",
      "Batch number : 035, Training: Loss:  0.5949, Accuracy: 0.8125\n",
      "Batch number : 036, Training: Loss:  0.4293, Accuracy: 0.8906\n",
      "Batch number : 037, Training: Loss:  0.3589, Accuracy: 0.8750\n",
      "Batch number : 038, Training: Loss:  0.2559, Accuracy: 0.9219\n",
      "Batch number : 039, Training: Loss:  0.7433, Accuracy: 0.7812\n",
      "Batch number : 040, Training: Loss:  0.3738, Accuracy: 0.8750\n",
      "Batch number : 041, Training: Loss:  0.3436, Accuracy: 0.8750\n",
      "Batch number : 042, Training: Loss:  0.3374, Accuracy: 0.9062\n",
      "Batch number : 043, Training: Loss:  0.4810, Accuracy: 0.8438\n",
      "Batch number : 044, Training: Loss:  0.2612, Accuracy: 0.9062\n",
      "Batch number : 045, Training: Loss:  0.4061, Accuracy: 0.8906\n",
      "Batch number : 046, Training: Loss:  0.5752, Accuracy: 0.8438\n",
      "Batch number : 047, Training: Loss:  0.4945, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.3272, Accuracy: 0.9375\n",
      "Batch number : 049, Training: Loss:  0.4391, Accuracy: 0.8750\n",
      "Batch number : 050, Training: Loss:  0.3328, Accuracy: 0.9219\n",
      "Batch number : 051, Training: Loss:  0.4883, Accuracy: 0.8594\n",
      "Batch number : 052, Training: Loss:  0.7541, Accuracy: 0.7656\n",
      "Batch number : 053, Training: Loss:  0.5487, Accuracy: 0.8594\n",
      "Batch number : 054, Training: Loss:  0.2881, Accuracy: 0.9375\n",
      "Batch number : 055, Training: Loss:  0.4336, Accuracy: 0.8438\n",
      "Batch number : 056, Training: Loss:  0.5827, Accuracy: 0.8438\n",
      "Batch number : 057, Training: Loss:  0.4905, Accuracy: 0.8906\n",
      "Batch number : 058, Training: Loss:  0.4768, Accuracy: 0.8281\n",
      "Batch number : 059, Training: Loss:  0.5081, Accuracy: 0.8750\n",
      "Batch number : 060, Training: Loss:  0.5382, Accuracy: 0.8281\n",
      "Batch number : 061, Training: Loss:  0.5594, Accuracy: 0.8281\n",
      "Batch number : 062, Training: Loss:  0.3905, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.5883, Accuracy: 0.8438\n",
      "Batch number : 064, Training: Loss:  0.4902, Accuracy: 0.8438\n",
      "Batch number : 065, Training: Loss:  0.4624, Accuracy: 0.8594\n",
      "Batch number : 066, Training: Loss:  0.4857, Accuracy: 0.8438\n",
      "Batch number : 067, Training: Loss:  0.3796, Accuracy: 0.8906\n",
      "Batch number : 068, Training: Loss:  0.6277, Accuracy: 0.7656\n",
      "Batch number : 069, Training: Loss:  0.6967, Accuracy: 0.8125\n",
      "Batch number : 070, Training: Loss:  0.4523, Accuracy: 0.8594\n",
      "Batch number : 071, Training: Loss:  0.4761, Accuracy: 0.8281\n",
      "Batch number : 072, Training: Loss:  0.4599, Accuracy: 0.8125\n",
      "Batch number : 073, Training: Loss:  0.3316, Accuracy: 0.9062\n",
      "Batch number : 074, Training: Loss:  0.5298, Accuracy: 0.7969\n",
      "Batch number : 075, Training: Loss:  0.3371, Accuracy: 0.9219\n",
      "Batch number : 076, Training: Loss:  0.2655, Accuracy: 0.9219\n",
      "Batch number : 077, Training: Loss:  0.4445, Accuracy: 0.8750\n",
      "Batch number : 078, Training: Loss:  0.5653, Accuracy: 0.8594\n",
      "Batch number : 079, Training: Loss:  0.5146, Accuracy: 0.8906\n",
      "Batch number : 080, Training: Loss:  0.4470, Accuracy: 0.8594\n",
      "Batch number : 081, Training: Loss:  0.4268, Accuracy: 0.8750\n",
      "Batch number : 082, Training: Loss:  0.3079, Accuracy: 0.8906\n",
      "Batch number : 083, Training: Loss:  0.4917, Accuracy: 0.8281\n",
      "Batch number : 084, Training: Loss:  0.5323, Accuracy: 0.8125\n",
      "Batch number : 085, Training: Loss:  0.1357, Accuracy: 0.9844\n",
      "Batch number : 086, Training: Loss:  0.9475, Accuracy: 0.7500\n",
      "Batch number : 087, Training: Loss:  0.6359, Accuracy: 0.8125\n",
      "Batch number : 088, Training: Loss:  0.4547, Accuracy: 0.8906\n",
      "Batch number : 089, Training: Loss:  0.4313, Accuracy: 0.9062\n",
      "Batch number : 090, Training: Loss:  0.6109, Accuracy: 0.7656\n",
      "Batch number : 091, Training: Loss:  0.4664, Accuracy: 0.8906\n",
      "Batch number : 092, Training: Loss:  0.4324, Accuracy: 0.8750\n",
      "Batch number : 093, Training: Loss:  0.5047, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.5205, Accuracy: 0.8125\n",
      "Batch number : 095, Training: Loss:  0.6119, Accuracy: 0.7969\n",
      "Batch number : 096, Training: Loss:  0.3303, Accuracy: 0.9062\n",
      "Batch number : 097, Training: Loss:  0.3796, Accuracy: 0.8906\n",
      "Batch number : 098, Training: Loss:  0.5472, Accuracy: 0.8125\n",
      "Batch number : 099, Training: Loss:  0.4413, Accuracy: 0.9219\n",
      "Batch number : 100, Training: Loss:  0.3176, Accuracy: 0.9062\n",
      "Batch number : 101, Training: Loss:  0.6263, Accuracy: 0.8438\n",
      "Batch number : 102, Training: Loss:  0.4779, Accuracy: 0.8594\n",
      "Batch number : 103, Training: Loss:  0.6427, Accuracy: 0.7812\n",
      "Batch number : 104, Training: Loss:  0.4541, Accuracy: 0.8594\n",
      "Batch number : 105, Training: Loss:  0.3642, Accuracy: 0.8750\n",
      "Batch number : 106, Training: Loss:  0.3888, Accuracy: 0.8750\n",
      "Batch number : 107, Training: Loss:  0.5129, Accuracy: 0.8750\n",
      "Batch number : 108, Training: Loss:  0.2909, Accuracy: 0.9062\n",
      "Batch number : 109, Training: Loss:  0.6674, Accuracy: 0.8125\n",
      "Batch number : 110, Training: Loss:  0.4754, Accuracy: 0.8438\n",
      "Batch number : 111, Training: Loss:  0.4147, Accuracy: 0.8750\n",
      "Batch number : 112, Training: Loss:  0.4890, Accuracy: 0.8438\n",
      "Batch number : 113, Training: Loss:  0.6403, Accuracy: 0.8438\n",
      "Batch number : 114, Training: Loss:  0.4889, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.5239, Accuracy: 0.8438\n",
      "Batch number : 116, Training: Loss:  0.6244, Accuracy: 0.8125\n",
      "Batch number : 117, Training: Loss:  0.5038, Accuracy: 0.7969\n",
      "Batch number : 118, Training: Loss:  0.4106, Accuracy: 0.9062\n",
      "Batch number : 119, Training: Loss:  0.6231, Accuracy: 0.7969\n",
      "Batch number : 120, Training: Loss:  0.4624, Accuracy: 0.8594\n",
      "Batch number : 121, Training: Loss:  0.5656, Accuracy: 0.8438\n",
      "Batch number : 122, Training: Loss:  0.3643, Accuracy: 0.8906\n",
      "Batch number : 123, Training: Loss:  0.5939, Accuracy: 0.8281\n",
      "Batch number : 124, Training: Loss:  0.6840, Accuracy: 0.8438\n",
      "Batch number : 125, Training: Loss:  0.3975, Accuracy: 0.8906\n",
      "Batch number : 126, Training: Loss:  0.5390, Accuracy: 0.8281\n",
      "Batch number : 127, Training: Loss:  0.3742, Accuracy: 0.8750\n",
      "Batch number : 128, Training: Loss:  0.5270, Accuracy: 0.8594\n",
      "Batch number : 129, Training: Loss:  0.4621, Accuracy: 0.8438\n",
      "Batch number : 130, Training: Loss:  0.4191, Accuracy: 0.8594\n",
      "Batch number : 131, Training: Loss:  0.3537, Accuracy: 0.8750\n",
      "Batch number : 132, Training: Loss:  0.2748, Accuracy: 0.9375\n",
      "Batch number : 133, Training: Loss:  0.6293, Accuracy: 0.8125\n",
      "Batch number : 134, Training: Loss:  0.5165, Accuracy: 0.8438\n",
      "Batch number : 135, Training: Loss:  0.8402, Accuracy: 0.7812\n",
      "Batch number : 136, Training: Loss:  0.6506, Accuracy: 0.7812\n",
      "Batch number : 137, Training: Loss:  0.4566, Accuracy: 0.8438\n",
      "Batch number : 138, Training: Loss:  0.4048, Accuracy: 0.9062\n",
      "Batch number : 139, Training: Loss:  0.6044, Accuracy: 0.7969\n",
      "Batch number : 140, Training: Loss:  0.5035, Accuracy: 0.8750\n",
      "Batch number : 141, Training: Loss:  0.4641, Accuracy: 0.8438\n",
      "Batch number : 142, Training: Loss:  0.3657, Accuracy: 0.8906\n",
      "Batch number : 143, Training: Loss:  0.4660, Accuracy: 0.8281\n",
      "Batch number : 144, Training: Loss:  0.3969, Accuracy: 0.8438\n",
      "Batch number : 145, Training: Loss:  0.4761, Accuracy: 0.8125\n",
      "Batch number : 146, Training: Loss:  0.3580, Accuracy: 0.8750\n",
      "Batch number : 147, Training: Loss:  0.4750, Accuracy: 0.8594\n",
      "Batch number : 148, Training: Loss:  0.3277, Accuracy: 0.9062\n",
      "Batch number : 149, Training: Loss:  0.4424, Accuracy: 0.8906\n",
      "Batch number : 150, Training: Loss:  0.4807, Accuracy: 0.8594\n",
      "Batch number : 151, Training: Loss:  0.4341, Accuracy: 0.8438\n",
      "Batch number : 152, Training: Loss:  0.2571, Accuracy: 0.9375\n",
      "Batch number : 153, Training: Loss:  0.2463, Accuracy: 0.9688\n",
      "Batch number : 154, Training: Loss:  0.5052, Accuracy: 0.8438\n",
      "Batch number : 155, Training: Loss:  0.4456, Accuracy: 0.8750\n",
      "Batch number : 156, Training: Loss:  0.8586, Accuracy: 0.7969\n",
      "Batch number : 157, Training: Loss:  0.6133, Accuracy: 0.8594\n",
      "Batch number : 158, Training: Loss:  0.5637, Accuracy: 0.8125\n",
      "Batch number : 159, Training: Loss:  0.6044, Accuracy: 0.8750\n",
      "Batch number : 160, Training: Loss:  0.4444, Accuracy: 0.8906\n",
      "Batch number : 161, Training: Loss:  0.3711, Accuracy: 0.9062\n",
      "Batch number : 162, Training: Loss:  0.5316, Accuracy: 0.8750\n",
      "Batch number : 163, Training: Loss:  0.2710, Accuracy: 0.9219\n",
      "Batch number : 164, Training: Loss:  0.4412, Accuracy: 0.8594\n",
      "Batch number : 165, Training: Loss:  0.4838, Accuracy: 0.8594\n",
      "Batch number : 166, Training: Loss:  0.6021, Accuracy: 0.8594\n",
      "Batch number : 167, Training: Loss:  0.5480, Accuracy: 0.8281\n",
      "Batch number : 168, Training: Loss:  0.5152, Accuracy: 0.8281\n",
      "Batch number : 169, Training: Loss:  0.4786, Accuracy: 0.8594\n",
      "Batch number : 170, Training: Loss:  0.8208, Accuracy: 0.7500\n",
      "Batch number : 171, Training: Loss:  0.4230, Accuracy: 0.8906\n",
      "Batch number : 172, Training: Loss:  0.7020, Accuracy: 0.7969\n",
      "Batch number : 173, Training: Loss:  0.4174, Accuracy: 0.9219\n",
      "Batch number : 174, Training: Loss:  0.5287, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.6692, Accuracy: 0.7812\n",
      "Batch number : 176, Training: Loss:  0.4019, Accuracy: 0.8906\n",
      "Batch number : 177, Training: Loss:  0.5030, Accuracy: 0.8438\n",
      "Batch number : 178, Training: Loss:  0.5967, Accuracy: 0.8281\n",
      "Batch number : 179, Training: Loss:  0.4972, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.3394, Accuracy: 0.9062\n",
      "Batch number : 181, Training: Loss:  0.5043, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.3597, Accuracy: 0.9219\n",
      "Batch number : 183, Training: Loss:  0.5146, Accuracy: 0.8438\n",
      "Batch number : 184, Training: Loss:  0.4831, Accuracy: 0.8750\n",
      "Batch number : 185, Training: Loss:  0.4055, Accuracy: 0.8906\n",
      "Batch number : 186, Training: Loss:  0.6792, Accuracy: 0.7969\n",
      "Batch number : 187, Training: Loss:  0.4735, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.3793, Accuracy: 0.8594\n",
      "Batch number : 189, Training: Loss:  0.3138, Accuracy: 0.9219\n",
      "Batch number : 190, Training: Loss:  0.5500, Accuracy: 0.8438\n",
      "Batch number : 191, Training: Loss:  0.5477, Accuracy: 0.8281\n",
      "Batch number : 192, Training: Loss:  0.4775, Accuracy: 0.8438\n",
      "Batch number : 193, Training: Loss:  0.5354, Accuracy: 0.8281\n",
      "Batch number : 194, Training: Loss:  0.3878, Accuracy: 0.8906\n",
      "Batch number : 195, Training: Loss:  0.4853, Accuracy: 0.8594\n",
      "Batch number : 196, Training: Loss:  0.6171, Accuracy: 0.8281\n",
      "Batch number : 197, Training: Loss:  0.4280, Accuracy: 0.8594\n",
      "Batch number : 198, Training: Loss:  0.4887, Accuracy: 0.8125\n",
      "Batch number : 199, Training: Loss:  0.4231, Accuracy: 0.8594\n",
      "Batch number : 200, Training: Loss:  0.4931, Accuracy: 0.8750\n",
      "Batch number : 201, Training: Loss:  0.5564, Accuracy: 0.8125\n",
      "Batch number : 202, Training: Loss:  0.7639, Accuracy: 0.7656\n",
      "Batch number : 203, Training: Loss:  0.5423, Accuracy: 0.8438\n",
      "Batch number : 204, Training: Loss:  0.6125, Accuracy: 0.8125\n",
      "Batch number : 205, Training: Loss:  0.4436, Accuracy: 0.8750\n",
      "Batch number : 206, Training: Loss:  0.4969, Accuracy: 0.8906\n",
      "Batch number : 207, Training: Loss:  0.4918, Accuracy: 0.8281\n",
      "Batch number : 208, Training: Loss:  0.5674, Accuracy: 0.8594\n",
      "Batch number : 209, Training: Loss:  0.4065, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.4588, Accuracy: 0.8750\n",
      "Batch number : 211, Training: Loss:  0.6641, Accuracy: 0.7969\n",
      "Batch number : 212, Training: Loss:  0.3087, Accuracy: 0.9062\n",
      "Batch number : 213, Training: Loss:  0.3893, Accuracy: 0.8750\n",
      "Batch number : 214, Training: Loss:  0.5727, Accuracy: 0.8281\n",
      "Batch number : 215, Training: Loss:  0.3563, Accuracy: 0.8750\n",
      "Batch number : 216, Training: Loss:  0.3660, Accuracy: 0.9062\n",
      "Batch number : 217, Training: Loss:  0.5871, Accuracy: 0.8125\n",
      "Batch number : 218, Training: Loss:  0.3061, Accuracy: 0.9219\n",
      "Batch number : 219, Training: Loss:  0.3885, Accuracy: 0.8906\n",
      "Batch number : 220, Training: Loss:  0.3699, Accuracy: 0.8906\n",
      "Batch number : 221, Training: Loss:  0.5749, Accuracy: 0.8281\n",
      "Batch number : 222, Training: Loss:  0.6099, Accuracy: 0.8125\n",
      "Batch number : 223, Training: Loss:  0.4391, Accuracy: 0.8906\n",
      "Batch number : 224, Training: Loss:  0.2537, Accuracy: 0.9531\n",
      "Batch number : 225, Training: Loss:  0.6247, Accuracy: 0.8125\n",
      "Batch number : 226, Training: Loss:  0.4735, Accuracy: 0.8125\n",
      "Batch number : 227, Training: Loss:  0.4259, Accuracy: 0.8594\n",
      "Batch number : 228, Training: Loss:  0.4201, Accuracy: 0.8750\n",
      "Batch number : 229, Training: Loss:  0.5891, Accuracy: 0.8125\n",
      "Batch number : 230, Training: Loss:  0.5326, Accuracy: 0.8438\n",
      "Batch number : 231, Training: Loss:  0.6659, Accuracy: 0.7969\n",
      "Batch number : 232, Training: Loss:  0.4409, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.5089, Accuracy: 0.8438\n",
      "Batch number : 234, Training: Loss:  0.4949, Accuracy: 0.8594\n",
      "Batch number : 235, Training: Loss:  0.6207, Accuracy: 0.8125\n",
      "Batch number : 236, Training: Loss:  0.3235, Accuracy: 0.9062\n",
      "Batch number : 237, Training: Loss:  0.5704, Accuracy: 0.7812\n",
      "Batch number : 238, Training: Loss:  0.3178, Accuracy: 0.9062\n",
      "Batch number : 239, Training: Loss:  0.7806, Accuracy: 0.7500\n",
      "Batch number : 240, Training: Loss:  0.6605, Accuracy: 0.7812\n",
      "Batch number : 241, Training: Loss:  0.3652, Accuracy: 0.8906\n",
      "Batch number : 242, Training: Loss:  0.5851, Accuracy: 0.7812\n",
      "Batch number : 243, Training: Loss:  0.2718, Accuracy: 0.9531\n",
      "Batch number : 244, Training: Loss:  0.4722, Accuracy: 0.8750\n",
      "Batch number : 245, Training: Loss:  0.4085, Accuracy: 0.8594\n",
      "Batch number : 246, Training: Loss:  0.4633, Accuracy: 0.8594\n",
      "Batch number : 247, Training: Loss:  0.4861, Accuracy: 0.8750\n",
      "Batch number : 248, Training: Loss:  0.3169, Accuracy: 0.9062\n",
      "Batch number : 249, Training: Loss:  0.3934, Accuracy: 0.9062\n",
      "Batch number : 250, Training: Loss:  0.3894, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.4901, Accuracy: 0.8438\n",
      "Batch number : 252, Training: Loss:  0.5350, Accuracy: 0.8750\n",
      "Batch number : 253, Training: Loss:  0.3560, Accuracy: 0.8906\n",
      "Batch number : 254, Training: Loss:  0.5614, Accuracy: 0.8281\n",
      "Batch number : 255, Training: Loss:  0.3278, Accuracy: 0.9375\n",
      "Batch number : 256, Training: Loss:  0.4218, Accuracy: 0.8750\n",
      "Batch number : 257, Training: Loss:  0.3333, Accuracy: 0.8906\n",
      "Batch number : 258, Training: Loss:  0.6990, Accuracy: 0.7969\n",
      "Batch number : 259, Training: Loss:  0.6661, Accuracy: 0.8281\n",
      "Batch number : 260, Training: Loss:  0.4359, Accuracy: 0.8750\n",
      "Batch number : 261, Training: Loss:  0.4239, Accuracy: 0.8750\n",
      "Batch number : 262, Training: Loss:  0.3280, Accuracy: 0.9375\n",
      "Batch number : 263, Training: Loss:  0.4073, Accuracy: 0.8906\n",
      "Batch number : 264, Training: Loss:  0.4236, Accuracy: 0.8750\n",
      "Batch number : 265, Training: Loss:  0.5303, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.5022, Accuracy: 0.8750\n",
      "Batch number : 267, Training: Loss:  0.5868, Accuracy: 0.8281\n",
      "Batch number : 268, Training: Loss:  0.3691, Accuracy: 0.9062\n",
      "Batch number : 269, Training: Loss:  0.2953, Accuracy: 0.9219\n",
      "Batch number : 270, Training: Loss:  0.4234, Accuracy: 0.8750\n",
      "Batch number : 271, Training: Loss:  0.5912, Accuracy: 0.8281\n",
      "Batch number : 272, Training: Loss:  0.3105, Accuracy: 0.9062\n",
      "Batch number : 273, Training: Loss:  0.4043, Accuracy: 0.8594\n",
      "Batch number : 274, Training: Loss:  0.5140, Accuracy: 0.8594\n",
      "Batch number : 275, Training: Loss:  0.7606, Accuracy: 0.8281\n",
      "Batch number : 276, Training: Loss:  0.7468, Accuracy: 0.7812\n",
      "Batch number : 277, Training: Loss:  0.4689, Accuracy: 0.8594\n",
      "Batch number : 278, Training: Loss:  0.5071, Accuracy: 0.8594\n",
      "Batch number : 279, Training: Loss:  0.4583, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.4066, Accuracy: 0.8594\n",
      "Batch number : 281, Training: Loss:  0.3470, Accuracy: 0.9062\n",
      "Batch number : 282, Training: Loss:  0.4465, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.4332, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.4209, Accuracy: 0.8750\n",
      "Batch number : 285, Training: Loss:  0.4759, Accuracy: 0.8438\n",
      "Batch number : 286, Training: Loss:  0.5399, Accuracy: 0.8281\n",
      "Batch number : 287, Training: Loss:  0.5277, Accuracy: 0.8281\n",
      "Batch number : 288, Training: Loss:  0.7735, Accuracy: 0.7656\n",
      "Batch number : 289, Training: Loss:  0.6694, Accuracy: 0.8281\n",
      "Batch number : 290, Training: Loss:  0.4506, Accuracy: 0.8438\n",
      "Batch number : 291, Training: Loss:  0.4918, Accuracy: 0.8750\n",
      "Batch number : 292, Training: Loss:  0.6284, Accuracy: 0.7812\n",
      "Batch number : 293, Training: Loss:  0.4678, Accuracy: 0.8438\n",
      "Batch number : 294, Training: Loss:  0.5423, Accuracy: 0.8750\n",
      "Batch number : 295, Training: Loss:  0.6153, Accuracy: 0.7969\n",
      "Batch number : 296, Training: Loss:  0.2892, Accuracy: 0.9062\n",
      "Batch number : 297, Training: Loss:  0.4475, Accuracy: 0.8906\n",
      "Batch number : 298, Training: Loss:  0.4411, Accuracy: 0.9062\n",
      "Batch number : 299, Training: Loss:  0.5231, Accuracy: 0.8281\n",
      "Batch number : 300, Training: Loss:  0.6142, Accuracy: 0.7969\n",
      "Batch number : 301, Training: Loss:  0.6569, Accuracy: 0.7812\n",
      "Batch number : 302, Training: Loss:  0.8978, Accuracy: 0.7812\n",
      "Batch number : 303, Training: Loss:  0.3469, Accuracy: 0.9062\n",
      "Batch number : 304, Training: Loss:  0.4803, Accuracy: 0.8906\n",
      "Batch number : 305, Training: Loss:  0.5599, Accuracy: 0.8438\n",
      "Batch number : 306, Training: Loss:  0.3952, Accuracy: 0.9531\n",
      "Batch number : 307, Training: Loss:  0.4081, Accuracy: 0.8750\n",
      "Batch number : 308, Training: Loss:  0.6113, Accuracy: 0.7500\n",
      "Batch number : 309, Training: Loss:  0.5166, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.4334, Accuracy: 0.8750\n",
      "Batch number : 311, Training: Loss:  0.5512, Accuracy: 0.8281\n",
      "Batch number : 312, Training: Loss:  0.4312, Accuracy: 0.8906\n",
      "Batch number : 313, Training: Loss:  0.4433, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.4081, Accuracy: 0.8750\n",
      "Batch number : 315, Training: Loss:  0.3152, Accuracy: 0.8906\n",
      "Batch number : 316, Training: Loss:  0.5470, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.4240, Accuracy: 0.9062\n",
      "Batch number : 318, Training: Loss:  0.3662, Accuracy: 0.8906\n",
      "Batch number : 319, Training: Loss:  0.3809, Accuracy: 0.8594\n",
      "Batch number : 320, Training: Loss:  0.2668, Accuracy: 0.9219\n",
      "Batch number : 321, Training: Loss:  0.2712, Accuracy: 0.9062\n",
      "Batch number : 322, Training: Loss:  0.2324, Accuracy: 0.9062\n",
      "Batch number : 323, Training: Loss:  0.6637, Accuracy: 0.7969\n",
      "Batch number : 324, Training: Loss:  0.7162, Accuracy: 0.7500\n",
      "Batch number : 325, Training: Loss:  0.4695, Accuracy: 0.8438\n",
      "Batch number : 326, Training: Loss:  0.4042, Accuracy: 0.8906\n",
      "Batch number : 327, Training: Loss:  0.3742, Accuracy: 0.8906\n",
      "Batch number : 328, Training: Loss:  0.3354, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.3728, Accuracy: 0.8906\n",
      "Batch number : 330, Training: Loss:  0.5138, Accuracy: 0.8281\n",
      "Batch number : 331, Training: Loss:  0.4340, Accuracy: 0.8750\n",
      "Batch number : 332, Training: Loss:  0.5570, Accuracy: 0.7969\n",
      "Batch number : 333, Training: Loss:  0.3946, Accuracy: 0.8594\n",
      "Batch number : 334, Training: Loss:  0.3861, Accuracy: 0.8594\n",
      "Batch number : 335, Training: Loss:  0.5462, Accuracy: 0.8125\n",
      "Batch number : 336, Training: Loss:  0.3622, Accuracy: 0.9219\n",
      "Batch number : 337, Training: Loss:  0.3686, Accuracy: 0.8750\n",
      "Batch number : 338, Training: Loss:  0.5142, Accuracy: 0.8281\n",
      "Batch number : 339, Training: Loss:  0.6573, Accuracy: 0.7812\n",
      "Batch number : 340, Training: Loss:  0.3364, Accuracy: 0.8906\n",
      "Batch number : 341, Training: Loss:  0.5188, Accuracy: 0.7969\n",
      "Batch number : 342, Training: Loss:  0.4466, Accuracy: 0.8594\n",
      "Batch number : 343, Training: Loss:  0.3729, Accuracy: 0.9062\n",
      "Batch number : 344, Training: Loss:  0.4632, Accuracy: 0.9062\n",
      "Batch number : 345, Training: Loss:  0.2139, Accuracy: 0.9531\n",
      "Batch number : 346, Training: Loss:  0.4399, Accuracy: 0.8594\n",
      "Batch number : 347, Training: Loss:  0.5355, Accuracy: 0.8125\n",
      "Batch number : 348, Training: Loss:  0.4023, Accuracy: 0.8750\n",
      "Batch number : 349, Training: Loss:  0.4182, Accuracy: 0.8750\n",
      "Batch number : 350, Training: Loss:  0.6961, Accuracy: 0.8125\n",
      "Batch number : 351, Training: Loss:  0.7899, Accuracy: 0.7500\n",
      "Batch number : 352, Training: Loss:  0.5670, Accuracy: 0.8281\n",
      "Batch number : 353, Training: Loss:  0.5225, Accuracy: 0.8750\n",
      "Batch number : 354, Training: Loss:  0.6461, Accuracy: 0.8125\n",
      "Batch number : 355, Training: Loss:  0.7017, Accuracy: 0.7969\n",
      "Batch number : 356, Training: Loss:  0.4808, Accuracy: 0.8281\n",
      "Batch number : 357, Training: Loss:  0.4760, Accuracy: 0.8750\n",
      "Batch number : 358, Training: Loss:  0.4585, Accuracy: 0.8594\n",
      "Batch number : 359, Training: Loss:  0.4225, Accuracy: 0.9062\n",
      "Batch number : 360, Training: Loss:  0.3857, Accuracy: 0.9219\n",
      "Batch number : 361, Training: Loss:  0.5325, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.4922, Accuracy: 0.8281\n",
      "Batch number : 363, Training: Loss:  0.3822, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.6831, Accuracy: 0.8125\n",
      "Batch number : 365, Training: Loss:  0.5779, Accuracy: 0.8125\n",
      "Batch number : 366, Training: Loss:  0.6347, Accuracy: 0.7969\n",
      "Batch number : 367, Training: Loss:  0.8109, Accuracy: 0.7656\n",
      "Batch number : 368, Training: Loss:  0.5365, Accuracy: 0.8281\n",
      "Batch number : 369, Training: Loss:  0.6485, Accuracy: 0.7656\n",
      "Batch number : 370, Training: Loss:  0.3991, Accuracy: 0.8906\n",
      "Batch number : 371, Training: Loss:  0.4285, Accuracy: 0.8906\n",
      "Batch number : 372, Training: Loss:  0.5226, Accuracy: 0.8438\n",
      "Batch number : 373, Training: Loss:  0.5650, Accuracy: 0.7812\n",
      "Batch number : 374, Training: Loss:  0.5582, Accuracy: 0.8125\n",
      "Batch number : 375, Training: Loss:  0.4043, Accuracy: 0.9062\n",
      "Batch number : 376, Training: Loss:  0.5547, Accuracy: 0.9062\n",
      "Batch number : 377, Training: Loss:  0.3199, Accuracy: 0.9250\n",
      "Epoch: 12/20\n",
      "Batch number : 000, Training: Loss:  0.5570, Accuracy: 0.8594\n",
      "Batch number : 001, Training: Loss:  0.4458, Accuracy: 0.8906\n",
      "Batch number : 002, Training: Loss:  0.4675, Accuracy: 0.8438\n",
      "Batch number : 003, Training: Loss:  0.6684, Accuracy: 0.7812\n",
      "Batch number : 004, Training: Loss:  0.4907, Accuracy: 0.8750\n",
      "Batch number : 005, Training: Loss:  0.5143, Accuracy: 0.8438\n",
      "Batch number : 006, Training: Loss:  0.3349, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.4138, Accuracy: 0.8594\n",
      "Batch number : 008, Training: Loss:  0.3500, Accuracy: 0.8750\n",
      "Batch number : 009, Training: Loss:  0.4260, Accuracy: 0.8750\n",
      "Batch number : 010, Training: Loss:  0.4050, Accuracy: 0.8594\n",
      "Batch number : 011, Training: Loss:  0.2600, Accuracy: 0.9219\n",
      "Batch number : 012, Training: Loss:  0.4198, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.7384, Accuracy: 0.7656\n",
      "Batch number : 014, Training: Loss:  0.3471, Accuracy: 0.8906\n",
      "Batch number : 015, Training: Loss:  0.1982, Accuracy: 0.9688\n",
      "Batch number : 016, Training: Loss:  0.5297, Accuracy: 0.9062\n",
      "Batch number : 017, Training: Loss:  0.4302, Accuracy: 0.9062\n",
      "Batch number : 018, Training: Loss:  0.4778, Accuracy: 0.8281\n",
      "Batch number : 019, Training: Loss:  0.3777, Accuracy: 0.8594\n",
      "Batch number : 020, Training: Loss:  0.2570, Accuracy: 0.9375\n",
      "Batch number : 021, Training: Loss:  0.3612, Accuracy: 0.8906\n",
      "Batch number : 022, Training: Loss:  0.4450, Accuracy: 0.8750\n",
      "Batch number : 023, Training: Loss:  0.4887, Accuracy: 0.8594\n",
      "Batch number : 024, Training: Loss:  0.6417, Accuracy: 0.8281\n",
      "Batch number : 025, Training: Loss:  0.3866, Accuracy: 0.8906\n",
      "Batch number : 026, Training: Loss:  0.4455, Accuracy: 0.8750\n",
      "Batch number : 027, Training: Loss:  0.5808, Accuracy: 0.8281\n",
      "Batch number : 028, Training: Loss:  0.4279, Accuracy: 0.8906\n",
      "Batch number : 029, Training: Loss:  0.5125, Accuracy: 0.8438\n",
      "Batch number : 030, Training: Loss:  0.3676, Accuracy: 0.9062\n",
      "Batch number : 031, Training: Loss:  0.4803, Accuracy: 0.8281\n",
      "Batch number : 032, Training: Loss:  0.5858, Accuracy: 0.8125\n",
      "Batch number : 033, Training: Loss:  0.2904, Accuracy: 0.9219\n",
      "Batch number : 034, Training: Loss:  0.5751, Accuracy: 0.8281\n",
      "Batch number : 035, Training: Loss:  0.4612, Accuracy: 0.8438\n",
      "Batch number : 036, Training: Loss:  0.3079, Accuracy: 0.9375\n",
      "Batch number : 037, Training: Loss:  0.6773, Accuracy: 0.7656\n",
      "Batch number : 038, Training: Loss:  0.5137, Accuracy: 0.8594\n",
      "Batch number : 039, Training: Loss:  0.4053, Accuracy: 0.8750\n",
      "Batch number : 040, Training: Loss:  0.7052, Accuracy: 0.7812\n",
      "Batch number : 041, Training: Loss:  0.5282, Accuracy: 0.8281\n",
      "Batch number : 042, Training: Loss:  0.4226, Accuracy: 0.8750\n",
      "Batch number : 043, Training: Loss:  0.6277, Accuracy: 0.7656\n",
      "Batch number : 044, Training: Loss:  0.4117, Accuracy: 0.8750\n",
      "Batch number : 045, Training: Loss:  0.5201, Accuracy: 0.8594\n",
      "Batch number : 046, Training: Loss:  0.3108, Accuracy: 0.9219\n",
      "Batch number : 047, Training: Loss:  0.3792, Accuracy: 0.8750\n",
      "Batch number : 048, Training: Loss:  0.5991, Accuracy: 0.8125\n",
      "Batch number : 049, Training: Loss:  0.4982, Accuracy: 0.8750\n",
      "Batch number : 050, Training: Loss:  0.6604, Accuracy: 0.7812\n",
      "Batch number : 051, Training: Loss:  0.4395, Accuracy: 0.8594\n",
      "Batch number : 052, Training: Loss:  0.3603, Accuracy: 0.8750\n",
      "Batch number : 053, Training: Loss:  0.7122, Accuracy: 0.7969\n",
      "Batch number : 054, Training: Loss:  0.3888, Accuracy: 0.8906\n",
      "Batch number : 055, Training: Loss:  0.3831, Accuracy: 0.8750\n",
      "Batch number : 056, Training: Loss:  0.3622, Accuracy: 0.9062\n",
      "Batch number : 057, Training: Loss:  0.4454, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.6749, Accuracy: 0.8281\n",
      "Batch number : 059, Training: Loss:  0.4696, Accuracy: 0.8594\n",
      "Batch number : 060, Training: Loss:  0.4742, Accuracy: 0.8750\n",
      "Batch number : 061, Training: Loss:  0.6746, Accuracy: 0.7656\n",
      "Batch number : 062, Training: Loss:  0.5072, Accuracy: 0.8438\n",
      "Batch number : 063, Training: Loss:  0.4858, Accuracy: 0.8281\n",
      "Batch number : 064, Training: Loss:  0.4190, Accuracy: 0.8594\n",
      "Batch number : 065, Training: Loss:  0.4259, Accuracy: 0.8594\n",
      "Batch number : 066, Training: Loss:  0.2320, Accuracy: 0.9375\n",
      "Batch number : 067, Training: Loss:  0.5697, Accuracy: 0.8594\n",
      "Batch number : 068, Training: Loss:  0.6049, Accuracy: 0.8594\n",
      "Batch number : 069, Training: Loss:  0.6255, Accuracy: 0.7969\n",
      "Batch number : 070, Training: Loss:  0.5162, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.2434, Accuracy: 0.9375\n",
      "Batch number : 072, Training: Loss:  0.6603, Accuracy: 0.7969\n",
      "Batch number : 073, Training: Loss:  0.4307, Accuracy: 0.8438\n",
      "Batch number : 074, Training: Loss:  0.6529, Accuracy: 0.7969\n",
      "Batch number : 075, Training: Loss:  0.5134, Accuracy: 0.8594\n",
      "Batch number : 076, Training: Loss:  0.3810, Accuracy: 0.9062\n",
      "Batch number : 077, Training: Loss:  0.3318, Accuracy: 0.9219\n",
      "Batch number : 078, Training: Loss:  0.4217, Accuracy: 0.9062\n",
      "Batch number : 079, Training: Loss:  0.5552, Accuracy: 0.8438\n",
      "Batch number : 080, Training: Loss:  0.5584, Accuracy: 0.8125\n",
      "Batch number : 081, Training: Loss:  0.2852, Accuracy: 0.9062\n",
      "Batch number : 082, Training: Loss:  0.5972, Accuracy: 0.8281\n",
      "Batch number : 083, Training: Loss:  0.9009, Accuracy: 0.7500\n",
      "Batch number : 084, Training: Loss:  0.3426, Accuracy: 0.9062\n",
      "Batch number : 085, Training: Loss:  0.3490, Accuracy: 0.8906\n",
      "Batch number : 086, Training: Loss:  0.2852, Accuracy: 0.9219\n",
      "Batch number : 087, Training: Loss:  0.3151, Accuracy: 0.9219\n",
      "Batch number : 088, Training: Loss:  0.6561, Accuracy: 0.7812\n",
      "Batch number : 089, Training: Loss:  0.4404, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.4049, Accuracy: 0.8750\n",
      "Batch number : 091, Training: Loss:  0.6101, Accuracy: 0.8281\n",
      "Batch number : 092, Training: Loss:  0.6050, Accuracy: 0.7812\n",
      "Batch number : 093, Training: Loss:  0.5716, Accuracy: 0.7969\n",
      "Batch number : 094, Training: Loss:  0.4061, Accuracy: 0.8906\n",
      "Batch number : 095, Training: Loss:  0.5624, Accuracy: 0.8438\n",
      "Batch number : 096, Training: Loss:  0.7132, Accuracy: 0.7812\n",
      "Batch number : 097, Training: Loss:  0.3391, Accuracy: 0.9219\n",
      "Batch number : 098, Training: Loss:  0.5291, Accuracy: 0.8125\n",
      "Batch number : 099, Training: Loss:  0.6247, Accuracy: 0.7656\n",
      "Batch number : 100, Training: Loss:  0.3805, Accuracy: 0.8750\n",
      "Batch number : 101, Training: Loss:  0.5400, Accuracy: 0.8438\n",
      "Batch number : 102, Training: Loss:  0.5229, Accuracy: 0.8438\n",
      "Batch number : 103, Training: Loss:  0.4501, Accuracy: 0.8594\n",
      "Batch number : 104, Training: Loss:  0.4087, Accuracy: 0.9062\n",
      "Batch number : 105, Training: Loss:  0.6952, Accuracy: 0.7969\n",
      "Batch number : 106, Training: Loss:  0.3483, Accuracy: 0.8750\n",
      "Batch number : 107, Training: Loss:  0.5603, Accuracy: 0.8438\n",
      "Batch number : 108, Training: Loss:  0.3216, Accuracy: 0.9062\n",
      "Batch number : 109, Training: Loss:  0.7135, Accuracy: 0.8281\n",
      "Batch number : 110, Training: Loss:  0.3664, Accuracy: 0.9062\n",
      "Batch number : 111, Training: Loss:  0.5234, Accuracy: 0.8281\n",
      "Batch number : 112, Training: Loss:  0.5693, Accuracy: 0.7812\n",
      "Batch number : 113, Training: Loss:  0.3856, Accuracy: 0.8750\n",
      "Batch number : 114, Training: Loss:  0.3951, Accuracy: 0.8750\n",
      "Batch number : 115, Training: Loss:  0.4164, Accuracy: 0.9062\n",
      "Batch number : 116, Training: Loss:  0.6981, Accuracy: 0.7344\n",
      "Batch number : 117, Training: Loss:  0.5762, Accuracy: 0.7969\n",
      "Batch number : 118, Training: Loss:  0.5146, Accuracy: 0.8594\n",
      "Batch number : 119, Training: Loss:  0.5376, Accuracy: 0.8281\n",
      "Batch number : 120, Training: Loss:  0.4493, Accuracy: 0.8594\n",
      "Batch number : 121, Training: Loss:  0.4309, Accuracy: 0.8906\n",
      "Batch number : 122, Training: Loss:  0.4012, Accuracy: 0.8594\n",
      "Batch number : 123, Training: Loss:  0.4123, Accuracy: 0.8906\n",
      "Batch number : 124, Training: Loss:  0.3415, Accuracy: 0.9219\n",
      "Batch number : 125, Training: Loss:  0.5484, Accuracy: 0.8125\n",
      "Batch number : 126, Training: Loss:  0.4475, Accuracy: 0.8438\n",
      "Batch number : 127, Training: Loss:  0.5176, Accuracy: 0.8438\n",
      "Batch number : 128, Training: Loss:  0.7105, Accuracy: 0.7812\n",
      "Batch number : 129, Training: Loss:  0.4716, Accuracy: 0.8750\n",
      "Batch number : 130, Training: Loss:  0.2645, Accuracy: 0.9531\n",
      "Batch number : 131, Training: Loss:  0.4376, Accuracy: 0.8906\n",
      "Batch number : 132, Training: Loss:  0.5798, Accuracy: 0.7969\n",
      "Batch number : 133, Training: Loss:  0.5347, Accuracy: 0.7812\n",
      "Batch number : 134, Training: Loss:  0.3697, Accuracy: 0.8750\n",
      "Batch number : 135, Training: Loss:  0.3597, Accuracy: 0.9688\n",
      "Batch number : 136, Training: Loss:  0.3364, Accuracy: 0.9062\n",
      "Batch number : 137, Training: Loss:  0.6213, Accuracy: 0.8438\n",
      "Batch number : 138, Training: Loss:  0.3416, Accuracy: 0.8906\n",
      "Batch number : 139, Training: Loss:  0.2525, Accuracy: 0.9531\n",
      "Batch number : 140, Training: Loss:  0.3371, Accuracy: 0.8906\n",
      "Batch number : 141, Training: Loss:  0.4023, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.5569, Accuracy: 0.8594\n",
      "Batch number : 143, Training: Loss:  0.6741, Accuracy: 0.8281\n",
      "Batch number : 144, Training: Loss:  0.4303, Accuracy: 0.8750\n",
      "Batch number : 145, Training: Loss:  0.3889, Accuracy: 0.9062\n",
      "Batch number : 146, Training: Loss:  0.5182, Accuracy: 0.8438\n",
      "Batch number : 147, Training: Loss:  0.2979, Accuracy: 0.9219\n",
      "Batch number : 148, Training: Loss:  0.2950, Accuracy: 0.9219\n",
      "Batch number : 149, Training: Loss:  0.4255, Accuracy: 0.8281\n",
      "Batch number : 150, Training: Loss:  0.3848, Accuracy: 0.8906\n",
      "Batch number : 151, Training: Loss:  0.5467, Accuracy: 0.8281\n",
      "Batch number : 152, Training: Loss:  0.6851, Accuracy: 0.8125\n",
      "Batch number : 153, Training: Loss:  0.5663, Accuracy: 0.8281\n",
      "Batch number : 154, Training: Loss:  0.3572, Accuracy: 0.9062\n",
      "Batch number : 155, Training: Loss:  0.5000, Accuracy: 0.8438\n",
      "Batch number : 156, Training: Loss:  0.4885, Accuracy: 0.8594\n",
      "Batch number : 157, Training: Loss:  0.4222, Accuracy: 0.8906\n",
      "Batch number : 158, Training: Loss:  0.4939, Accuracy: 0.8594\n",
      "Batch number : 159, Training: Loss:  0.4448, Accuracy: 0.8594\n",
      "Batch number : 160, Training: Loss:  0.5197, Accuracy: 0.8281\n",
      "Batch number : 161, Training: Loss:  0.5256, Accuracy: 0.7969\n",
      "Batch number : 162, Training: Loss:  0.3488, Accuracy: 0.9219\n",
      "Batch number : 163, Training: Loss:  0.4439, Accuracy: 0.8594\n",
      "Batch number : 164, Training: Loss:  0.4490, Accuracy: 0.8281\n",
      "Batch number : 165, Training: Loss:  0.6414, Accuracy: 0.8125\n",
      "Batch number : 166, Training: Loss:  0.4763, Accuracy: 0.8281\n",
      "Batch number : 167, Training: Loss:  0.4448, Accuracy: 0.8906\n",
      "Batch number : 168, Training: Loss:  0.5842, Accuracy: 0.8125\n",
      "Batch number : 169, Training: Loss:  0.4429, Accuracy: 0.8906\n",
      "Batch number : 170, Training: Loss:  0.5257, Accuracy: 0.8750\n",
      "Batch number : 171, Training: Loss:  0.4424, Accuracy: 0.8906\n",
      "Batch number : 172, Training: Loss:  0.5795, Accuracy: 0.8438\n",
      "Batch number : 173, Training: Loss:  0.3954, Accuracy: 0.8906\n",
      "Batch number : 174, Training: Loss:  0.5180, Accuracy: 0.8281\n",
      "Batch number : 175, Training: Loss:  0.6086, Accuracy: 0.7969\n",
      "Batch number : 176, Training: Loss:  0.4518, Accuracy: 0.8750\n",
      "Batch number : 177, Training: Loss:  0.4187, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.5528, Accuracy: 0.8281\n",
      "Batch number : 179, Training: Loss:  0.5729, Accuracy: 0.8125\n",
      "Batch number : 180, Training: Loss:  0.3817, Accuracy: 0.8906\n",
      "Batch number : 181, Training: Loss:  0.4107, Accuracy: 0.8750\n",
      "Batch number : 182, Training: Loss:  0.4484, Accuracy: 0.8594\n",
      "Batch number : 183, Training: Loss:  0.3119, Accuracy: 0.9375\n",
      "Batch number : 184, Training: Loss:  0.4152, Accuracy: 0.8906\n",
      "Batch number : 185, Training: Loss:  0.4164, Accuracy: 0.8594\n",
      "Batch number : 186, Training: Loss:  0.2338, Accuracy: 0.9375\n",
      "Batch number : 187, Training: Loss:  0.4679, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.6173, Accuracy: 0.8594\n",
      "Batch number : 189, Training: Loss:  0.3927, Accuracy: 0.8906\n",
      "Batch number : 190, Training: Loss:  0.2663, Accuracy: 0.9375\n",
      "Batch number : 191, Training: Loss:  0.4103, Accuracy: 0.8906\n",
      "Batch number : 192, Training: Loss:  0.4580, Accuracy: 0.8594\n",
      "Batch number : 193, Training: Loss:  0.3042, Accuracy: 0.9375\n",
      "Batch number : 194, Training: Loss:  0.5385, Accuracy: 0.8438\n",
      "Batch number : 195, Training: Loss:  0.5053, Accuracy: 0.8281\n",
      "Batch number : 196, Training: Loss:  0.4031, Accuracy: 0.8594\n",
      "Batch number : 197, Training: Loss:  0.3198, Accuracy: 0.9062\n",
      "Batch number : 198, Training: Loss:  0.7339, Accuracy: 0.7656\n",
      "Batch number : 199, Training: Loss:  0.6073, Accuracy: 0.8281\n",
      "Batch number : 200, Training: Loss:  0.5822, Accuracy: 0.7969\n",
      "Batch number : 201, Training: Loss:  0.4408, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.4193, Accuracy: 0.8750\n",
      "Batch number : 203, Training: Loss:  0.3973, Accuracy: 0.8906\n",
      "Batch number : 204, Training: Loss:  0.4618, Accuracy: 0.8438\n",
      "Batch number : 205, Training: Loss:  0.4327, Accuracy: 0.8438\n",
      "Batch number : 206, Training: Loss:  0.4180, Accuracy: 0.8906\n",
      "Batch number : 207, Training: Loss:  0.3364, Accuracy: 0.8906\n",
      "Batch number : 208, Training: Loss:  0.4379, Accuracy: 0.8906\n",
      "Batch number : 209, Training: Loss:  0.4216, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.5671, Accuracy: 0.8438\n",
      "Batch number : 211, Training: Loss:  0.6419, Accuracy: 0.8281\n",
      "Batch number : 212, Training: Loss:  0.3559, Accuracy: 0.8906\n",
      "Batch number : 213, Training: Loss:  0.4097, Accuracy: 0.8750\n",
      "Batch number : 214, Training: Loss:  0.4457, Accuracy: 0.8594\n",
      "Batch number : 215, Training: Loss:  0.4059, Accuracy: 0.8594\n",
      "Batch number : 216, Training: Loss:  0.7513, Accuracy: 0.7969\n",
      "Batch number : 217, Training: Loss:  0.5414, Accuracy: 0.8750\n",
      "Batch number : 218, Training: Loss:  0.4143, Accuracy: 0.8750\n",
      "Batch number : 219, Training: Loss:  0.5478, Accuracy: 0.8438\n",
      "Batch number : 220, Training: Loss:  0.4725, Accuracy: 0.8594\n",
      "Batch number : 221, Training: Loss:  0.4783, Accuracy: 0.8906\n",
      "Batch number : 222, Training: Loss:  0.3623, Accuracy: 0.8906\n",
      "Batch number : 223, Training: Loss:  0.4059, Accuracy: 0.8750\n",
      "Batch number : 224, Training: Loss:  0.4745, Accuracy: 0.8438\n",
      "Batch number : 225, Training: Loss:  0.4824, Accuracy: 0.8438\n",
      "Batch number : 226, Training: Loss:  0.4239, Accuracy: 0.8906\n",
      "Batch number : 227, Training: Loss:  0.4372, Accuracy: 0.8438\n",
      "Batch number : 228, Training: Loss:  0.5021, Accuracy: 0.8438\n",
      "Batch number : 229, Training: Loss:  0.4591, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.5077, Accuracy: 0.8594\n",
      "Batch number : 231, Training: Loss:  0.7086, Accuracy: 0.7656\n",
      "Batch number : 232, Training: Loss:  0.3933, Accuracy: 0.8594\n",
      "Batch number : 233, Training: Loss:  0.5778, Accuracy: 0.8125\n",
      "Batch number : 234, Training: Loss:  0.8131, Accuracy: 0.7969\n",
      "Batch number : 235, Training: Loss:  0.5471, Accuracy: 0.8750\n",
      "Batch number : 236, Training: Loss:  0.5990, Accuracy: 0.8125\n",
      "Batch number : 237, Training: Loss:  0.5794, Accuracy: 0.8125\n",
      "Batch number : 238, Training: Loss:  0.4165, Accuracy: 0.9219\n",
      "Batch number : 239, Training: Loss:  0.4169, Accuracy: 0.9062\n",
      "Batch number : 240, Training: Loss:  0.4302, Accuracy: 0.8750\n",
      "Batch number : 241, Training: Loss:  0.6116, Accuracy: 0.7969\n",
      "Batch number : 242, Training: Loss:  0.4250, Accuracy: 0.8906\n",
      "Batch number : 243, Training: Loss:  0.4748, Accuracy: 0.8281\n",
      "Batch number : 244, Training: Loss:  0.3102, Accuracy: 0.9219\n",
      "Batch number : 245, Training: Loss:  0.5359, Accuracy: 0.8438\n",
      "Batch number : 246, Training: Loss:  0.5368, Accuracy: 0.8438\n",
      "Batch number : 247, Training: Loss:  0.7133, Accuracy: 0.7812\n",
      "Batch number : 248, Training: Loss:  0.5553, Accuracy: 0.8281\n",
      "Batch number : 249, Training: Loss:  0.6096, Accuracy: 0.8438\n",
      "Batch number : 250, Training: Loss:  0.3435, Accuracy: 0.8906\n",
      "Batch number : 251, Training: Loss:  0.3923, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.3973, Accuracy: 0.8906\n",
      "Batch number : 253, Training: Loss:  0.4365, Accuracy: 0.8750\n",
      "Batch number : 254, Training: Loss:  0.4080, Accuracy: 0.8750\n",
      "Batch number : 255, Training: Loss:  0.5311, Accuracy: 0.8125\n",
      "Batch number : 256, Training: Loss:  0.5335, Accuracy: 0.8125\n",
      "Batch number : 257, Training: Loss:  0.4284, Accuracy: 0.8594\n",
      "Batch number : 258, Training: Loss:  0.4669, Accuracy: 0.8594\n",
      "Batch number : 259, Training: Loss:  0.6358, Accuracy: 0.7969\n",
      "Batch number : 260, Training: Loss:  0.4401, Accuracy: 0.8906\n",
      "Batch number : 261, Training: Loss:  0.3624, Accuracy: 0.8906\n",
      "Batch number : 262, Training: Loss:  0.4290, Accuracy: 0.8438\n",
      "Batch number : 263, Training: Loss:  0.5161, Accuracy: 0.8750\n",
      "Batch number : 264, Training: Loss:  0.3518, Accuracy: 0.9062\n",
      "Batch number : 265, Training: Loss:  0.4544, Accuracy: 0.8594\n",
      "Batch number : 266, Training: Loss:  0.3045, Accuracy: 0.8906\n",
      "Batch number : 267, Training: Loss:  0.4405, Accuracy: 0.9062\n",
      "Batch number : 268, Training: Loss:  0.7078, Accuracy: 0.8281\n",
      "Batch number : 269, Training: Loss:  0.5951, Accuracy: 0.8281\n",
      "Batch number : 270, Training: Loss:  0.4456, Accuracy: 0.8750\n",
      "Batch number : 271, Training: Loss:  0.6932, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.4922, Accuracy: 0.8750\n",
      "Batch number : 273, Training: Loss:  0.3334, Accuracy: 0.9062\n",
      "Batch number : 274, Training: Loss:  0.3345, Accuracy: 0.9219\n",
      "Batch number : 275, Training: Loss:  0.4621, Accuracy: 0.8438\n",
      "Batch number : 276, Training: Loss:  0.4825, Accuracy: 0.8750\n",
      "Batch number : 277, Training: Loss:  0.5491, Accuracy: 0.8438\n",
      "Batch number : 278, Training: Loss:  0.6196, Accuracy: 0.7812\n",
      "Batch number : 279, Training: Loss:  0.6290, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.5166, Accuracy: 0.8438\n",
      "Batch number : 281, Training: Loss:  0.5087, Accuracy: 0.8594\n",
      "Batch number : 282, Training: Loss:  0.3650, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.4539, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.5242, Accuracy: 0.8125\n",
      "Batch number : 285, Training: Loss:  0.4624, Accuracy: 0.8438\n",
      "Batch number : 286, Training: Loss:  0.4006, Accuracy: 0.8906\n",
      "Batch number : 287, Training: Loss:  0.3700, Accuracy: 0.9062\n",
      "Batch number : 288, Training: Loss:  0.6803, Accuracy: 0.7812\n",
      "Batch number : 289, Training: Loss:  0.4946, Accuracy: 0.8594\n",
      "Batch number : 290, Training: Loss:  0.5345, Accuracy: 0.9062\n",
      "Batch number : 291, Training: Loss:  0.3304, Accuracy: 0.9219\n",
      "Batch number : 292, Training: Loss:  0.5023, Accuracy: 0.8281\n",
      "Batch number : 293, Training: Loss:  0.4029, Accuracy: 0.8438\n",
      "Batch number : 294, Training: Loss:  0.2705, Accuracy: 0.9219\n",
      "Batch number : 295, Training: Loss:  0.7610, Accuracy: 0.7969\n",
      "Batch number : 296, Training: Loss:  0.3816, Accuracy: 0.9219\n",
      "Batch number : 297, Training: Loss:  0.4689, Accuracy: 0.8281\n",
      "Batch number : 298, Training: Loss:  0.5976, Accuracy: 0.8125\n",
      "Batch number : 299, Training: Loss:  0.4136, Accuracy: 0.9062\n",
      "Batch number : 300, Training: Loss:  0.4796, Accuracy: 0.8438\n",
      "Batch number : 301, Training: Loss:  0.3906, Accuracy: 0.8750\n",
      "Batch number : 302, Training: Loss:  0.3849, Accuracy: 0.9062\n",
      "Batch number : 303, Training: Loss:  0.3045, Accuracy: 0.9219\n",
      "Batch number : 304, Training: Loss:  0.5642, Accuracy: 0.8281\n",
      "Batch number : 305, Training: Loss:  0.3988, Accuracy: 0.8438\n",
      "Batch number : 306, Training: Loss:  0.5474, Accuracy: 0.8125\n",
      "Batch number : 307, Training: Loss:  0.3467, Accuracy: 0.9062\n",
      "Batch number : 308, Training: Loss:  0.5589, Accuracy: 0.8594\n",
      "Batch number : 309, Training: Loss:  0.5731, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.6379, Accuracy: 0.8438\n",
      "Batch number : 311, Training: Loss:  0.5235, Accuracy: 0.8438\n",
      "Batch number : 312, Training: Loss:  0.5569, Accuracy: 0.8125\n",
      "Batch number : 313, Training: Loss:  0.5029, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.3762, Accuracy: 0.8594\n",
      "Batch number : 315, Training: Loss:  0.6357, Accuracy: 0.8125\n",
      "Batch number : 316, Training: Loss:  0.6384, Accuracy: 0.7500\n",
      "Batch number : 317, Training: Loss:  0.7344, Accuracy: 0.7656\n",
      "Batch number : 318, Training: Loss:  0.5370, Accuracy: 0.8125\n",
      "Batch number : 319, Training: Loss:  0.3925, Accuracy: 0.8906\n",
      "Batch number : 320, Training: Loss:  0.4977, Accuracy: 0.7969\n",
      "Batch number : 321, Training: Loss:  0.4485, Accuracy: 0.8594\n",
      "Batch number : 322, Training: Loss:  0.6296, Accuracy: 0.7812\n",
      "Batch number : 323, Training: Loss:  0.4329, Accuracy: 0.8750\n",
      "Batch number : 324, Training: Loss:  0.4917, Accuracy: 0.8438\n",
      "Batch number : 325, Training: Loss:  0.3327, Accuracy: 0.9062\n",
      "Batch number : 326, Training: Loss:  0.4587, Accuracy: 0.8750\n",
      "Batch number : 327, Training: Loss:  0.4310, Accuracy: 0.8906\n",
      "Batch number : 328, Training: Loss:  0.5769, Accuracy: 0.7969\n",
      "Batch number : 329, Training: Loss:  0.3261, Accuracy: 0.9062\n",
      "Batch number : 330, Training: Loss:  0.6651, Accuracy: 0.7969\n",
      "Batch number : 331, Training: Loss:  0.5593, Accuracy: 0.8281\n",
      "Batch number : 332, Training: Loss:  0.5563, Accuracy: 0.8438\n",
      "Batch number : 333, Training: Loss:  0.3043, Accuracy: 0.9219\n",
      "Batch number : 334, Training: Loss:  0.3551, Accuracy: 0.8906\n",
      "Batch number : 335, Training: Loss:  0.4811, Accuracy: 0.8594\n",
      "Batch number : 336, Training: Loss:  0.4078, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.4068, Accuracy: 0.8906\n",
      "Batch number : 338, Training: Loss:  0.4658, Accuracy: 0.8906\n",
      "Batch number : 339, Training: Loss:  0.5215, Accuracy: 0.8125\n",
      "Batch number : 340, Training: Loss:  0.4449, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.4326, Accuracy: 0.8750\n",
      "Batch number : 342, Training: Loss:  0.5615, Accuracy: 0.8438\n",
      "Batch number : 343, Training: Loss:  0.3666, Accuracy: 0.8906\n",
      "Batch number : 344, Training: Loss:  0.3366, Accuracy: 0.8906\n",
      "Batch number : 345, Training: Loss:  0.9329, Accuracy: 0.7188\n",
      "Batch number : 346, Training: Loss:  0.5376, Accuracy: 0.8125\n",
      "Batch number : 347, Training: Loss:  0.5483, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.5432, Accuracy: 0.8438\n",
      "Batch number : 349, Training: Loss:  0.5844, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.4800, Accuracy: 0.8438\n",
      "Batch number : 351, Training: Loss:  0.4561, Accuracy: 0.8906\n",
      "Batch number : 352, Training: Loss:  0.3690, Accuracy: 0.9062\n",
      "Batch number : 353, Training: Loss:  0.4673, Accuracy: 0.8438\n",
      "Batch number : 354, Training: Loss:  0.3319, Accuracy: 0.9062\n",
      "Batch number : 355, Training: Loss:  0.3718, Accuracy: 0.8906\n",
      "Batch number : 356, Training: Loss:  0.3210, Accuracy: 0.9219\n",
      "Batch number : 357, Training: Loss:  0.6564, Accuracy: 0.8438\n",
      "Batch number : 358, Training: Loss:  0.4059, Accuracy: 0.9062\n",
      "Batch number : 359, Training: Loss:  0.5088, Accuracy: 0.8438\n",
      "Batch number : 360, Training: Loss:  0.5019, Accuracy: 0.8594\n",
      "Batch number : 361, Training: Loss:  0.5167, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.5487, Accuracy: 0.8281\n",
      "Batch number : 363, Training: Loss:  0.5367, Accuracy: 0.8281\n",
      "Batch number : 364, Training: Loss:  0.6146, Accuracy: 0.7656\n",
      "Batch number : 365, Training: Loss:  0.3585, Accuracy: 0.9062\n",
      "Batch number : 366, Training: Loss:  0.4371, Accuracy: 0.8438\n",
      "Batch number : 367, Training: Loss:  0.3969, Accuracy: 0.8906\n",
      "Batch number : 368, Training: Loss:  0.6143, Accuracy: 0.7812\n",
      "Batch number : 369, Training: Loss:  0.4404, Accuracy: 0.8594\n",
      "Batch number : 370, Training: Loss:  0.5060, Accuracy: 0.8438\n",
      "Batch number : 371, Training: Loss:  0.4125, Accuracy: 0.8906\n",
      "Batch number : 372, Training: Loss:  0.4815, Accuracy: 0.8281\n",
      "Batch number : 373, Training: Loss:  0.5038, Accuracy: 0.8594\n",
      "Batch number : 374, Training: Loss:  0.4358, Accuracy: 0.8438\n",
      "Batch number : 375, Training: Loss:  0.7183, Accuracy: 0.7344\n",
      "Batch number : 376, Training: Loss:  0.3797, Accuracy: 0.8750\n",
      "Batch number : 377, Training: Loss:  0.3300, Accuracy: 0.9000\n",
      "Epoch: 13/20\n",
      "Batch number : 000, Training: Loss:  0.3787, Accuracy: 0.9062\n",
      "Batch number : 001, Training: Loss:  0.5115, Accuracy: 0.8281\n",
      "Batch number : 002, Training: Loss:  0.3132, Accuracy: 0.9375\n",
      "Batch number : 003, Training: Loss:  0.5024, Accuracy: 0.8438\n",
      "Batch number : 004, Training: Loss:  0.2376, Accuracy: 0.9531\n",
      "Batch number : 005, Training: Loss:  0.4857, Accuracy: 0.7969\n",
      "Batch number : 006, Training: Loss:  0.2672, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.5591, Accuracy: 0.8594\n",
      "Batch number : 008, Training: Loss:  0.5487, Accuracy: 0.9219\n",
      "Batch number : 009, Training: Loss:  0.4332, Accuracy: 0.8594\n",
      "Batch number : 010, Training: Loss:  0.5556, Accuracy: 0.7969\n",
      "Batch number : 011, Training: Loss:  0.4178, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.4866, Accuracy: 0.8594\n",
      "Batch number : 013, Training: Loss:  0.6276, Accuracy: 0.8438\n",
      "Batch number : 014, Training: Loss:  0.4704, Accuracy: 0.8438\n",
      "Batch number : 015, Training: Loss:  0.6210, Accuracy: 0.8281\n",
      "Batch number : 016, Training: Loss:  0.4108, Accuracy: 0.9062\n",
      "Batch number : 017, Training: Loss:  0.4360, Accuracy: 0.9062\n",
      "Batch number : 018, Training: Loss:  0.3829, Accuracy: 0.9219\n",
      "Batch number : 019, Training: Loss:  0.3821, Accuracy: 0.9062\n",
      "Batch number : 020, Training: Loss:  0.4585, Accuracy: 0.8750\n",
      "Batch number : 021, Training: Loss:  0.3692, Accuracy: 0.8906\n",
      "Batch number : 022, Training: Loss:  0.4766, Accuracy: 0.8906\n",
      "Batch number : 023, Training: Loss:  0.3219, Accuracy: 0.9062\n",
      "Batch number : 024, Training: Loss:  0.5015, Accuracy: 0.8125\n",
      "Batch number : 025, Training: Loss:  0.4875, Accuracy: 0.8438\n",
      "Batch number : 026, Training: Loss:  0.5039, Accuracy: 0.8594\n",
      "Batch number : 027, Training: Loss:  0.6489, Accuracy: 0.8125\n",
      "Batch number : 028, Training: Loss:  0.6656, Accuracy: 0.8281\n",
      "Batch number : 029, Training: Loss:  0.3268, Accuracy: 0.8906\n",
      "Batch number : 030, Training: Loss:  0.3588, Accuracy: 0.9062\n",
      "Batch number : 031, Training: Loss:  0.4708, Accuracy: 0.8594\n",
      "Batch number : 032, Training: Loss:  0.6185, Accuracy: 0.8281\n",
      "Batch number : 033, Training: Loss:  0.5780, Accuracy: 0.8281\n",
      "Batch number : 034, Training: Loss:  0.5395, Accuracy: 0.8750\n",
      "Batch number : 035, Training: Loss:  0.4976, Accuracy: 0.8750\n",
      "Batch number : 036, Training: Loss:  0.5677, Accuracy: 0.7969\n",
      "Batch number : 037, Training: Loss:  0.4532, Accuracy: 0.8594\n",
      "Batch number : 038, Training: Loss:  0.5280, Accuracy: 0.8438\n",
      "Batch number : 039, Training: Loss:  0.5234, Accuracy: 0.8750\n",
      "Batch number : 040, Training: Loss:  0.4212, Accuracy: 0.9219\n",
      "Batch number : 041, Training: Loss:  0.6145, Accuracy: 0.7969\n",
      "Batch number : 042, Training: Loss:  0.5069, Accuracy: 0.8125\n",
      "Batch number : 043, Training: Loss:  0.3752, Accuracy: 0.9062\n",
      "Batch number : 044, Training: Loss:  0.3257, Accuracy: 0.9062\n",
      "Batch number : 045, Training: Loss:  0.2273, Accuracy: 0.9531\n",
      "Batch number : 046, Training: Loss:  0.2861, Accuracy: 0.9062\n",
      "Batch number : 047, Training: Loss:  0.4768, Accuracy: 0.8438\n",
      "Batch number : 048, Training: Loss:  0.8816, Accuracy: 0.7344\n",
      "Batch number : 049, Training: Loss:  0.7134, Accuracy: 0.7969\n",
      "Batch number : 050, Training: Loss:  0.7501, Accuracy: 0.7812\n",
      "Batch number : 051, Training: Loss:  0.6357, Accuracy: 0.7969\n",
      "Batch number : 052, Training: Loss:  0.5402, Accuracy: 0.8594\n",
      "Batch number : 053, Training: Loss:  0.4907, Accuracy: 0.8594\n",
      "Batch number : 054, Training: Loss:  0.6408, Accuracy: 0.8438\n",
      "Batch number : 055, Training: Loss:  0.5907, Accuracy: 0.8281\n",
      "Batch number : 056, Training: Loss:  0.4433, Accuracy: 0.8906\n",
      "Batch number : 057, Training: Loss:  0.5255, Accuracy: 0.8438\n",
      "Batch number : 058, Training: Loss:  0.4729, Accuracy: 0.8750\n",
      "Batch number : 059, Training: Loss:  0.3811, Accuracy: 0.9375\n",
      "Batch number : 060, Training: Loss:  0.4284, Accuracy: 0.8906\n",
      "Batch number : 061, Training: Loss:  0.3828, Accuracy: 0.8906\n",
      "Batch number : 062, Training: Loss:  0.7999, Accuracy: 0.7500\n",
      "Batch number : 063, Training: Loss:  0.6674, Accuracy: 0.8438\n",
      "Batch number : 064, Training: Loss:  0.3880, Accuracy: 0.8906\n",
      "Batch number : 065, Training: Loss:  0.6464, Accuracy: 0.7969\n",
      "Batch number : 066, Training: Loss:  0.6574, Accuracy: 0.8281\n",
      "Batch number : 067, Training: Loss:  0.2951, Accuracy: 0.9062\n",
      "Batch number : 068, Training: Loss:  0.5811, Accuracy: 0.8281\n",
      "Batch number : 069, Training: Loss:  0.4303, Accuracy: 0.8906\n",
      "Batch number : 070, Training: Loss:  0.6104, Accuracy: 0.8125\n",
      "Batch number : 071, Training: Loss:  0.4397, Accuracy: 0.8750\n",
      "Batch number : 072, Training: Loss:  0.4391, Accuracy: 0.8438\n",
      "Batch number : 073, Training: Loss:  0.3640, Accuracy: 0.9062\n",
      "Batch number : 074, Training: Loss:  0.4374, Accuracy: 0.8438\n",
      "Batch number : 075, Training: Loss:  0.5397, Accuracy: 0.8750\n",
      "Batch number : 076, Training: Loss:  0.3770, Accuracy: 0.8594\n",
      "Batch number : 077, Training: Loss:  0.3356, Accuracy: 0.9062\n",
      "Batch number : 078, Training: Loss:  0.4607, Accuracy: 0.8750\n",
      "Batch number : 079, Training: Loss:  0.4705, Accuracy: 0.8438\n",
      "Batch number : 080, Training: Loss:  0.6567, Accuracy: 0.7656\n",
      "Batch number : 081, Training: Loss:  0.7188, Accuracy: 0.7656\n",
      "Batch number : 082, Training: Loss:  0.4536, Accuracy: 0.8750\n",
      "Batch number : 083, Training: Loss:  0.4045, Accuracy: 0.8750\n",
      "Batch number : 084, Training: Loss:  0.4658, Accuracy: 0.8438\n",
      "Batch number : 085, Training: Loss:  0.6311, Accuracy: 0.8281\n",
      "Batch number : 086, Training: Loss:  0.3991, Accuracy: 0.8750\n",
      "Batch number : 087, Training: Loss:  0.5403, Accuracy: 0.8281\n",
      "Batch number : 088, Training: Loss:  0.4632, Accuracy: 0.8750\n",
      "Batch number : 089, Training: Loss:  0.7515, Accuracy: 0.7500\n",
      "Batch number : 090, Training: Loss:  0.5420, Accuracy: 0.7969\n",
      "Batch number : 091, Training: Loss:  0.4939, Accuracy: 0.8750\n",
      "Batch number : 092, Training: Loss:  0.5406, Accuracy: 0.8125\n",
      "Batch number : 093, Training: Loss:  0.3589, Accuracy: 0.9219\n",
      "Batch number : 094, Training: Loss:  0.4321, Accuracy: 0.8906\n",
      "Batch number : 095, Training: Loss:  0.4037, Accuracy: 0.8750\n",
      "Batch number : 096, Training: Loss:  0.4222, Accuracy: 0.8594\n",
      "Batch number : 097, Training: Loss:  0.5186, Accuracy: 0.8594\n",
      "Batch number : 098, Training: Loss:  0.4608, Accuracy: 0.8594\n",
      "Batch number : 099, Training: Loss:  0.5214, Accuracy: 0.8438\n",
      "Batch number : 100, Training: Loss:  0.6991, Accuracy: 0.8125\n",
      "Batch number : 101, Training: Loss:  0.4775, Accuracy: 0.8594\n",
      "Batch number : 102, Training: Loss:  0.5645, Accuracy: 0.8281\n",
      "Batch number : 103, Training: Loss:  0.5304, Accuracy: 0.8125\n",
      "Batch number : 104, Training: Loss:  0.5953, Accuracy: 0.8281\n",
      "Batch number : 105, Training: Loss:  0.4568, Accuracy: 0.8594\n",
      "Batch number : 106, Training: Loss:  0.3810, Accuracy: 0.8750\n",
      "Batch number : 107, Training: Loss:  0.3891, Accuracy: 0.9062\n",
      "Batch number : 108, Training: Loss:  0.4681, Accuracy: 0.8125\n",
      "Batch number : 109, Training: Loss:  0.2957, Accuracy: 0.9062\n",
      "Batch number : 110, Training: Loss:  0.3678, Accuracy: 0.8750\n",
      "Batch number : 111, Training: Loss:  0.4804, Accuracy: 0.8750\n",
      "Batch number : 112, Training: Loss:  0.6448, Accuracy: 0.8281\n",
      "Batch number : 113, Training: Loss:  0.5686, Accuracy: 0.8438\n",
      "Batch number : 114, Training: Loss:  0.5297, Accuracy: 0.8438\n",
      "Batch number : 115, Training: Loss:  0.6517, Accuracy: 0.8125\n",
      "Batch number : 116, Training: Loss:  0.3748, Accuracy: 0.8906\n",
      "Batch number : 117, Training: Loss:  0.3628, Accuracy: 0.8906\n",
      "Batch number : 118, Training: Loss:  0.3660, Accuracy: 0.9062\n",
      "Batch number : 119, Training: Loss:  0.4313, Accuracy: 0.8750\n",
      "Batch number : 120, Training: Loss:  0.5335, Accuracy: 0.8281\n",
      "Batch number : 121, Training: Loss:  0.4921, Accuracy: 0.8594\n",
      "Batch number : 122, Training: Loss:  0.5867, Accuracy: 0.8281\n",
      "Batch number : 123, Training: Loss:  0.3335, Accuracy: 0.9062\n",
      "Batch number : 124, Training: Loss:  0.6372, Accuracy: 0.8281\n",
      "Batch number : 125, Training: Loss:  0.3659, Accuracy: 0.8750\n",
      "Batch number : 126, Training: Loss:  0.3591, Accuracy: 0.8438\n",
      "Batch number : 127, Training: Loss:  0.5421, Accuracy: 0.8125\n",
      "Batch number : 128, Training: Loss:  0.6255, Accuracy: 0.8281\n",
      "Batch number : 129, Training: Loss:  0.3550, Accuracy: 0.9219\n",
      "Batch number : 130, Training: Loss:  0.4583, Accuracy: 0.8594\n",
      "Batch number : 131, Training: Loss:  0.3846, Accuracy: 0.8906\n",
      "Batch number : 132, Training: Loss:  0.4174, Accuracy: 0.8906\n",
      "Batch number : 133, Training: Loss:  0.4710, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.2889, Accuracy: 0.9219\n",
      "Batch number : 135, Training: Loss:  0.5419, Accuracy: 0.8281\n",
      "Batch number : 136, Training: Loss:  0.3670, Accuracy: 0.9062\n",
      "Batch number : 137, Training: Loss:  0.4556, Accuracy: 0.8750\n",
      "Batch number : 138, Training: Loss:  0.3956, Accuracy: 0.8906\n",
      "Batch number : 139, Training: Loss:  0.5248, Accuracy: 0.8438\n",
      "Batch number : 140, Training: Loss:  0.4122, Accuracy: 0.8438\n",
      "Batch number : 141, Training: Loss:  0.3977, Accuracy: 0.8750\n",
      "Batch number : 142, Training: Loss:  0.4666, Accuracy: 0.8438\n",
      "Batch number : 143, Training: Loss:  0.4159, Accuracy: 0.8750\n",
      "Batch number : 144, Training: Loss:  0.4760, Accuracy: 0.8438\n",
      "Batch number : 145, Training: Loss:  0.3409, Accuracy: 0.8906\n",
      "Batch number : 146, Training: Loss:  0.6389, Accuracy: 0.8125\n",
      "Batch number : 147, Training: Loss:  0.4775, Accuracy: 0.8438\n",
      "Batch number : 148, Training: Loss:  0.5397, Accuracy: 0.7969\n",
      "Batch number : 149, Training: Loss:  0.5713, Accuracy: 0.8125\n",
      "Batch number : 150, Training: Loss:  0.5579, Accuracy: 0.7969\n",
      "Batch number : 151, Training: Loss:  0.4477, Accuracy: 0.8906\n",
      "Batch number : 152, Training: Loss:  0.4991, Accuracy: 0.8281\n",
      "Batch number : 153, Training: Loss:  0.3896, Accuracy: 0.8750\n",
      "Batch number : 154, Training: Loss:  0.5752, Accuracy: 0.7656\n",
      "Batch number : 155, Training: Loss:  0.5101, Accuracy: 0.9062\n",
      "Batch number : 156, Training: Loss:  0.4438, Accuracy: 0.8438\n",
      "Batch number : 157, Training: Loss:  0.5680, Accuracy: 0.7969\n",
      "Batch number : 158, Training: Loss:  0.3714, Accuracy: 0.9219\n",
      "Batch number : 159, Training: Loss:  0.3850, Accuracy: 0.8750\n",
      "Batch number : 160, Training: Loss:  0.7447, Accuracy: 0.8125\n",
      "Batch number : 161, Training: Loss:  0.6601, Accuracy: 0.7812\n",
      "Batch number : 162, Training: Loss:  0.4153, Accuracy: 0.8594\n",
      "Batch number : 163, Training: Loss:  0.6770, Accuracy: 0.8125\n",
      "Batch number : 164, Training: Loss:  0.6470, Accuracy: 0.7812\n",
      "Batch number : 165, Training: Loss:  0.3232, Accuracy: 0.9219\n",
      "Batch number : 166, Training: Loss:  0.3859, Accuracy: 0.9062\n",
      "Batch number : 167, Training: Loss:  0.4129, Accuracy: 0.8438\n",
      "Batch number : 168, Training: Loss:  0.6166, Accuracy: 0.7969\n",
      "Batch number : 169, Training: Loss:  0.2738, Accuracy: 0.9219\n",
      "Batch number : 170, Training: Loss:  0.3971, Accuracy: 0.8750\n",
      "Batch number : 171, Training: Loss:  0.3766, Accuracy: 0.9062\n",
      "Batch number : 172, Training: Loss:  0.5357, Accuracy: 0.8438\n",
      "Batch number : 173, Training: Loss:  0.5279, Accuracy: 0.8281\n",
      "Batch number : 174, Training: Loss:  0.5033, Accuracy: 0.8594\n",
      "Batch number : 175, Training: Loss:  0.3491, Accuracy: 0.9062\n",
      "Batch number : 176, Training: Loss:  0.2705, Accuracy: 0.9219\n",
      "Batch number : 177, Training: Loss:  0.4465, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.3548, Accuracy: 0.9062\n",
      "Batch number : 179, Training: Loss:  0.3589, Accuracy: 0.8906\n",
      "Batch number : 180, Training: Loss:  0.5806, Accuracy: 0.8125\n",
      "Batch number : 181, Training: Loss:  0.2958, Accuracy: 0.9531\n",
      "Batch number : 182, Training: Loss:  0.6209, Accuracy: 0.7969\n",
      "Batch number : 183, Training: Loss:  0.5655, Accuracy: 0.8281\n",
      "Batch number : 184, Training: Loss:  0.4446, Accuracy: 0.8594\n",
      "Batch number : 185, Training: Loss:  0.4450, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.4357, Accuracy: 0.8594\n",
      "Batch number : 187, Training: Loss:  0.4701, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.5893, Accuracy: 0.8438\n",
      "Batch number : 189, Training: Loss:  0.3337, Accuracy: 0.9062\n",
      "Batch number : 190, Training: Loss:  0.4382, Accuracy: 0.8750\n",
      "Batch number : 191, Training: Loss:  0.4662, Accuracy: 0.8594\n",
      "Batch number : 192, Training: Loss:  0.6295, Accuracy: 0.7812\n",
      "Batch number : 193, Training: Loss:  0.2862, Accuracy: 0.9219\n",
      "Batch number : 194, Training: Loss:  0.4974, Accuracy: 0.8906\n",
      "Batch number : 195, Training: Loss:  0.7258, Accuracy: 0.7656\n",
      "Batch number : 196, Training: Loss:  0.5658, Accuracy: 0.8594\n",
      "Batch number : 197, Training: Loss:  0.2393, Accuracy: 0.9375\n",
      "Batch number : 198, Training: Loss:  0.3736, Accuracy: 0.8594\n",
      "Batch number : 199, Training: Loss:  0.4947, Accuracy: 0.8438\n",
      "Batch number : 200, Training: Loss:  0.6398, Accuracy: 0.7812\n",
      "Batch number : 201, Training: Loss:  0.6596, Accuracy: 0.7656\n",
      "Batch number : 202, Training: Loss:  0.4952, Accuracy: 0.8125\n",
      "Batch number : 203, Training: Loss:  0.4386, Accuracy: 0.9062\n",
      "Batch number : 204, Training: Loss:  0.5402, Accuracy: 0.8125\n",
      "Batch number : 205, Training: Loss:  0.4491, Accuracy: 0.8594\n",
      "Batch number : 206, Training: Loss:  0.4495, Accuracy: 0.8750\n",
      "Batch number : 207, Training: Loss:  0.5347, Accuracy: 0.8594\n",
      "Batch number : 208, Training: Loss:  0.6114, Accuracy: 0.8125\n",
      "Batch number : 209, Training: Loss:  0.5033, Accuracy: 0.8594\n",
      "Batch number : 210, Training: Loss:  0.4486, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.3923, Accuracy: 0.9062\n",
      "Batch number : 212, Training: Loss:  0.3787, Accuracy: 0.8906\n",
      "Batch number : 213, Training: Loss:  0.3415, Accuracy: 0.9062\n",
      "Batch number : 214, Training: Loss:  0.5753, Accuracy: 0.8125\n",
      "Batch number : 215, Training: Loss:  0.7826, Accuracy: 0.8125\n",
      "Batch number : 216, Training: Loss:  0.3839, Accuracy: 0.8750\n",
      "Batch number : 217, Training: Loss:  0.5008, Accuracy: 0.8594\n",
      "Batch number : 218, Training: Loss:  0.3042, Accuracy: 0.9219\n",
      "Batch number : 219, Training: Loss:  0.6826, Accuracy: 0.7969\n",
      "Batch number : 220, Training: Loss:  0.6681, Accuracy: 0.8125\n",
      "Batch number : 221, Training: Loss:  0.4845, Accuracy: 0.8594\n",
      "Batch number : 222, Training: Loss:  0.8182, Accuracy: 0.7500\n",
      "Batch number : 223, Training: Loss:  0.4298, Accuracy: 0.8750\n",
      "Batch number : 224, Training: Loss:  0.5636, Accuracy: 0.7969\n",
      "Batch number : 225, Training: Loss:  0.4832, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.6072, Accuracy: 0.8906\n",
      "Batch number : 227, Training: Loss:  0.5378, Accuracy: 0.8438\n",
      "Batch number : 228, Training: Loss:  0.3773, Accuracy: 0.8750\n",
      "Batch number : 229, Training: Loss:  0.5219, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.4465, Accuracy: 0.8750\n",
      "Batch number : 231, Training: Loss:  0.4384, Accuracy: 0.8281\n",
      "Batch number : 232, Training: Loss:  0.3707, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.5084, Accuracy: 0.8906\n",
      "Batch number : 234, Training: Loss:  0.3679, Accuracy: 0.9219\n",
      "Batch number : 235, Training: Loss:  0.5933, Accuracy: 0.7969\n",
      "Batch number : 236, Training: Loss:  0.5930, Accuracy: 0.8750\n",
      "Batch number : 237, Training: Loss:  0.6894, Accuracy: 0.8281\n",
      "Batch number : 238, Training: Loss:  0.3928, Accuracy: 0.8750\n",
      "Batch number : 239, Training: Loss:  0.3650, Accuracy: 0.8750\n",
      "Batch number : 240, Training: Loss:  0.3722, Accuracy: 0.8906\n",
      "Batch number : 241, Training: Loss:  0.4793, Accuracy: 0.8281\n",
      "Batch number : 242, Training: Loss:  0.4550, Accuracy: 0.8594\n",
      "Batch number : 243, Training: Loss:  0.4354, Accuracy: 0.9219\n",
      "Batch number : 244, Training: Loss:  0.5212, Accuracy: 0.8125\n",
      "Batch number : 245, Training: Loss:  0.3950, Accuracy: 0.8750\n",
      "Batch number : 246, Training: Loss:  0.5134, Accuracy: 0.7969\n",
      "Batch number : 247, Training: Loss:  0.3523, Accuracy: 0.8906\n",
      "Batch number : 248, Training: Loss:  0.4015, Accuracy: 0.8906\n",
      "Batch number : 249, Training: Loss:  0.4906, Accuracy: 0.8906\n",
      "Batch number : 250, Training: Loss:  0.4353, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.5403, Accuracy: 0.8438\n",
      "Batch number : 252, Training: Loss:  0.4126, Accuracy: 0.8594\n",
      "Batch number : 253, Training: Loss:  0.4547, Accuracy: 0.8906\n",
      "Batch number : 254, Training: Loss:  0.4782, Accuracy: 0.8594\n",
      "Batch number : 255, Training: Loss:  0.5557, Accuracy: 0.8281\n",
      "Batch number : 256, Training: Loss:  0.3524, Accuracy: 0.9219\n",
      "Batch number : 257, Training: Loss:  0.5530, Accuracy: 0.7812\n",
      "Batch number : 258, Training: Loss:  0.3874, Accuracy: 0.9062\n",
      "Batch number : 259, Training: Loss:  0.3375, Accuracy: 0.9219\n",
      "Batch number : 260, Training: Loss:  0.5558, Accuracy: 0.8281\n",
      "Batch number : 261, Training: Loss:  0.5981, Accuracy: 0.7812\n",
      "Batch number : 262, Training: Loss:  0.4917, Accuracy: 0.8438\n",
      "Batch number : 263, Training: Loss:  0.3806, Accuracy: 0.9062\n",
      "Batch number : 264, Training: Loss:  0.3213, Accuracy: 0.9062\n",
      "Batch number : 265, Training: Loss:  0.4906, Accuracy: 0.8906\n",
      "Batch number : 266, Training: Loss:  0.5225, Accuracy: 0.8281\n",
      "Batch number : 267, Training: Loss:  0.4628, Accuracy: 0.8125\n",
      "Batch number : 268, Training: Loss:  0.3826, Accuracy: 0.9219\n",
      "Batch number : 269, Training: Loss:  0.6839, Accuracy: 0.8281\n",
      "Batch number : 270, Training: Loss:  0.6390, Accuracy: 0.8125\n",
      "Batch number : 271, Training: Loss:  0.3402, Accuracy: 0.9062\n",
      "Batch number : 272, Training: Loss:  0.3831, Accuracy: 0.8906\n",
      "Batch number : 273, Training: Loss:  0.4799, Accuracy: 0.8281\n",
      "Batch number : 274, Training: Loss:  0.3980, Accuracy: 0.8594\n",
      "Batch number : 275, Training: Loss:  0.4133, Accuracy: 0.8594\n",
      "Batch number : 276, Training: Loss:  0.3802, Accuracy: 0.8906\n",
      "Batch number : 277, Training: Loss:  0.6133, Accuracy: 0.8125\n",
      "Batch number : 278, Training: Loss:  0.7789, Accuracy: 0.7969\n",
      "Batch number : 279, Training: Loss:  0.4244, Accuracy: 0.8750\n",
      "Batch number : 280, Training: Loss:  0.4639, Accuracy: 0.8594\n",
      "Batch number : 281, Training: Loss:  0.4862, Accuracy: 0.8438\n",
      "Batch number : 282, Training: Loss:  0.3821, Accuracy: 0.9219\n",
      "Batch number : 283, Training: Loss:  0.3759, Accuracy: 0.9219\n",
      "Batch number : 284, Training: Loss:  0.3769, Accuracy: 0.9062\n",
      "Batch number : 285, Training: Loss:  0.6374, Accuracy: 0.8281\n",
      "Batch number : 286, Training: Loss:  0.5229, Accuracy: 0.8125\n",
      "Batch number : 287, Training: Loss:  0.5099, Accuracy: 0.8281\n",
      "Batch number : 288, Training: Loss:  0.5443, Accuracy: 0.8281\n",
      "Batch number : 289, Training: Loss:  0.3116, Accuracy: 0.9219\n",
      "Batch number : 290, Training: Loss:  0.4987, Accuracy: 0.8281\n",
      "Batch number : 291, Training: Loss:  0.3767, Accuracy: 0.8750\n",
      "Batch number : 292, Training: Loss:  0.5591, Accuracy: 0.8281\n",
      "Batch number : 293, Training: Loss:  0.2967, Accuracy: 0.9219\n",
      "Batch number : 294, Training: Loss:  0.4988, Accuracy: 0.9062\n",
      "Batch number : 295, Training: Loss:  0.5407, Accuracy: 0.8438\n",
      "Batch number : 296, Training: Loss:  0.2828, Accuracy: 0.9375\n",
      "Batch number : 297, Training: Loss:  0.8450, Accuracy: 0.7656\n",
      "Batch number : 298, Training: Loss:  0.5406, Accuracy: 0.8438\n",
      "Batch number : 299, Training: Loss:  0.3547, Accuracy: 0.9062\n",
      "Batch number : 300, Training: Loss:  0.6709, Accuracy: 0.7969\n",
      "Batch number : 301, Training: Loss:  0.4698, Accuracy: 0.9062\n",
      "Batch number : 302, Training: Loss:  0.4432, Accuracy: 0.8594\n",
      "Batch number : 303, Training: Loss:  0.4047, Accuracy: 0.8750\n",
      "Batch number : 304, Training: Loss:  0.2476, Accuracy: 0.9688\n",
      "Batch number : 305, Training: Loss:  0.5823, Accuracy: 0.8594\n",
      "Batch number : 306, Training: Loss:  0.3422, Accuracy: 0.9062\n",
      "Batch number : 307, Training: Loss:  0.6000, Accuracy: 0.8281\n",
      "Batch number : 308, Training: Loss:  0.2945, Accuracy: 0.9062\n",
      "Batch number : 309, Training: Loss:  0.5193, Accuracy: 0.8438\n",
      "Batch number : 310, Training: Loss:  0.4630, Accuracy: 0.8750\n",
      "Batch number : 311, Training: Loss:  0.6164, Accuracy: 0.8281\n",
      "Batch number : 312, Training: Loss:  0.5940, Accuracy: 0.8281\n",
      "Batch number : 313, Training: Loss:  0.4406, Accuracy: 0.8594\n",
      "Batch number : 314, Training: Loss:  0.4610, Accuracy: 0.8750\n",
      "Batch number : 315, Training: Loss:  0.3923, Accuracy: 0.9219\n",
      "Batch number : 316, Training: Loss:  0.4912, Accuracy: 0.8594\n",
      "Batch number : 317, Training: Loss:  0.4042, Accuracy: 0.8906\n",
      "Batch number : 318, Training: Loss:  0.6138, Accuracy: 0.8125\n",
      "Batch number : 319, Training: Loss:  0.4677, Accuracy: 0.8594\n",
      "Batch number : 320, Training: Loss:  0.4732, Accuracy: 0.8750\n",
      "Batch number : 321, Training: Loss:  0.4786, Accuracy: 0.8438\n",
      "Batch number : 322, Training: Loss:  0.5409, Accuracy: 0.8125\n",
      "Batch number : 323, Training: Loss:  0.6469, Accuracy: 0.7656\n",
      "Batch number : 324, Training: Loss:  0.4872, Accuracy: 0.8281\n",
      "Batch number : 325, Training: Loss:  0.5422, Accuracy: 0.8750\n",
      "Batch number : 326, Training: Loss:  0.4754, Accuracy: 0.8438\n",
      "Batch number : 327, Training: Loss:  0.8405, Accuracy: 0.7812\n",
      "Batch number : 328, Training: Loss:  0.6697, Accuracy: 0.8125\n",
      "Batch number : 329, Training: Loss:  0.5802, Accuracy: 0.7969\n",
      "Batch number : 330, Training: Loss:  0.4486, Accuracy: 0.8906\n",
      "Batch number : 331, Training: Loss:  0.5329, Accuracy: 0.8438\n",
      "Batch number : 332, Training: Loss:  0.5161, Accuracy: 0.8125\n",
      "Batch number : 333, Training: Loss:  0.5018, Accuracy: 0.8281\n",
      "Batch number : 334, Training: Loss:  0.4270, Accuracy: 0.8750\n",
      "Batch number : 335, Training: Loss:  0.4421, Accuracy: 0.8750\n",
      "Batch number : 336, Training: Loss:  0.4361, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.3920, Accuracy: 0.8750\n",
      "Batch number : 338, Training: Loss:  0.5344, Accuracy: 0.8281\n",
      "Batch number : 339, Training: Loss:  0.4706, Accuracy: 0.8438\n",
      "Batch number : 340, Training: Loss:  0.2813, Accuracy: 0.9062\n",
      "Batch number : 341, Training: Loss:  0.4077, Accuracy: 0.8906\n",
      "Batch number : 342, Training: Loss:  0.3414, Accuracy: 0.9375\n",
      "Batch number : 343, Training: Loss:  0.4672, Accuracy: 0.8594\n",
      "Batch number : 344, Training: Loss:  0.2788, Accuracy: 0.9219\n",
      "Batch number : 345, Training: Loss:  0.8754, Accuracy: 0.7500\n",
      "Batch number : 346, Training: Loss:  0.3010, Accuracy: 0.8906\n",
      "Batch number : 347, Training: Loss:  0.2209, Accuracy: 0.9375\n",
      "Batch number : 348, Training: Loss:  0.4572, Accuracy: 0.8594\n",
      "Batch number : 349, Training: Loss:  0.5222, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.5354, Accuracy: 0.8125\n",
      "Batch number : 351, Training: Loss:  0.4838, Accuracy: 0.8281\n",
      "Batch number : 352, Training: Loss:  0.5896, Accuracy: 0.7969\n",
      "Batch number : 353, Training: Loss:  0.4694, Accuracy: 0.8594\n",
      "Batch number : 354, Training: Loss:  0.5185, Accuracy: 0.8594\n",
      "Batch number : 355, Training: Loss:  0.4324, Accuracy: 0.8438\n",
      "Batch number : 356, Training: Loss:  0.4357, Accuracy: 0.8750\n",
      "Batch number : 357, Training: Loss:  0.5358, Accuracy: 0.8125\n",
      "Batch number : 358, Training: Loss:  0.4757, Accuracy: 0.8750\n",
      "Batch number : 359, Training: Loss:  0.4438, Accuracy: 0.8906\n",
      "Batch number : 360, Training: Loss:  0.5611, Accuracy: 0.8438\n",
      "Batch number : 361, Training: Loss:  0.4456, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.4612, Accuracy: 0.8438\n",
      "Batch number : 363, Training: Loss:  0.4388, Accuracy: 0.8594\n",
      "Batch number : 364, Training: Loss:  0.3298, Accuracy: 0.9062\n",
      "Batch number : 365, Training: Loss:  0.5701, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.4321, Accuracy: 0.8594\n",
      "Batch number : 367, Training: Loss:  0.5028, Accuracy: 0.8594\n",
      "Batch number : 368, Training: Loss:  0.3971, Accuracy: 0.8750\n",
      "Batch number : 369, Training: Loss:  0.3465, Accuracy: 0.9062\n",
      "Batch number : 370, Training: Loss:  0.4395, Accuracy: 0.8906\n",
      "Batch number : 371, Training: Loss:  0.5207, Accuracy: 0.8438\n",
      "Batch number : 372, Training: Loss:  0.3533, Accuracy: 0.9062\n",
      "Batch number : 373, Training: Loss:  0.4222, Accuracy: 0.8594\n",
      "Batch number : 374, Training: Loss:  0.4124, Accuracy: 0.8906\n",
      "Batch number : 375, Training: Loss:  0.5229, Accuracy: 0.8438\n",
      "Batch number : 376, Training: Loss:  0.3601, Accuracy: 0.9062\n",
      "Batch number : 377, Training: Loss:  0.4228, Accuracy: 0.8750\n",
      "Epoch: 14/20\n",
      "Batch number : 000, Training: Loss:  0.5025, Accuracy: 0.8438\n",
      "Batch number : 001, Training: Loss:  0.5029, Accuracy: 0.8281\n",
      "Batch number : 002, Training: Loss:  0.3422, Accuracy: 0.9375\n",
      "Batch number : 003, Training: Loss:  0.2307, Accuracy: 0.9375\n",
      "Batch number : 004, Training: Loss:  0.4097, Accuracy: 0.8906\n",
      "Batch number : 005, Training: Loss:  0.2890, Accuracy: 0.9219\n",
      "Batch number : 006, Training: Loss:  0.4891, Accuracy: 0.8281\n",
      "Batch number : 007, Training: Loss:  0.5809, Accuracy: 0.8281\n",
      "Batch number : 008, Training: Loss:  0.5315, Accuracy: 0.8281\n",
      "Batch number : 009, Training: Loss:  0.5900, Accuracy: 0.7969\n",
      "Batch number : 010, Training: Loss:  0.4629, Accuracy: 0.8281\n",
      "Batch number : 011, Training: Loss:  0.4442, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.5282, Accuracy: 0.8594\n",
      "Batch number : 013, Training: Loss:  0.3498, Accuracy: 0.9062\n",
      "Batch number : 014, Training: Loss:  0.6173, Accuracy: 0.7812\n",
      "Batch number : 015, Training: Loss:  0.3070, Accuracy: 0.9531\n",
      "Batch number : 016, Training: Loss:  0.4805, Accuracy: 0.8906\n",
      "Batch number : 017, Training: Loss:  0.3346, Accuracy: 0.9219\n",
      "Batch number : 018, Training: Loss:  0.6266, Accuracy: 0.8281\n",
      "Batch number : 019, Training: Loss:  0.4697, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.3432, Accuracy: 0.9062\n",
      "Batch number : 021, Training: Loss:  0.5592, Accuracy: 0.8125\n",
      "Batch number : 022, Training: Loss:  0.7927, Accuracy: 0.7969\n",
      "Batch number : 023, Training: Loss:  0.6135, Accuracy: 0.7656\n",
      "Batch number : 024, Training: Loss:  0.3495, Accuracy: 0.8906\n",
      "Batch number : 025, Training: Loss:  0.5683, Accuracy: 0.8125\n",
      "Batch number : 026, Training: Loss:  0.5447, Accuracy: 0.8438\n",
      "Batch number : 027, Training: Loss:  0.5591, Accuracy: 0.8281\n",
      "Batch number : 028, Training: Loss:  0.3031, Accuracy: 0.9062\n",
      "Batch number : 029, Training: Loss:  0.4518, Accuracy: 0.8125\n",
      "Batch number : 030, Training: Loss:  0.3777, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.6137, Accuracy: 0.8125\n",
      "Batch number : 032, Training: Loss:  0.6133, Accuracy: 0.8281\n",
      "Batch number : 033, Training: Loss:  0.5531, Accuracy: 0.8125\n",
      "Batch number : 034, Training: Loss:  0.2925, Accuracy: 0.9375\n",
      "Batch number : 035, Training: Loss:  0.4812, Accuracy: 0.8750\n",
      "Batch number : 036, Training: Loss:  0.2690, Accuracy: 0.9531\n",
      "Batch number : 037, Training: Loss:  0.6219, Accuracy: 0.7812\n",
      "Batch number : 038, Training: Loss:  0.6244, Accuracy: 0.8125\n",
      "Batch number : 039, Training: Loss:  0.4839, Accuracy: 0.8594\n",
      "Batch number : 040, Training: Loss:  0.7133, Accuracy: 0.7812\n",
      "Batch number : 041, Training: Loss:  0.2987, Accuracy: 0.9062\n",
      "Batch number : 042, Training: Loss:  0.4888, Accuracy: 0.8594\n",
      "Batch number : 043, Training: Loss:  0.5300, Accuracy: 0.8281\n",
      "Batch number : 044, Training: Loss:  0.5570, Accuracy: 0.8281\n",
      "Batch number : 045, Training: Loss:  0.5586, Accuracy: 0.8125\n",
      "Batch number : 046, Training: Loss:  0.3510, Accuracy: 0.9062\n",
      "Batch number : 047, Training: Loss:  0.3556, Accuracy: 0.9375\n",
      "Batch number : 048, Training: Loss:  0.3670, Accuracy: 0.8906\n",
      "Batch number : 049, Training: Loss:  0.5472, Accuracy: 0.8281\n",
      "Batch number : 050, Training: Loss:  0.4135, Accuracy: 0.8594\n",
      "Batch number : 051, Training: Loss:  0.4177, Accuracy: 0.9062\n",
      "Batch number : 052, Training: Loss:  0.3522, Accuracy: 0.9062\n",
      "Batch number : 053, Training: Loss:  0.4271, Accuracy: 0.8438\n",
      "Batch number : 054, Training: Loss:  0.4494, Accuracy: 0.8594\n",
      "Batch number : 055, Training: Loss:  0.8885, Accuracy: 0.7656\n",
      "Batch number : 056, Training: Loss:  0.3905, Accuracy: 0.8750\n",
      "Batch number : 057, Training: Loss:  0.6505, Accuracy: 0.7969\n",
      "Batch number : 058, Training: Loss:  0.2535, Accuracy: 0.9531\n",
      "Batch number : 059, Training: Loss:  0.5629, Accuracy: 0.7969\n",
      "Batch number : 060, Training: Loss:  0.2792, Accuracy: 0.9375\n",
      "Batch number : 061, Training: Loss:  0.4163, Accuracy: 0.8594\n",
      "Batch number : 062, Training: Loss:  0.4661, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.2570, Accuracy: 0.9062\n",
      "Batch number : 064, Training: Loss:  0.4146, Accuracy: 0.8438\n",
      "Batch number : 065, Training: Loss:  0.4411, Accuracy: 0.8438\n",
      "Batch number : 066, Training: Loss:  0.4140, Accuracy: 0.8750\n",
      "Batch number : 067, Training: Loss:  0.5548, Accuracy: 0.8281\n",
      "Batch number : 068, Training: Loss:  0.6062, Accuracy: 0.8281\n",
      "Batch number : 069, Training: Loss:  0.3615, Accuracy: 0.8750\n",
      "Batch number : 070, Training: Loss:  0.5257, Accuracy: 0.8125\n",
      "Batch number : 071, Training: Loss:  0.3990, Accuracy: 0.8906\n",
      "Batch number : 072, Training: Loss:  0.5295, Accuracy: 0.8594\n",
      "Batch number : 073, Training: Loss:  0.3057, Accuracy: 0.8906\n",
      "Batch number : 074, Training: Loss:  0.4430, Accuracy: 0.8750\n",
      "Batch number : 075, Training: Loss:  0.4958, Accuracy: 0.8906\n",
      "Batch number : 076, Training: Loss:  0.3840, Accuracy: 0.8906\n",
      "Batch number : 077, Training: Loss:  0.3667, Accuracy: 0.8906\n",
      "Batch number : 078, Training: Loss:  0.4599, Accuracy: 0.8438\n",
      "Batch number : 079, Training: Loss:  0.4146, Accuracy: 0.8594\n",
      "Batch number : 080, Training: Loss:  0.6550, Accuracy: 0.8125\n",
      "Batch number : 081, Training: Loss:  0.5122, Accuracy: 0.8594\n",
      "Batch number : 082, Training: Loss:  0.6117, Accuracy: 0.7969\n",
      "Batch number : 083, Training: Loss:  0.4361, Accuracy: 0.8750\n",
      "Batch number : 084, Training: Loss:  0.3580, Accuracy: 0.9219\n",
      "Batch number : 085, Training: Loss:  0.3513, Accuracy: 0.8750\n",
      "Batch number : 086, Training: Loss:  0.4207, Accuracy: 0.9062\n",
      "Batch number : 087, Training: Loss:  0.2989, Accuracy: 0.9375\n",
      "Batch number : 088, Training: Loss:  0.3649, Accuracy: 0.8594\n",
      "Batch number : 089, Training: Loss:  0.2611, Accuracy: 0.9062\n",
      "Batch number : 090, Training: Loss:  0.2589, Accuracy: 0.9375\n",
      "Batch number : 091, Training: Loss:  0.3504, Accuracy: 0.9219\n",
      "Batch number : 092, Training: Loss:  0.4179, Accuracy: 0.8594\n",
      "Batch number : 093, Training: Loss:  0.5317, Accuracy: 0.8281\n",
      "Batch number : 094, Training: Loss:  0.6417, Accuracy: 0.8281\n",
      "Batch number : 095, Training: Loss:  0.4939, Accuracy: 0.8438\n",
      "Batch number : 096, Training: Loss:  0.6510, Accuracy: 0.8438\n",
      "Batch number : 097, Training: Loss:  0.3826, Accuracy: 0.8750\n",
      "Batch number : 098, Training: Loss:  0.5753, Accuracy: 0.8438\n",
      "Batch number : 099, Training: Loss:  0.3161, Accuracy: 0.9062\n",
      "Batch number : 100, Training: Loss:  0.4069, Accuracy: 0.8906\n",
      "Batch number : 101, Training: Loss:  0.4207, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.4880, Accuracy: 0.8281\n",
      "Batch number : 103, Training: Loss:  0.5593, Accuracy: 0.8281\n",
      "Batch number : 104, Training: Loss:  0.4602, Accuracy: 0.9062\n",
      "Batch number : 105, Training: Loss:  0.4989, Accuracy: 0.8594\n",
      "Batch number : 106, Training: Loss:  0.4872, Accuracy: 0.8594\n",
      "Batch number : 107, Training: Loss:  0.4991, Accuracy: 0.8438\n",
      "Batch number : 108, Training: Loss:  0.3991, Accuracy: 0.8906\n",
      "Batch number : 109, Training: Loss:  0.4489, Accuracy: 0.8594\n",
      "Batch number : 110, Training: Loss:  0.6207, Accuracy: 0.7812\n",
      "Batch number : 111, Training: Loss:  0.5046, Accuracy: 0.8438\n",
      "Batch number : 112, Training: Loss:  0.4378, Accuracy: 0.8438\n",
      "Batch number : 113, Training: Loss:  0.5701, Accuracy: 0.8438\n",
      "Batch number : 114, Training: Loss:  0.4396, Accuracy: 0.8906\n",
      "Batch number : 115, Training: Loss:  0.5919, Accuracy: 0.7969\n",
      "Batch number : 116, Training: Loss:  0.4719, Accuracy: 0.8438\n",
      "Batch number : 117, Training: Loss:  0.4668, Accuracy: 0.8906\n",
      "Batch number : 118, Training: Loss:  0.3464, Accuracy: 0.9219\n",
      "Batch number : 119, Training: Loss:  0.4823, Accuracy: 0.8438\n",
      "Batch number : 120, Training: Loss:  0.3084, Accuracy: 0.9375\n",
      "Batch number : 121, Training: Loss:  0.6007, Accuracy: 0.7969\n",
      "Batch number : 122, Training: Loss:  0.4718, Accuracy: 0.8594\n",
      "Batch number : 123, Training: Loss:  0.4993, Accuracy: 0.8438\n",
      "Batch number : 124, Training: Loss:  0.4066, Accuracy: 0.8906\n",
      "Batch number : 125, Training: Loss:  0.6934, Accuracy: 0.7656\n",
      "Batch number : 126, Training: Loss:  0.5292, Accuracy: 0.8750\n",
      "Batch number : 127, Training: Loss:  0.4527, Accuracy: 0.8594\n",
      "Batch number : 128, Training: Loss:  0.7214, Accuracy: 0.7344\n",
      "Batch number : 129, Training: Loss:  0.3785, Accuracy: 0.8906\n",
      "Batch number : 130, Training: Loss:  0.4480, Accuracy: 0.8750\n",
      "Batch number : 131, Training: Loss:  0.3445, Accuracy: 0.9219\n",
      "Batch number : 132, Training: Loss:  0.5782, Accuracy: 0.8281\n",
      "Batch number : 133, Training: Loss:  0.4061, Accuracy: 0.8438\n",
      "Batch number : 134, Training: Loss:  0.4608, Accuracy: 0.8438\n",
      "Batch number : 135, Training: Loss:  0.4698, Accuracy: 0.9062\n",
      "Batch number : 136, Training: Loss:  0.4592, Accuracy: 0.8594\n",
      "Batch number : 137, Training: Loss:  0.3813, Accuracy: 0.9062\n",
      "Batch number : 138, Training: Loss:  0.3478, Accuracy: 0.9062\n",
      "Batch number : 139, Training: Loss:  0.5519, Accuracy: 0.8438\n",
      "Batch number : 140, Training: Loss:  0.5329, Accuracy: 0.8125\n",
      "Batch number : 141, Training: Loss:  0.5420, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.7430, Accuracy: 0.7812\n",
      "Batch number : 143, Training: Loss:  0.3189, Accuracy: 0.9219\n",
      "Batch number : 144, Training: Loss:  0.4189, Accuracy: 0.8594\n",
      "Batch number : 145, Training: Loss:  0.4402, Accuracy: 0.8594\n",
      "Batch number : 146, Training: Loss:  0.3696, Accuracy: 0.8750\n",
      "Batch number : 147, Training: Loss:  0.3520, Accuracy: 0.9062\n",
      "Batch number : 148, Training: Loss:  0.5289, Accuracy: 0.8594\n",
      "Batch number : 149, Training: Loss:  0.3809, Accuracy: 0.9219\n",
      "Batch number : 150, Training: Loss:  0.6604, Accuracy: 0.8281\n",
      "Batch number : 151, Training: Loss:  0.4072, Accuracy: 0.8906\n",
      "Batch number : 152, Training: Loss:  0.3178, Accuracy: 0.8750\n",
      "Batch number : 153, Training: Loss:  0.5427, Accuracy: 0.7969\n",
      "Batch number : 154, Training: Loss:  0.4391, Accuracy: 0.8438\n",
      "Batch number : 155, Training: Loss:  0.5719, Accuracy: 0.8750\n",
      "Batch number : 156, Training: Loss:  0.5207, Accuracy: 0.8281\n",
      "Batch number : 157, Training: Loss:  0.5561, Accuracy: 0.7969\n",
      "Batch number : 158, Training: Loss:  0.3770, Accuracy: 0.8906\n",
      "Batch number : 159, Training: Loss:  0.4855, Accuracy: 0.8281\n",
      "Batch number : 160, Training: Loss:  0.4043, Accuracy: 0.8906\n",
      "Batch number : 161, Training: Loss:  0.5403, Accuracy: 0.8906\n",
      "Batch number : 162, Training: Loss:  0.3692, Accuracy: 0.9062\n",
      "Batch number : 163, Training: Loss:  0.4337, Accuracy: 0.8594\n",
      "Batch number : 164, Training: Loss:  0.4286, Accuracy: 0.8906\n",
      "Batch number : 165, Training: Loss:  0.5102, Accuracy: 0.8281\n",
      "Batch number : 166, Training: Loss:  0.4870, Accuracy: 0.8438\n",
      "Batch number : 167, Training: Loss:  0.3858, Accuracy: 0.8750\n",
      "Batch number : 168, Training: Loss:  0.6809, Accuracy: 0.7812\n",
      "Batch number : 169, Training: Loss:  0.6698, Accuracy: 0.7969\n",
      "Batch number : 170, Training: Loss:  0.6102, Accuracy: 0.7969\n",
      "Batch number : 171, Training: Loss:  0.3662, Accuracy: 0.9062\n",
      "Batch number : 172, Training: Loss:  0.4535, Accuracy: 0.8594\n",
      "Batch number : 173, Training: Loss:  0.4489, Accuracy: 0.8750\n",
      "Batch number : 174, Training: Loss:  0.2415, Accuracy: 0.9375\n",
      "Batch number : 175, Training: Loss:  0.4838, Accuracy: 0.8125\n",
      "Batch number : 176, Training: Loss:  0.5359, Accuracy: 0.8750\n",
      "Batch number : 177, Training: Loss:  0.4333, Accuracy: 0.8750\n",
      "Batch number : 178, Training: Loss:  0.6175, Accuracy: 0.8125\n",
      "Batch number : 179, Training: Loss:  0.5318, Accuracy: 0.8125\n",
      "Batch number : 180, Training: Loss:  0.4985, Accuracy: 0.8438\n",
      "Batch number : 181, Training: Loss:  0.3475, Accuracy: 0.9062\n",
      "Batch number : 182, Training: Loss:  0.4621, Accuracy: 0.8594\n",
      "Batch number : 183, Training: Loss:  0.3530, Accuracy: 0.9219\n",
      "Batch number : 184, Training: Loss:  0.3842, Accuracy: 0.8750\n",
      "Batch number : 185, Training: Loss:  0.2399, Accuracy: 0.9375\n",
      "Batch number : 186, Training: Loss:  0.4973, Accuracy: 0.8281\n",
      "Batch number : 187, Training: Loss:  0.5116, Accuracy: 0.8438\n",
      "Batch number : 188, Training: Loss:  0.3529, Accuracy: 0.8750\n",
      "Batch number : 189, Training: Loss:  0.4988, Accuracy: 0.8594\n",
      "Batch number : 190, Training: Loss:  0.3563, Accuracy: 0.8750\n",
      "Batch number : 191, Training: Loss:  0.3528, Accuracy: 0.9219\n",
      "Batch number : 192, Training: Loss:  0.3312, Accuracy: 0.9219\n",
      "Batch number : 193, Training: Loss:  0.3298, Accuracy: 0.8906\n",
      "Batch number : 194, Training: Loss:  0.4205, Accuracy: 0.8906\n",
      "Batch number : 195, Training: Loss:  0.5783, Accuracy: 0.8125\n",
      "Batch number : 196, Training: Loss:  0.3484, Accuracy: 0.8906\n",
      "Batch number : 197, Training: Loss:  0.4145, Accuracy: 0.8750\n",
      "Batch number : 198, Training: Loss:  0.6015, Accuracy: 0.8438\n",
      "Batch number : 199, Training: Loss:  0.4112, Accuracy: 0.8750\n",
      "Batch number : 200, Training: Loss:  0.5317, Accuracy: 0.8281\n",
      "Batch number : 201, Training: Loss:  0.5016, Accuracy: 0.8438\n",
      "Batch number : 202, Training: Loss:  0.5112, Accuracy: 0.8281\n",
      "Batch number : 203, Training: Loss:  0.4475, Accuracy: 0.8906\n",
      "Batch number : 204, Training: Loss:  0.3618, Accuracy: 0.9062\n",
      "Batch number : 205, Training: Loss:  0.4670, Accuracy: 0.8594\n",
      "Batch number : 206, Training: Loss:  0.4655, Accuracy: 0.8594\n",
      "Batch number : 207, Training: Loss:  0.3204, Accuracy: 0.9219\n",
      "Batch number : 208, Training: Loss:  0.2318, Accuracy: 0.9531\n",
      "Batch number : 209, Training: Loss:  0.4493, Accuracy: 0.8594\n",
      "Batch number : 210, Training: Loss:  0.4039, Accuracy: 0.8750\n",
      "Batch number : 211, Training: Loss:  0.4539, Accuracy: 0.8906\n",
      "Batch number : 212, Training: Loss:  0.5094, Accuracy: 0.8594\n",
      "Batch number : 213, Training: Loss:  0.4634, Accuracy: 0.8594\n",
      "Batch number : 214, Training: Loss:  0.9544, Accuracy: 0.7812\n",
      "Batch number : 215, Training: Loss:  0.5756, Accuracy: 0.8125\n",
      "Batch number : 216, Training: Loss:  0.5274, Accuracy: 0.8750\n",
      "Batch number : 217, Training: Loss:  0.5064, Accuracy: 0.8594\n",
      "Batch number : 218, Training: Loss:  0.4312, Accuracy: 0.8750\n",
      "Batch number : 219, Training: Loss:  0.4244, Accuracy: 0.8906\n",
      "Batch number : 220, Training: Loss:  0.5276, Accuracy: 0.8906\n",
      "Batch number : 221, Training: Loss:  0.4394, Accuracy: 0.8750\n",
      "Batch number : 222, Training: Loss:  0.4249, Accuracy: 0.9062\n",
      "Batch number : 223, Training: Loss:  0.5349, Accuracy: 0.8438\n",
      "Batch number : 224, Training: Loss:  0.4882, Accuracy: 0.8594\n",
      "Batch number : 225, Training: Loss:  0.4099, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.4764, Accuracy: 0.8750\n",
      "Batch number : 227, Training: Loss:  0.4349, Accuracy: 0.8906\n",
      "Batch number : 228, Training: Loss:  0.5920, Accuracy: 0.7969\n",
      "Batch number : 229, Training: Loss:  0.4127, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.5284, Accuracy: 0.8281\n",
      "Batch number : 231, Training: Loss:  0.5010, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.6176, Accuracy: 0.8438\n",
      "Batch number : 233, Training: Loss:  0.3908, Accuracy: 0.9062\n",
      "Batch number : 234, Training: Loss:  0.4595, Accuracy: 0.8750\n",
      "Batch number : 235, Training: Loss:  0.4451, Accuracy: 0.8594\n",
      "Batch number : 236, Training: Loss:  0.4991, Accuracy: 0.8438\n",
      "Batch number : 237, Training: Loss:  0.3968, Accuracy: 0.8750\n",
      "Batch number : 238, Training: Loss:  0.4219, Accuracy: 0.8906\n",
      "Batch number : 239, Training: Loss:  0.6157, Accuracy: 0.7969\n",
      "Batch number : 240, Training: Loss:  0.3480, Accuracy: 0.8906\n",
      "Batch number : 241, Training: Loss:  0.4487, Accuracy: 0.8594\n",
      "Batch number : 242, Training: Loss:  0.6257, Accuracy: 0.8281\n",
      "Batch number : 243, Training: Loss:  0.6767, Accuracy: 0.7812\n",
      "Batch number : 244, Training: Loss:  0.6313, Accuracy: 0.8438\n",
      "Batch number : 245, Training: Loss:  0.3838, Accuracy: 0.9062\n",
      "Batch number : 246, Training: Loss:  0.2978, Accuracy: 0.9375\n",
      "Batch number : 247, Training: Loss:  0.5447, Accuracy: 0.8438\n",
      "Batch number : 248, Training: Loss:  0.4887, Accuracy: 0.8594\n",
      "Batch number : 249, Training: Loss:  0.4418, Accuracy: 0.8750\n",
      "Batch number : 250, Training: Loss:  0.3019, Accuracy: 0.9219\n",
      "Batch number : 251, Training: Loss:  0.5596, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.4309, Accuracy: 0.8750\n",
      "Batch number : 253, Training: Loss:  0.5639, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.5958, Accuracy: 0.8438\n",
      "Batch number : 255, Training: Loss:  0.6375, Accuracy: 0.7812\n",
      "Batch number : 256, Training: Loss:  0.3541, Accuracy: 0.9062\n",
      "Batch number : 257, Training: Loss:  0.3616, Accuracy: 0.8906\n",
      "Batch number : 258, Training: Loss:  0.3938, Accuracy: 0.8906\n",
      "Batch number : 259, Training: Loss:  0.4258, Accuracy: 0.8594\n",
      "Batch number : 260, Training: Loss:  0.5416, Accuracy: 0.8281\n",
      "Batch number : 261, Training: Loss:  0.7344, Accuracy: 0.7969\n",
      "Batch number : 262, Training: Loss:  0.3596, Accuracy: 0.8906\n",
      "Batch number : 263, Training: Loss:  0.7467, Accuracy: 0.7656\n",
      "Batch number : 264, Training: Loss:  0.4631, Accuracy: 0.8594\n",
      "Batch number : 265, Training: Loss:  0.5246, Accuracy: 0.8281\n",
      "Batch number : 266, Training: Loss:  0.7363, Accuracy: 0.8125\n",
      "Batch number : 267, Training: Loss:  0.4765, Accuracy: 0.8594\n",
      "Batch number : 268, Training: Loss:  0.6235, Accuracy: 0.8281\n",
      "Batch number : 269, Training: Loss:  0.3265, Accuracy: 0.9375\n",
      "Batch number : 270, Training: Loss:  0.6396, Accuracy: 0.7656\n",
      "Batch number : 271, Training: Loss:  0.3475, Accuracy: 0.9375\n",
      "Batch number : 272, Training: Loss:  0.6634, Accuracy: 0.8281\n",
      "Batch number : 273, Training: Loss:  0.5092, Accuracy: 0.8594\n",
      "Batch number : 274, Training: Loss:  0.2834, Accuracy: 0.9219\n",
      "Batch number : 275, Training: Loss:  0.7302, Accuracy: 0.7812\n",
      "Batch number : 276, Training: Loss:  0.7913, Accuracy: 0.7656\n",
      "Batch number : 277, Training: Loss:  0.3643, Accuracy: 0.8594\n",
      "Batch number : 278, Training: Loss:  0.5559, Accuracy: 0.8281\n",
      "Batch number : 279, Training: Loss:  0.4488, Accuracy: 0.8906\n",
      "Batch number : 280, Training: Loss:  0.4928, Accuracy: 0.8281\n",
      "Batch number : 281, Training: Loss:  0.6085, Accuracy: 0.8125\n",
      "Batch number : 282, Training: Loss:  0.5826, Accuracy: 0.8125\n",
      "Batch number : 283, Training: Loss:  0.4799, Accuracy: 0.8125\n",
      "Batch number : 284, Training: Loss:  0.4265, Accuracy: 0.8906\n",
      "Batch number : 285, Training: Loss:  0.5191, Accuracy: 0.8281\n",
      "Batch number : 286, Training: Loss:  0.4770, Accuracy: 0.8594\n",
      "Batch number : 287, Training: Loss:  0.6047, Accuracy: 0.8125\n",
      "Batch number : 288, Training: Loss:  0.4163, Accuracy: 0.8906\n",
      "Batch number : 289, Training: Loss:  0.6081, Accuracy: 0.8438\n",
      "Batch number : 290, Training: Loss:  0.6674, Accuracy: 0.7500\n",
      "Batch number : 291, Training: Loss:  0.4001, Accuracy: 0.8906\n",
      "Batch number : 292, Training: Loss:  0.3753, Accuracy: 0.9062\n",
      "Batch number : 293, Training: Loss:  0.5419, Accuracy: 0.8594\n",
      "Batch number : 294, Training: Loss:  0.4274, Accuracy: 0.8750\n",
      "Batch number : 295, Training: Loss:  0.3741, Accuracy: 0.8906\n",
      "Batch number : 296, Training: Loss:  0.4265, Accuracy: 0.8906\n",
      "Batch number : 297, Training: Loss:  0.5442, Accuracy: 0.8594\n",
      "Batch number : 298, Training: Loss:  0.4169, Accuracy: 0.8750\n",
      "Batch number : 299, Training: Loss:  0.2894, Accuracy: 0.9375\n",
      "Batch number : 300, Training: Loss:  0.5582, Accuracy: 0.8438\n",
      "Batch number : 301, Training: Loss:  0.4539, Accuracy: 0.8594\n",
      "Batch number : 302, Training: Loss:  0.5392, Accuracy: 0.8438\n",
      "Batch number : 303, Training: Loss:  0.3499, Accuracy: 0.9062\n",
      "Batch number : 304, Training: Loss:  0.6550, Accuracy: 0.7344\n",
      "Batch number : 305, Training: Loss:  0.3029, Accuracy: 0.8906\n",
      "Batch number : 306, Training: Loss:  0.5750, Accuracy: 0.8125\n",
      "Batch number : 307, Training: Loss:  0.5827, Accuracy: 0.8125\n",
      "Batch number : 308, Training: Loss:  0.3611, Accuracy: 0.8906\n",
      "Batch number : 309, Training: Loss:  0.5682, Accuracy: 0.8438\n",
      "Batch number : 310, Training: Loss:  0.3574, Accuracy: 0.8750\n",
      "Batch number : 311, Training: Loss:  0.5406, Accuracy: 0.8125\n",
      "Batch number : 312, Training: Loss:  0.4771, Accuracy: 0.8281\n",
      "Batch number : 313, Training: Loss:  0.4801, Accuracy: 0.8281\n",
      "Batch number : 314, Training: Loss:  0.4134, Accuracy: 0.8438\n",
      "Batch number : 315, Training: Loss:  0.5158, Accuracy: 0.8438\n",
      "Batch number : 316, Training: Loss:  0.4474, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.4821, Accuracy: 0.8281\n",
      "Batch number : 318, Training: Loss:  0.5143, Accuracy: 0.8281\n",
      "Batch number : 319, Training: Loss:  0.4337, Accuracy: 0.9062\n",
      "Batch number : 320, Training: Loss:  0.5399, Accuracy: 0.8125\n",
      "Batch number : 321, Training: Loss:  0.6337, Accuracy: 0.8438\n",
      "Batch number : 322, Training: Loss:  0.5355, Accuracy: 0.8438\n",
      "Batch number : 323, Training: Loss:  0.4025, Accuracy: 0.8906\n",
      "Batch number : 324, Training: Loss:  0.5236, Accuracy: 0.8125\n",
      "Batch number : 325, Training: Loss:  0.8125, Accuracy: 0.7500\n",
      "Batch number : 326, Training: Loss:  0.5747, Accuracy: 0.8594\n",
      "Batch number : 327, Training: Loss:  0.4340, Accuracy: 0.9062\n",
      "Batch number : 328, Training: Loss:  0.3958, Accuracy: 0.8750\n",
      "Batch number : 329, Training: Loss:  0.3874, Accuracy: 0.9219\n",
      "Batch number : 330, Training: Loss:  0.4928, Accuracy: 0.8750\n",
      "Batch number : 331, Training: Loss:  0.4467, Accuracy: 0.8594\n",
      "Batch number : 332, Training: Loss:  0.3462, Accuracy: 0.9062\n",
      "Batch number : 333, Training: Loss:  0.6533, Accuracy: 0.8438\n",
      "Batch number : 334, Training: Loss:  0.7107, Accuracy: 0.8281\n",
      "Batch number : 335, Training: Loss:  0.5121, Accuracy: 0.8281\n",
      "Batch number : 336, Training: Loss:  0.3855, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.5964, Accuracy: 0.8281\n",
      "Batch number : 338, Training: Loss:  0.5045, Accuracy: 0.8281\n",
      "Batch number : 339, Training: Loss:  0.5560, Accuracy: 0.8594\n",
      "Batch number : 340, Training: Loss:  0.6340, Accuracy: 0.8125\n",
      "Batch number : 341, Training: Loss:  0.2876, Accuracy: 0.9531\n",
      "Batch number : 342, Training: Loss:  0.4548, Accuracy: 0.8594\n",
      "Batch number : 343, Training: Loss:  0.5681, Accuracy: 0.8438\n",
      "Batch number : 344, Training: Loss:  0.3822, Accuracy: 0.8906\n",
      "Batch number : 345, Training: Loss:  0.5020, Accuracy: 0.8125\n",
      "Batch number : 346, Training: Loss:  0.3543, Accuracy: 0.8906\n",
      "Batch number : 347, Training: Loss:  0.5401, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.2938, Accuracy: 0.9062\n",
      "Batch number : 349, Training: Loss:  0.3982, Accuracy: 0.8594\n",
      "Batch number : 350, Training: Loss:  0.7409, Accuracy: 0.7344\n",
      "Batch number : 351, Training: Loss:  0.4443, Accuracy: 0.8594\n",
      "Batch number : 352, Training: Loss:  0.6738, Accuracy: 0.7969\n",
      "Batch number : 353, Training: Loss:  0.2939, Accuracy: 0.9219\n",
      "Batch number : 354, Training: Loss:  0.5695, Accuracy: 0.7969\n",
      "Batch number : 355, Training: Loss:  0.3685, Accuracy: 0.9219\n",
      "Batch number : 356, Training: Loss:  0.3950, Accuracy: 0.8750\n",
      "Batch number : 357, Training: Loss:  0.5373, Accuracy: 0.8281\n",
      "Batch number : 358, Training: Loss:  0.5015, Accuracy: 0.8594\n",
      "Batch number : 359, Training: Loss:  0.3676, Accuracy: 0.9219\n",
      "Batch number : 360, Training: Loss:  0.5655, Accuracy: 0.8125\n",
      "Batch number : 361, Training: Loss:  0.2874, Accuracy: 0.9219\n",
      "Batch number : 362, Training: Loss:  0.5266, Accuracy: 0.7969\n",
      "Batch number : 363, Training: Loss:  0.4331, Accuracy: 0.8750\n",
      "Batch number : 364, Training: Loss:  0.5657, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.5843, Accuracy: 0.8125\n",
      "Batch number : 366, Training: Loss:  0.5016, Accuracy: 0.8594\n",
      "Batch number : 367, Training: Loss:  0.5439, Accuracy: 0.8281\n",
      "Batch number : 368, Training: Loss:  0.4298, Accuracy: 0.8906\n",
      "Batch number : 369, Training: Loss:  0.4625, Accuracy: 0.8594\n",
      "Batch number : 370, Training: Loss:  0.5038, Accuracy: 0.8594\n",
      "Batch number : 371, Training: Loss:  0.5240, Accuracy: 0.8594\n",
      "Batch number : 372, Training: Loss:  0.3804, Accuracy: 0.8750\n",
      "Batch number : 373, Training: Loss:  0.4516, Accuracy: 0.8438\n",
      "Batch number : 374, Training: Loss:  0.2659, Accuracy: 0.9062\n",
      "Batch number : 375, Training: Loss:  0.4253, Accuracy: 0.8438\n",
      "Batch number : 376, Training: Loss:  0.5579, Accuracy: 0.7812\n",
      "Batch number : 377, Training: Loss:  0.8680, Accuracy: 0.7250\n",
      "Epoch: 15/20\n",
      "Batch number : 000, Training: Loss:  0.4739, Accuracy: 0.8281\n",
      "Batch number : 001, Training: Loss:  0.4186, Accuracy: 0.8594\n",
      "Batch number : 002, Training: Loss:  0.5311, Accuracy: 0.9062\n",
      "Batch number : 003, Training: Loss:  0.4304, Accuracy: 0.8438\n",
      "Batch number : 004, Training: Loss:  0.5659, Accuracy: 0.8438\n",
      "Batch number : 005, Training: Loss:  0.7421, Accuracy: 0.7500\n",
      "Batch number : 006, Training: Loss:  0.4037, Accuracy: 0.8750\n",
      "Batch number : 007, Training: Loss:  0.5346, Accuracy: 0.8281\n",
      "Batch number : 008, Training: Loss:  0.4891, Accuracy: 0.8906\n",
      "Batch number : 009, Training: Loss:  0.4305, Accuracy: 0.8438\n",
      "Batch number : 010, Training: Loss:  0.5035, Accuracy: 0.8750\n",
      "Batch number : 011, Training: Loss:  0.3956, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.3900, Accuracy: 0.8594\n",
      "Batch number : 013, Training: Loss:  0.3745, Accuracy: 0.9219\n",
      "Batch number : 014, Training: Loss:  0.3807, Accuracy: 0.8594\n",
      "Batch number : 015, Training: Loss:  0.6516, Accuracy: 0.7812\n",
      "Batch number : 016, Training: Loss:  0.6262, Accuracy: 0.7969\n",
      "Batch number : 017, Training: Loss:  0.2363, Accuracy: 0.9219\n",
      "Batch number : 018, Training: Loss:  0.5387, Accuracy: 0.8438\n",
      "Batch number : 019, Training: Loss:  0.6271, Accuracy: 0.8125\n",
      "Batch number : 020, Training: Loss:  0.3758, Accuracy: 0.8906\n",
      "Batch number : 021, Training: Loss:  0.6953, Accuracy: 0.7812\n",
      "Batch number : 022, Training: Loss:  0.2977, Accuracy: 0.9219\n",
      "Batch number : 023, Training: Loss:  0.3214, Accuracy: 0.9375\n",
      "Batch number : 024, Training: Loss:  0.4086, Accuracy: 0.8750\n",
      "Batch number : 025, Training: Loss:  0.6595, Accuracy: 0.7812\n",
      "Batch number : 026, Training: Loss:  0.3429, Accuracy: 0.9219\n",
      "Batch number : 027, Training: Loss:  0.6202, Accuracy: 0.8125\n",
      "Batch number : 028, Training: Loss:  0.6373, Accuracy: 0.8125\n",
      "Batch number : 029, Training: Loss:  0.4020, Accuracy: 0.8906\n",
      "Batch number : 030, Training: Loss:  0.4538, Accuracy: 0.8594\n",
      "Batch number : 031, Training: Loss:  0.5039, Accuracy: 0.8438\n",
      "Batch number : 032, Training: Loss:  0.5375, Accuracy: 0.8906\n",
      "Batch number : 033, Training: Loss:  0.4063, Accuracy: 0.8750\n",
      "Batch number : 034, Training: Loss:  0.3866, Accuracy: 0.9062\n",
      "Batch number : 035, Training: Loss:  0.4399, Accuracy: 0.8750\n",
      "Batch number : 036, Training: Loss:  0.4332, Accuracy: 0.8594\n",
      "Batch number : 037, Training: Loss:  0.5191, Accuracy: 0.8594\n",
      "Batch number : 038, Training: Loss:  0.3769, Accuracy: 0.8750\n",
      "Batch number : 039, Training: Loss:  0.4633, Accuracy: 0.8750\n",
      "Batch number : 040, Training: Loss:  0.4085, Accuracy: 0.8750\n",
      "Batch number : 041, Training: Loss:  0.6170, Accuracy: 0.8438\n",
      "Batch number : 042, Training: Loss:  0.7294, Accuracy: 0.7812\n",
      "Batch number : 043, Training: Loss:  0.5188, Accuracy: 0.8281\n",
      "Batch number : 044, Training: Loss:  0.4768, Accuracy: 0.8906\n",
      "Batch number : 045, Training: Loss:  0.5227, Accuracy: 0.8750\n",
      "Batch number : 046, Training: Loss:  0.6891, Accuracy: 0.8281\n",
      "Batch number : 047, Training: Loss:  0.5317, Accuracy: 0.8281\n",
      "Batch number : 048, Training: Loss:  0.5877, Accuracy: 0.8906\n",
      "Batch number : 049, Training: Loss:  0.5613, Accuracy: 0.8281\n",
      "Batch number : 050, Training: Loss:  0.3553, Accuracy: 0.9219\n",
      "Batch number : 051, Training: Loss:  0.3722, Accuracy: 0.8906\n",
      "Batch number : 052, Training: Loss:  0.5938, Accuracy: 0.8125\n",
      "Batch number : 053, Training: Loss:  0.3984, Accuracy: 0.8750\n",
      "Batch number : 054, Training: Loss:  0.4205, Accuracy: 0.8906\n",
      "Batch number : 055, Training: Loss:  0.5160, Accuracy: 0.7969\n",
      "Batch number : 056, Training: Loss:  0.7225, Accuracy: 0.7500\n",
      "Batch number : 057, Training: Loss:  0.3735, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.4096, Accuracy: 0.8906\n",
      "Batch number : 059, Training: Loss:  0.4164, Accuracy: 0.8750\n",
      "Batch number : 060, Training: Loss:  0.4959, Accuracy: 0.8594\n",
      "Batch number : 061, Training: Loss:  0.5427, Accuracy: 0.8281\n",
      "Batch number : 062, Training: Loss:  0.4421, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.5327, Accuracy: 0.8125\n",
      "Batch number : 064, Training: Loss:  0.6316, Accuracy: 0.7969\n",
      "Batch number : 065, Training: Loss:  0.3964, Accuracy: 0.8906\n",
      "Batch number : 066, Training: Loss:  0.3468, Accuracy: 0.8906\n",
      "Batch number : 067, Training: Loss:  0.2940, Accuracy: 0.9219\n",
      "Batch number : 068, Training: Loss:  0.3323, Accuracy: 0.8906\n",
      "Batch number : 069, Training: Loss:  0.4901, Accuracy: 0.8906\n",
      "Batch number : 070, Training: Loss:  0.5134, Accuracy: 0.8281\n",
      "Batch number : 071, Training: Loss:  0.4819, Accuracy: 0.8750\n",
      "Batch number : 072, Training: Loss:  0.4244, Accuracy: 0.8906\n",
      "Batch number : 073, Training: Loss:  0.3765, Accuracy: 0.9062\n",
      "Batch number : 074, Training: Loss:  0.7525, Accuracy: 0.7969\n",
      "Batch number : 075, Training: Loss:  0.3584, Accuracy: 0.8750\n",
      "Batch number : 076, Training: Loss:  0.3878, Accuracy: 0.8750\n",
      "Batch number : 077, Training: Loss:  0.4846, Accuracy: 0.8594\n",
      "Batch number : 078, Training: Loss:  0.4351, Accuracy: 0.8594\n",
      "Batch number : 079, Training: Loss:  0.5343, Accuracy: 0.8438\n",
      "Batch number : 080, Training: Loss:  0.2665, Accuracy: 0.9062\n",
      "Batch number : 081, Training: Loss:  0.4360, Accuracy: 0.8906\n",
      "Batch number : 082, Training: Loss:  0.6049, Accuracy: 0.8281\n",
      "Batch number : 083, Training: Loss:  0.4229, Accuracy: 0.8906\n",
      "Batch number : 084, Training: Loss:  0.2563, Accuracy: 0.9375\n",
      "Batch number : 085, Training: Loss:  0.3685, Accuracy: 0.9062\n",
      "Batch number : 086, Training: Loss:  0.4143, Accuracy: 0.9062\n",
      "Batch number : 087, Training: Loss:  0.4755, Accuracy: 0.8438\n",
      "Batch number : 088, Training: Loss:  0.7551, Accuracy: 0.7500\n",
      "Batch number : 089, Training: Loss:  0.4435, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.2716, Accuracy: 0.9062\n",
      "Batch number : 091, Training: Loss:  0.4907, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.4090, Accuracy: 0.8906\n",
      "Batch number : 093, Training: Loss:  0.3452, Accuracy: 0.9062\n",
      "Batch number : 094, Training: Loss:  0.4521, Accuracy: 0.8750\n",
      "Batch number : 095, Training: Loss:  0.4434, Accuracy: 0.8438\n",
      "Batch number : 096, Training: Loss:  0.4857, Accuracy: 0.8438\n",
      "Batch number : 097, Training: Loss:  0.4248, Accuracy: 0.8750\n",
      "Batch number : 098, Training: Loss:  0.3518, Accuracy: 0.9062\n",
      "Batch number : 099, Training: Loss:  0.3960, Accuracy: 0.8594\n",
      "Batch number : 100, Training: Loss:  0.3403, Accuracy: 0.8906\n",
      "Batch number : 101, Training: Loss:  0.4426, Accuracy: 0.8438\n",
      "Batch number : 102, Training: Loss:  0.5182, Accuracy: 0.8281\n",
      "Batch number : 103, Training: Loss:  0.8959, Accuracy: 0.7344\n",
      "Batch number : 104, Training: Loss:  0.3903, Accuracy: 0.8750\n",
      "Batch number : 105, Training: Loss:  0.4493, Accuracy: 0.8594\n",
      "Batch number : 106, Training: Loss:  0.7371, Accuracy: 0.7656\n",
      "Batch number : 107, Training: Loss:  0.5813, Accuracy: 0.8125\n",
      "Batch number : 108, Training: Loss:  0.5790, Accuracy: 0.8750\n",
      "Batch number : 109, Training: Loss:  0.4488, Accuracy: 0.8906\n",
      "Batch number : 110, Training: Loss:  0.5482, Accuracy: 0.8438\n",
      "Batch number : 111, Training: Loss:  0.4254, Accuracy: 0.9062\n",
      "Batch number : 112, Training: Loss:  0.6015, Accuracy: 0.7656\n",
      "Batch number : 113, Training: Loss:  0.3409, Accuracy: 0.9062\n",
      "Batch number : 114, Training: Loss:  0.4550, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.4901, Accuracy: 0.8906\n",
      "Batch number : 116, Training: Loss:  0.4779, Accuracy: 0.8281\n",
      "Batch number : 117, Training: Loss:  0.2993, Accuracy: 0.8906\n",
      "Batch number : 118, Training: Loss:  0.6237, Accuracy: 0.8281\n",
      "Batch number : 119, Training: Loss:  0.4959, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.2445, Accuracy: 0.9219\n",
      "Batch number : 121, Training: Loss:  0.4856, Accuracy: 0.9062\n",
      "Batch number : 122, Training: Loss:  0.4537, Accuracy: 0.8750\n",
      "Batch number : 123, Training: Loss:  0.5080, Accuracy: 0.8438\n",
      "Batch number : 124, Training: Loss:  0.4072, Accuracy: 0.8906\n",
      "Batch number : 125, Training: Loss:  0.4873, Accuracy: 0.8281\n",
      "Batch number : 126, Training: Loss:  0.4160, Accuracy: 0.8594\n",
      "Batch number : 127, Training: Loss:  0.3977, Accuracy: 0.8750\n",
      "Batch number : 128, Training: Loss:  0.6866, Accuracy: 0.7969\n",
      "Batch number : 129, Training: Loss:  0.4736, Accuracy: 0.8594\n",
      "Batch number : 130, Training: Loss:  0.4382, Accuracy: 0.8438\n",
      "Batch number : 131, Training: Loss:  0.5016, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.5929, Accuracy: 0.7969\n",
      "Batch number : 133, Training: Loss:  0.5212, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.4212, Accuracy: 0.8438\n",
      "Batch number : 135, Training: Loss:  0.4947, Accuracy: 0.8438\n",
      "Batch number : 136, Training: Loss:  0.4280, Accuracy: 0.8438\n",
      "Batch number : 137, Training: Loss:  0.4591, Accuracy: 0.8438\n",
      "Batch number : 138, Training: Loss:  0.4460, Accuracy: 0.8750\n",
      "Batch number : 139, Training: Loss:  0.5226, Accuracy: 0.8281\n",
      "Batch number : 140, Training: Loss:  0.3950, Accuracy: 0.8594\n",
      "Batch number : 141, Training: Loss:  0.2740, Accuracy: 0.9219\n",
      "Batch number : 142, Training: Loss:  0.6058, Accuracy: 0.8281\n",
      "Batch number : 143, Training: Loss:  0.4535, Accuracy: 0.8594\n",
      "Batch number : 144, Training: Loss:  0.4770, Accuracy: 0.8594\n",
      "Batch number : 145, Training: Loss:  0.4944, Accuracy: 0.8594\n",
      "Batch number : 146, Training: Loss:  0.5288, Accuracy: 0.8594\n",
      "Batch number : 147, Training: Loss:  0.6415, Accuracy: 0.8125\n",
      "Batch number : 148, Training: Loss:  0.6312, Accuracy: 0.8125\n",
      "Batch number : 149, Training: Loss:  0.5932, Accuracy: 0.8281\n",
      "Batch number : 150, Training: Loss:  0.5239, Accuracy: 0.8438\n",
      "Batch number : 151, Training: Loss:  0.3774, Accuracy: 0.9062\n",
      "Batch number : 152, Training: Loss:  0.5335, Accuracy: 0.8125\n",
      "Batch number : 153, Training: Loss:  0.4688, Accuracy: 0.8438\n",
      "Batch number : 154, Training: Loss:  0.3983, Accuracy: 0.8750\n",
      "Batch number : 155, Training: Loss:  0.4821, Accuracy: 0.8594\n",
      "Batch number : 156, Training: Loss:  0.4608, Accuracy: 0.8906\n",
      "Batch number : 157, Training: Loss:  0.6759, Accuracy: 0.7500\n",
      "Batch number : 158, Training: Loss:  0.4062, Accuracy: 0.8906\n",
      "Batch number : 159, Training: Loss:  0.5270, Accuracy: 0.8281\n",
      "Batch number : 160, Training: Loss:  0.4285, Accuracy: 0.8750\n",
      "Batch number : 161, Training: Loss:  0.4721, Accuracy: 0.8750\n",
      "Batch number : 162, Training: Loss:  0.5534, Accuracy: 0.8438\n",
      "Batch number : 163, Training: Loss:  0.3966, Accuracy: 0.8750\n",
      "Batch number : 164, Training: Loss:  0.3848, Accuracy: 0.8594\n",
      "Batch number : 165, Training: Loss:  0.5294, Accuracy: 0.8594\n",
      "Batch number : 166, Training: Loss:  0.5073, Accuracy: 0.8438\n",
      "Batch number : 167, Training: Loss:  0.4429, Accuracy: 0.8750\n",
      "Batch number : 168, Training: Loss:  0.5097, Accuracy: 0.8125\n",
      "Batch number : 169, Training: Loss:  0.3037, Accuracy: 0.8906\n",
      "Batch number : 170, Training: Loss:  0.3299, Accuracy: 0.9219\n",
      "Batch number : 171, Training: Loss:  0.4099, Accuracy: 0.9062\n",
      "Batch number : 172, Training: Loss:  0.4841, Accuracy: 0.8594\n",
      "Batch number : 173, Training: Loss:  0.4785, Accuracy: 0.8438\n",
      "Batch number : 174, Training: Loss:  0.4637, Accuracy: 0.8594\n",
      "Batch number : 175, Training: Loss:  0.4388, Accuracy: 0.8594\n",
      "Batch number : 176, Training: Loss:  0.4278, Accuracy: 0.8594\n",
      "Batch number : 177, Training: Loss:  0.4436, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.7020, Accuracy: 0.8125\n",
      "Batch number : 179, Training: Loss:  0.5484, Accuracy: 0.8438\n",
      "Batch number : 180, Training: Loss:  0.5073, Accuracy: 0.8594\n",
      "Batch number : 181, Training: Loss:  0.3744, Accuracy: 0.8906\n",
      "Batch number : 182, Training: Loss:  0.4799, Accuracy: 0.8906\n",
      "Batch number : 183, Training: Loss:  0.3911, Accuracy: 0.9062\n",
      "Batch number : 184, Training: Loss:  0.6085, Accuracy: 0.7969\n",
      "Batch number : 185, Training: Loss:  0.5281, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.5968, Accuracy: 0.8125\n",
      "Batch number : 187, Training: Loss:  0.4014, Accuracy: 0.9062\n",
      "Batch number : 188, Training: Loss:  0.5275, Accuracy: 0.7969\n",
      "Batch number : 189, Training: Loss:  0.5004, Accuracy: 0.8594\n",
      "Batch number : 190, Training: Loss:  0.4932, Accuracy: 0.8750\n",
      "Batch number : 191, Training: Loss:  0.2886, Accuracy: 0.9062\n",
      "Batch number : 192, Training: Loss:  0.4065, Accuracy: 0.8594\n",
      "Batch number : 193, Training: Loss:  0.4792, Accuracy: 0.8281\n",
      "Batch number : 194, Training: Loss:  0.4152, Accuracy: 0.8750\n",
      "Batch number : 195, Training: Loss:  0.5706, Accuracy: 0.8281\n",
      "Batch number : 196, Training: Loss:  0.5681, Accuracy: 0.8125\n",
      "Batch number : 197, Training: Loss:  0.3363, Accuracy: 0.9062\n",
      "Batch number : 198, Training: Loss:  0.3617, Accuracy: 0.8750\n",
      "Batch number : 199, Training: Loss:  0.4460, Accuracy: 0.8750\n",
      "Batch number : 200, Training: Loss:  0.5164, Accuracy: 0.8125\n",
      "Batch number : 201, Training: Loss:  0.5255, Accuracy: 0.8125\n",
      "Batch number : 202, Training: Loss:  0.4241, Accuracy: 0.8750\n",
      "Batch number : 203, Training: Loss:  0.3641, Accuracy: 0.8750\n",
      "Batch number : 204, Training: Loss:  0.3563, Accuracy: 0.8906\n",
      "Batch number : 205, Training: Loss:  0.2999, Accuracy: 0.9375\n",
      "Batch number : 206, Training: Loss:  0.5520, Accuracy: 0.8438\n",
      "Batch number : 207, Training: Loss:  0.3494, Accuracy: 0.8906\n",
      "Batch number : 208, Training: Loss:  0.7263, Accuracy: 0.7812\n",
      "Batch number : 209, Training: Loss:  0.4693, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.4383, Accuracy: 0.9062\n",
      "Batch number : 211, Training: Loss:  0.4619, Accuracy: 0.9062\n",
      "Batch number : 212, Training: Loss:  0.2644, Accuracy: 0.8750\n",
      "Batch number : 213, Training: Loss:  0.5181, Accuracy: 0.7969\n",
      "Batch number : 214, Training: Loss:  0.4084, Accuracy: 0.8906\n",
      "Batch number : 215, Training: Loss:  0.5618, Accuracy: 0.8594\n",
      "Batch number : 216, Training: Loss:  0.3519, Accuracy: 0.8906\n",
      "Batch number : 217, Training: Loss:  0.5221, Accuracy: 0.8281\n",
      "Batch number : 218, Training: Loss:  0.5260, Accuracy: 0.8125\n",
      "Batch number : 219, Training: Loss:  0.5601, Accuracy: 0.8594\n",
      "Batch number : 220, Training: Loss:  0.3290, Accuracy: 0.8906\n",
      "Batch number : 221, Training: Loss:  0.5135, Accuracy: 0.8594\n",
      "Batch number : 222, Training: Loss:  0.4524, Accuracy: 0.8750\n",
      "Batch number : 223, Training: Loss:  0.5921, Accuracy: 0.8125\n",
      "Batch number : 224, Training: Loss:  0.4915, Accuracy: 0.8594\n",
      "Batch number : 225, Training: Loss:  0.4737, Accuracy: 0.8750\n",
      "Batch number : 226, Training: Loss:  0.4230, Accuracy: 0.8906\n",
      "Batch number : 227, Training: Loss:  0.5047, Accuracy: 0.8594\n",
      "Batch number : 228, Training: Loss:  0.6102, Accuracy: 0.8125\n",
      "Batch number : 229, Training: Loss:  0.5199, Accuracy: 0.8594\n",
      "Batch number : 230, Training: Loss:  0.3837, Accuracy: 0.8750\n",
      "Batch number : 231, Training: Loss:  0.6986, Accuracy: 0.7812\n",
      "Batch number : 232, Training: Loss:  0.3941, Accuracy: 0.8906\n",
      "Batch number : 233, Training: Loss:  0.4782, Accuracy: 0.8594\n",
      "Batch number : 234, Training: Loss:  0.6707, Accuracy: 0.7969\n",
      "Batch number : 235, Training: Loss:  0.4122, Accuracy: 0.8750\n",
      "Batch number : 236, Training: Loss:  0.4937, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.4729, Accuracy: 0.8594\n",
      "Batch number : 238, Training: Loss:  0.4017, Accuracy: 0.8906\n",
      "Batch number : 239, Training: Loss:  0.5791, Accuracy: 0.7656\n",
      "Batch number : 240, Training: Loss:  0.4751, Accuracy: 0.8281\n",
      "Batch number : 241, Training: Loss:  0.4465, Accuracy: 0.8281\n",
      "Batch number : 242, Training: Loss:  0.3429, Accuracy: 0.9219\n",
      "Batch number : 243, Training: Loss:  0.3522, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.3022, Accuracy: 0.9219\n",
      "Batch number : 245, Training: Loss:  0.4880, Accuracy: 0.8281\n",
      "Batch number : 246, Training: Loss:  0.6185, Accuracy: 0.8281\n",
      "Batch number : 247, Training: Loss:  0.4086, Accuracy: 0.8750\n",
      "Batch number : 248, Training: Loss:  0.5761, Accuracy: 0.8125\n",
      "Batch number : 249, Training: Loss:  0.3523, Accuracy: 0.8750\n",
      "Batch number : 250, Training: Loss:  0.5254, Accuracy: 0.8594\n",
      "Batch number : 251, Training: Loss:  0.3807, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.2966, Accuracy: 0.9219\n",
      "Batch number : 253, Training: Loss:  0.5717, Accuracy: 0.8281\n",
      "Batch number : 254, Training: Loss:  0.4474, Accuracy: 0.8594\n",
      "Batch number : 255, Training: Loss:  0.5618, Accuracy: 0.8281\n",
      "Batch number : 256, Training: Loss:  0.3943, Accuracy: 0.8750\n",
      "Batch number : 257, Training: Loss:  0.6685, Accuracy: 0.7656\n",
      "Batch number : 258, Training: Loss:  0.5304, Accuracy: 0.8125\n",
      "Batch number : 259, Training: Loss:  0.4052, Accuracy: 0.8750\n",
      "Batch number : 260, Training: Loss:  0.4346, Accuracy: 0.8906\n",
      "Batch number : 261, Training: Loss:  0.5117, Accuracy: 0.8594\n",
      "Batch number : 262, Training: Loss:  0.3691, Accuracy: 0.8906\n",
      "Batch number : 263, Training: Loss:  0.3887, Accuracy: 0.8906\n",
      "Batch number : 264, Training: Loss:  0.4904, Accuracy: 0.8594\n",
      "Batch number : 265, Training: Loss:  0.5051, Accuracy: 0.8281\n",
      "Batch number : 266, Training: Loss:  0.3998, Accuracy: 0.8906\n",
      "Batch number : 267, Training: Loss:  0.3741, Accuracy: 0.8906\n",
      "Batch number : 268, Training: Loss:  0.5846, Accuracy: 0.8281\n",
      "Batch number : 269, Training: Loss:  0.4451, Accuracy: 0.8906\n",
      "Batch number : 270, Training: Loss:  0.8320, Accuracy: 0.7656\n",
      "Batch number : 271, Training: Loss:  0.6332, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.2873, Accuracy: 0.9219\n",
      "Batch number : 273, Training: Loss:  0.3473, Accuracy: 0.9219\n",
      "Batch number : 274, Training: Loss:  0.4881, Accuracy: 0.8281\n",
      "Batch number : 275, Training: Loss:  0.3477, Accuracy: 0.9062\n",
      "Batch number : 276, Training: Loss:  0.4935, Accuracy: 0.8594\n",
      "Batch number : 277, Training: Loss:  0.4552, Accuracy: 0.8906\n",
      "Batch number : 278, Training: Loss:  0.4760, Accuracy: 0.8438\n",
      "Batch number : 279, Training: Loss:  0.4566, Accuracy: 0.8594\n",
      "Batch number : 280, Training: Loss:  0.3608, Accuracy: 0.8906\n",
      "Batch number : 281, Training: Loss:  0.5946, Accuracy: 0.7969\n",
      "Batch number : 282, Training: Loss:  0.3699, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.7162, Accuracy: 0.7656\n",
      "Batch number : 284, Training: Loss:  0.5187, Accuracy: 0.8125\n",
      "Batch number : 285, Training: Loss:  0.5500, Accuracy: 0.8438\n",
      "Batch number : 286, Training: Loss:  0.3855, Accuracy: 0.8750\n",
      "Batch number : 287, Training: Loss:  0.8076, Accuracy: 0.7812\n",
      "Batch number : 288, Training: Loss:  0.3902, Accuracy: 0.8750\n",
      "Batch number : 289, Training: Loss:  0.5771, Accuracy: 0.8125\n",
      "Batch number : 290, Training: Loss:  0.4699, Accuracy: 0.8438\n",
      "Batch number : 291, Training: Loss:  0.5769, Accuracy: 0.8125\n",
      "Batch number : 292, Training: Loss:  0.3690, Accuracy: 0.9219\n",
      "Batch number : 293, Training: Loss:  0.3959, Accuracy: 0.8750\n",
      "Batch number : 294, Training: Loss:  0.5781, Accuracy: 0.8125\n",
      "Batch number : 295, Training: Loss:  0.4814, Accuracy: 0.8281\n",
      "Batch number : 296, Training: Loss:  0.4635, Accuracy: 0.8594\n",
      "Batch number : 297, Training: Loss:  0.5012, Accuracy: 0.8594\n",
      "Batch number : 298, Training: Loss:  0.2840, Accuracy: 0.9375\n",
      "Batch number : 299, Training: Loss:  0.3938, Accuracy: 0.8594\n",
      "Batch number : 300, Training: Loss:  0.7061, Accuracy: 0.7969\n",
      "Batch number : 301, Training: Loss:  0.3802, Accuracy: 0.8906\n",
      "Batch number : 302, Training: Loss:  0.4947, Accuracy: 0.8438\n",
      "Batch number : 303, Training: Loss:  0.3819, Accuracy: 0.8750\n",
      "Batch number : 304, Training: Loss:  0.5322, Accuracy: 0.8594\n",
      "Batch number : 305, Training: Loss:  0.5186, Accuracy: 0.8594\n",
      "Batch number : 306, Training: Loss:  0.6911, Accuracy: 0.7656\n",
      "Batch number : 307, Training: Loss:  0.4642, Accuracy: 0.8594\n",
      "Batch number : 308, Training: Loss:  0.6146, Accuracy: 0.8438\n",
      "Batch number : 309, Training: Loss:  0.3006, Accuracy: 0.9219\n",
      "Batch number : 310, Training: Loss:  0.3742, Accuracy: 0.8906\n",
      "Batch number : 311, Training: Loss:  0.6527, Accuracy: 0.7344\n",
      "Batch number : 312, Training: Loss:  0.5125, Accuracy: 0.8438\n",
      "Batch number : 313, Training: Loss:  0.3713, Accuracy: 0.8750\n",
      "Batch number : 314, Training: Loss:  0.4094, Accuracy: 0.8906\n",
      "Batch number : 315, Training: Loss:  0.4283, Accuracy: 0.8438\n",
      "Batch number : 316, Training: Loss:  0.4942, Accuracy: 0.8594\n",
      "Batch number : 317, Training: Loss:  0.5452, Accuracy: 0.7969\n",
      "Batch number : 318, Training: Loss:  0.6016, Accuracy: 0.7969\n",
      "Batch number : 319, Training: Loss:  0.6446, Accuracy: 0.8125\n",
      "Batch number : 320, Training: Loss:  0.3434, Accuracy: 0.9219\n",
      "Batch number : 321, Training: Loss:  0.4164, Accuracy: 0.8906\n",
      "Batch number : 322, Training: Loss:  0.5294, Accuracy: 0.8594\n",
      "Batch number : 323, Training: Loss:  0.4168, Accuracy: 0.8750\n",
      "Batch number : 324, Training: Loss:  0.5085, Accuracy: 0.8281\n",
      "Batch number : 325, Training: Loss:  0.5671, Accuracy: 0.7969\n",
      "Batch number : 326, Training: Loss:  0.3998, Accuracy: 0.8750\n",
      "Batch number : 327, Training: Loss:  0.5001, Accuracy: 0.8438\n",
      "Batch number : 328, Training: Loss:  0.3718, Accuracy: 0.9219\n",
      "Batch number : 329, Training: Loss:  0.3352, Accuracy: 0.9062\n",
      "Batch number : 330, Training: Loss:  0.2974, Accuracy: 0.9219\n",
      "Batch number : 331, Training: Loss:  0.3101, Accuracy: 0.9062\n",
      "Batch number : 332, Training: Loss:  0.4974, Accuracy: 0.8750\n",
      "Batch number : 333, Training: Loss:  0.4233, Accuracy: 0.8750\n",
      "Batch number : 334, Training: Loss:  0.4508, Accuracy: 0.8594\n",
      "Batch number : 335, Training: Loss:  0.5074, Accuracy: 0.8281\n",
      "Batch number : 336, Training: Loss:  0.3701, Accuracy: 0.8750\n",
      "Batch number : 337, Training: Loss:  0.4481, Accuracy: 0.8594\n",
      "Batch number : 338, Training: Loss:  0.4206, Accuracy: 0.8906\n",
      "Batch number : 339, Training: Loss:  0.3584, Accuracy: 0.8906\n",
      "Batch number : 340, Training: Loss:  0.3861, Accuracy: 0.8750\n",
      "Batch number : 341, Training: Loss:  0.3660, Accuracy: 0.9219\n",
      "Batch number : 342, Training: Loss:  0.5072, Accuracy: 0.8750\n",
      "Batch number : 343, Training: Loss:  0.7048, Accuracy: 0.7969\n",
      "Batch number : 344, Training: Loss:  0.4664, Accuracy: 0.8438\n",
      "Batch number : 345, Training: Loss:  0.5051, Accuracy: 0.9062\n",
      "Batch number : 346, Training: Loss:  0.4836, Accuracy: 0.8750\n",
      "Batch number : 347, Training: Loss:  0.4838, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.3978, Accuracy: 0.8750\n",
      "Batch number : 349, Training: Loss:  0.2825, Accuracy: 0.9375\n",
      "Batch number : 350, Training: Loss:  0.4672, Accuracy: 0.8594\n",
      "Batch number : 351, Training: Loss:  0.4998, Accuracy: 0.8438\n",
      "Batch number : 352, Training: Loss:  0.3545, Accuracy: 0.9062\n",
      "Batch number : 353, Training: Loss:  0.4135, Accuracy: 0.8906\n",
      "Batch number : 354, Training: Loss:  0.5226, Accuracy: 0.8438\n",
      "Batch number : 355, Training: Loss:  0.5152, Accuracy: 0.8594\n",
      "Batch number : 356, Training: Loss:  0.4431, Accuracy: 0.8438\n",
      "Batch number : 357, Training: Loss:  0.2286, Accuracy: 0.9531\n",
      "Batch number : 358, Training: Loss:  0.5919, Accuracy: 0.8125\n",
      "Batch number : 359, Training: Loss:  0.5181, Accuracy: 0.8594\n",
      "Batch number : 360, Training: Loss:  0.5749, Accuracy: 0.8125\n",
      "Batch number : 361, Training: Loss:  0.4341, Accuracy: 0.8906\n",
      "Batch number : 362, Training: Loss:  0.4120, Accuracy: 0.8750\n",
      "Batch number : 363, Training: Loss:  0.4010, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.4486, Accuracy: 0.8750\n",
      "Batch number : 365, Training: Loss:  0.5997, Accuracy: 0.8125\n",
      "Batch number : 366, Training: Loss:  0.4303, Accuracy: 0.8438\n",
      "Batch number : 367, Training: Loss:  0.3961, Accuracy: 0.8906\n",
      "Batch number : 368, Training: Loss:  0.5047, Accuracy: 0.8281\n",
      "Batch number : 369, Training: Loss:  0.5486, Accuracy: 0.8594\n",
      "Batch number : 370, Training: Loss:  0.3600, Accuracy: 0.8750\n",
      "Batch number : 371, Training: Loss:  0.3786, Accuracy: 0.8906\n",
      "Batch number : 372, Training: Loss:  0.5622, Accuracy: 0.8438\n",
      "Batch number : 373, Training: Loss:  0.5863, Accuracy: 0.7969\n",
      "Batch number : 374, Training: Loss:  0.3518, Accuracy: 0.8750\n",
      "Batch number : 375, Training: Loss:  0.5694, Accuracy: 0.8125\n",
      "Batch number : 376, Training: Loss:  0.6170, Accuracy: 0.8125\n",
      "Batch number : 377, Training: Loss:  0.3522, Accuracy: 0.9000\n",
      "Epoch: 16/20\n",
      "Batch number : 000, Training: Loss:  0.5699, Accuracy: 0.8594\n",
      "Batch number : 001, Training: Loss:  0.4286, Accuracy: 0.9062\n",
      "Batch number : 002, Training: Loss:  0.5548, Accuracy: 0.8438\n",
      "Batch number : 003, Training: Loss:  0.5426, Accuracy: 0.8438\n",
      "Batch number : 004, Training: Loss:  0.6011, Accuracy: 0.8125\n",
      "Batch number : 005, Training: Loss:  0.3895, Accuracy: 0.8906\n",
      "Batch number : 006, Training: Loss:  0.4829, Accuracy: 0.8438\n",
      "Batch number : 007, Training: Loss:  0.5364, Accuracy: 0.8594\n",
      "Batch number : 008, Training: Loss:  0.4929, Accuracy: 0.8281\n",
      "Batch number : 009, Training: Loss:  0.3202, Accuracy: 0.9375\n",
      "Batch number : 010, Training: Loss:  0.4554, Accuracy: 0.8750\n",
      "Batch number : 011, Training: Loss:  0.5517, Accuracy: 0.8438\n",
      "Batch number : 012, Training: Loss:  0.3449, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.5226, Accuracy: 0.7969\n",
      "Batch number : 014, Training: Loss:  0.3630, Accuracy: 0.8750\n",
      "Batch number : 015, Training: Loss:  0.4414, Accuracy: 0.8906\n",
      "Batch number : 016, Training: Loss:  0.4378, Accuracy: 0.8750\n",
      "Batch number : 017, Training: Loss:  0.6335, Accuracy: 0.7969\n",
      "Batch number : 018, Training: Loss:  0.5218, Accuracy: 0.8438\n",
      "Batch number : 019, Training: Loss:  0.4941, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.3887, Accuracy: 0.8438\n",
      "Batch number : 021, Training: Loss:  0.4604, Accuracy: 0.8906\n",
      "Batch number : 022, Training: Loss:  0.4888, Accuracy: 0.8438\n",
      "Batch number : 023, Training: Loss:  0.3433, Accuracy: 0.9219\n",
      "Batch number : 024, Training: Loss:  0.2637, Accuracy: 0.9219\n",
      "Batch number : 025, Training: Loss:  0.4729, Accuracy: 0.8594\n",
      "Batch number : 026, Training: Loss:  0.4184, Accuracy: 0.8906\n",
      "Batch number : 027, Training: Loss:  0.4605, Accuracy: 0.8594\n",
      "Batch number : 028, Training: Loss:  0.5724, Accuracy: 0.8750\n",
      "Batch number : 029, Training: Loss:  0.3511, Accuracy: 0.8906\n",
      "Batch number : 030, Training: Loss:  0.6429, Accuracy: 0.8125\n",
      "Batch number : 031, Training: Loss:  0.4508, Accuracy: 0.8594\n",
      "Batch number : 032, Training: Loss:  0.7169, Accuracy: 0.8594\n",
      "Batch number : 033, Training: Loss:  0.4032, Accuracy: 0.8594\n",
      "Batch number : 034, Training: Loss:  0.7106, Accuracy: 0.7969\n",
      "Batch number : 035, Training: Loss:  0.5289, Accuracy: 0.8438\n",
      "Batch number : 036, Training: Loss:  0.5148, Accuracy: 0.8750\n",
      "Batch number : 037, Training: Loss:  0.4070, Accuracy: 0.8750\n",
      "Batch number : 038, Training: Loss:  0.5266, Accuracy: 0.8594\n",
      "Batch number : 039, Training: Loss:  0.3846, Accuracy: 0.8750\n",
      "Batch number : 040, Training: Loss:  0.3490, Accuracy: 0.9062\n",
      "Batch number : 041, Training: Loss:  0.7113, Accuracy: 0.7656\n",
      "Batch number : 042, Training: Loss:  0.4453, Accuracy: 0.8750\n",
      "Batch number : 043, Training: Loss:  0.2812, Accuracy: 0.9375\n",
      "Batch number : 044, Training: Loss:  0.4235, Accuracy: 0.8594\n",
      "Batch number : 045, Training: Loss:  0.5535, Accuracy: 0.8125\n",
      "Batch number : 046, Training: Loss:  0.3098, Accuracy: 0.9219\n",
      "Batch number : 047, Training: Loss:  0.5779, Accuracy: 0.8281\n",
      "Batch number : 048, Training: Loss:  0.6552, Accuracy: 0.7812\n",
      "Batch number : 049, Training: Loss:  0.5420, Accuracy: 0.8438\n",
      "Batch number : 050, Training: Loss:  0.5947, Accuracy: 0.8125\n",
      "Batch number : 051, Training: Loss:  0.4332, Accuracy: 0.8906\n",
      "Batch number : 052, Training: Loss:  0.3881, Accuracy: 0.8750\n",
      "Batch number : 053, Training: Loss:  0.5707, Accuracy: 0.8281\n",
      "Batch number : 054, Training: Loss:  0.6848, Accuracy: 0.7812\n",
      "Batch number : 055, Training: Loss:  0.5512, Accuracy: 0.8438\n",
      "Batch number : 056, Training: Loss:  0.5608, Accuracy: 0.7969\n",
      "Batch number : 057, Training: Loss:  0.5510, Accuracy: 0.8750\n",
      "Batch number : 058, Training: Loss:  0.4496, Accuracy: 0.8750\n",
      "Batch number : 059, Training: Loss:  0.4610, Accuracy: 0.8438\n",
      "Batch number : 060, Training: Loss:  0.4236, Accuracy: 0.8594\n",
      "Batch number : 061, Training: Loss:  0.4853, Accuracy: 0.8750\n",
      "Batch number : 062, Training: Loss:  0.3188, Accuracy: 0.9062\n",
      "Batch number : 063, Training: Loss:  0.5633, Accuracy: 0.8125\n",
      "Batch number : 064, Training: Loss:  0.2832, Accuracy: 0.9062\n",
      "Batch number : 065, Training: Loss:  0.4701, Accuracy: 0.8594\n",
      "Batch number : 066, Training: Loss:  0.4258, Accuracy: 0.8750\n",
      "Batch number : 067, Training: Loss:  0.5438, Accuracy: 0.8594\n",
      "Batch number : 068, Training: Loss:  0.5152, Accuracy: 0.8438\n",
      "Batch number : 069, Training: Loss:  0.5602, Accuracy: 0.8281\n",
      "Batch number : 070, Training: Loss:  0.5986, Accuracy: 0.8125\n",
      "Batch number : 071, Training: Loss:  0.2756, Accuracy: 0.9375\n",
      "Batch number : 072, Training: Loss:  0.4865, Accuracy: 0.8281\n",
      "Batch number : 073, Training: Loss:  0.4941, Accuracy: 0.8281\n",
      "Batch number : 074, Training: Loss:  0.5835, Accuracy: 0.8438\n",
      "Batch number : 075, Training: Loss:  0.4922, Accuracy: 0.7969\n",
      "Batch number : 076, Training: Loss:  0.3865, Accuracy: 0.9062\n",
      "Batch number : 077, Training: Loss:  0.6042, Accuracy: 0.8438\n",
      "Batch number : 078, Training: Loss:  0.4699, Accuracy: 0.8750\n",
      "Batch number : 079, Training: Loss:  0.3073, Accuracy: 0.9531\n",
      "Batch number : 080, Training: Loss:  0.5985, Accuracy: 0.8125\n",
      "Batch number : 081, Training: Loss:  0.5930, Accuracy: 0.8281\n",
      "Batch number : 082, Training: Loss:  0.3067, Accuracy: 0.9375\n",
      "Batch number : 083, Training: Loss:  0.3975, Accuracy: 0.8750\n",
      "Batch number : 084, Training: Loss:  0.4918, Accuracy: 0.8750\n",
      "Batch number : 085, Training: Loss:  0.4296, Accuracy: 0.8906\n",
      "Batch number : 086, Training: Loss:  0.4631, Accuracy: 0.8594\n",
      "Batch number : 087, Training: Loss:  0.3393, Accuracy: 0.8906\n",
      "Batch number : 088, Training: Loss:  0.2762, Accuracy: 0.9375\n",
      "Batch number : 089, Training: Loss:  0.2896, Accuracy: 0.9062\n",
      "Batch number : 090, Training: Loss:  0.5616, Accuracy: 0.8281\n",
      "Batch number : 091, Training: Loss:  0.3790, Accuracy: 0.8750\n",
      "Batch number : 092, Training: Loss:  0.5805, Accuracy: 0.8281\n",
      "Batch number : 093, Training: Loss:  0.5091, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.2985, Accuracy: 0.9062\n",
      "Batch number : 095, Training: Loss:  0.5135, Accuracy: 0.8594\n",
      "Batch number : 096, Training: Loss:  0.7616, Accuracy: 0.7969\n",
      "Batch number : 097, Training: Loss:  0.5649, Accuracy: 0.7969\n",
      "Batch number : 098, Training: Loss:  0.6414, Accuracy: 0.7812\n",
      "Batch number : 099, Training: Loss:  0.4336, Accuracy: 0.8750\n",
      "Batch number : 100, Training: Loss:  0.4882, Accuracy: 0.8438\n",
      "Batch number : 101, Training: Loss:  0.3408, Accuracy: 0.9062\n",
      "Batch number : 102, Training: Loss:  0.3912, Accuracy: 0.9062\n",
      "Batch number : 103, Training: Loss:  0.4772, Accuracy: 0.8594\n",
      "Batch number : 104, Training: Loss:  0.4603, Accuracy: 0.8750\n",
      "Batch number : 105, Training: Loss:  0.4722, Accuracy: 0.8438\n",
      "Batch number : 106, Training: Loss:  0.4424, Accuracy: 0.8906\n",
      "Batch number : 107, Training: Loss:  0.2750, Accuracy: 0.9219\n",
      "Batch number : 108, Training: Loss:  0.6047, Accuracy: 0.8438\n",
      "Batch number : 109, Training: Loss:  0.5083, Accuracy: 0.8594\n",
      "Batch number : 110, Training: Loss:  0.5293, Accuracy: 0.8125\n",
      "Batch number : 111, Training: Loss:  0.5405, Accuracy: 0.8125\n",
      "Batch number : 112, Training: Loss:  0.4583, Accuracy: 0.8906\n",
      "Batch number : 113, Training: Loss:  0.4715, Accuracy: 0.8438\n",
      "Batch number : 114, Training: Loss:  0.3108, Accuracy: 0.9062\n",
      "Batch number : 115, Training: Loss:  0.2947, Accuracy: 0.9531\n",
      "Batch number : 116, Training: Loss:  0.6115, Accuracy: 0.8281\n",
      "Batch number : 117, Training: Loss:  0.4652, Accuracy: 0.8594\n",
      "Batch number : 118, Training: Loss:  0.6396, Accuracy: 0.7812\n",
      "Batch number : 119, Training: Loss:  0.7849, Accuracy: 0.7500\n",
      "Batch number : 120, Training: Loss:  0.5455, Accuracy: 0.8438\n",
      "Batch number : 121, Training: Loss:  0.4528, Accuracy: 0.8438\n",
      "Batch number : 122, Training: Loss:  0.3952, Accuracy: 0.9062\n",
      "Batch number : 123, Training: Loss:  0.6132, Accuracy: 0.8281\n",
      "Batch number : 124, Training: Loss:  0.4821, Accuracy: 0.8750\n",
      "Batch number : 125, Training: Loss:  0.6461, Accuracy: 0.7500\n",
      "Batch number : 126, Training: Loss:  0.4992, Accuracy: 0.8750\n",
      "Batch number : 127, Training: Loss:  0.6178, Accuracy: 0.8125\n",
      "Batch number : 128, Training: Loss:  0.5168, Accuracy: 0.8281\n",
      "Batch number : 129, Training: Loss:  0.6500, Accuracy: 0.7969\n",
      "Batch number : 130, Training: Loss:  0.4195, Accuracy: 0.8750\n",
      "Batch number : 131, Training: Loss:  0.4618, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.4251, Accuracy: 0.8594\n",
      "Batch number : 133, Training: Loss:  0.4305, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.4721, Accuracy: 0.8594\n",
      "Batch number : 135, Training: Loss:  0.3507, Accuracy: 0.9062\n",
      "Batch number : 136, Training: Loss:  0.5785, Accuracy: 0.8125\n",
      "Batch number : 137, Training: Loss:  0.5810, Accuracy: 0.8281\n",
      "Batch number : 138, Training: Loss:  0.3221, Accuracy: 0.8906\n",
      "Batch number : 139, Training: Loss:  0.5417, Accuracy: 0.8281\n",
      "Batch number : 140, Training: Loss:  0.5991, Accuracy: 0.8281\n",
      "Batch number : 141, Training: Loss:  0.4600, Accuracy: 0.8281\n",
      "Batch number : 142, Training: Loss:  0.3947, Accuracy: 0.8750\n",
      "Batch number : 143, Training: Loss:  0.4164, Accuracy: 0.8750\n",
      "Batch number : 144, Training: Loss:  0.3170, Accuracy: 0.9375\n",
      "Batch number : 145, Training: Loss:  0.4037, Accuracy: 0.8750\n",
      "Batch number : 146, Training: Loss:  0.4613, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.6645, Accuracy: 0.7344\n",
      "Batch number : 148, Training: Loss:  0.4880, Accuracy: 0.8281\n",
      "Batch number : 149, Training: Loss:  0.2762, Accuracy: 0.9375\n",
      "Batch number : 150, Training: Loss:  0.3994, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.4189, Accuracy: 0.8594\n",
      "Batch number : 152, Training: Loss:  0.4391, Accuracy: 0.9062\n",
      "Batch number : 153, Training: Loss:  0.3824, Accuracy: 0.8906\n",
      "Batch number : 154, Training: Loss:  0.3908, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.6613, Accuracy: 0.7812\n",
      "Batch number : 156, Training: Loss:  0.6533, Accuracy: 0.8281\n",
      "Batch number : 157, Training: Loss:  0.6003, Accuracy: 0.8125\n",
      "Batch number : 158, Training: Loss:  0.4467, Accuracy: 0.8750\n",
      "Batch number : 159, Training: Loss:  0.4055, Accuracy: 0.8438\n",
      "Batch number : 160, Training: Loss:  0.5071, Accuracy: 0.8594\n",
      "Batch number : 161, Training: Loss:  0.5068, Accuracy: 0.8438\n",
      "Batch number : 162, Training: Loss:  0.7104, Accuracy: 0.7656\n",
      "Batch number : 163, Training: Loss:  0.5074, Accuracy: 0.8438\n",
      "Batch number : 164, Training: Loss:  0.4959, Accuracy: 0.8438\n",
      "Batch number : 165, Training: Loss:  0.3670, Accuracy: 0.9219\n",
      "Batch number : 166, Training: Loss:  0.4938, Accuracy: 0.8438\n",
      "Batch number : 167, Training: Loss:  0.6059, Accuracy: 0.8594\n",
      "Batch number : 168, Training: Loss:  0.4166, Accuracy: 0.9062\n",
      "Batch number : 169, Training: Loss:  0.4979, Accuracy: 0.8438\n",
      "Batch number : 170, Training: Loss:  0.4262, Accuracy: 0.8750\n",
      "Batch number : 171, Training: Loss:  0.4574, Accuracy: 0.8594\n",
      "Batch number : 172, Training: Loss:  0.4251, Accuracy: 0.8906\n",
      "Batch number : 173, Training: Loss:  0.5557, Accuracy: 0.8125\n",
      "Batch number : 174, Training: Loss:  0.5605, Accuracy: 0.7812\n",
      "Batch number : 175, Training: Loss:  0.2778, Accuracy: 0.9219\n",
      "Batch number : 176, Training: Loss:  0.5644, Accuracy: 0.8281\n",
      "Batch number : 177, Training: Loss:  0.4444, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.6090, Accuracy: 0.8594\n",
      "Batch number : 179, Training: Loss:  0.4997, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.3352, Accuracy: 0.9062\n",
      "Batch number : 181, Training: Loss:  0.4743, Accuracy: 0.8594\n",
      "Batch number : 182, Training: Loss:  0.4921, Accuracy: 0.8750\n",
      "Batch number : 183, Training: Loss:  0.4752, Accuracy: 0.8438\n",
      "Batch number : 184, Training: Loss:  0.5253, Accuracy: 0.8281\n",
      "Batch number : 185, Training: Loss:  0.4613, Accuracy: 0.8594\n",
      "Batch number : 186, Training: Loss:  0.3326, Accuracy: 0.9375\n",
      "Batch number : 187, Training: Loss:  0.5470, Accuracy: 0.8281\n",
      "Batch number : 188, Training: Loss:  0.3385, Accuracy: 0.8906\n",
      "Batch number : 189, Training: Loss:  0.5655, Accuracy: 0.8125\n",
      "Batch number : 190, Training: Loss:  0.5253, Accuracy: 0.8281\n",
      "Batch number : 191, Training: Loss:  0.5324, Accuracy: 0.8438\n",
      "Batch number : 192, Training: Loss:  0.4952, Accuracy: 0.8750\n",
      "Batch number : 193, Training: Loss:  0.7710, Accuracy: 0.7344\n",
      "Batch number : 194, Training: Loss:  0.7147, Accuracy: 0.7812\n",
      "Batch number : 195, Training: Loss:  0.5166, Accuracy: 0.8281\n",
      "Batch number : 196, Training: Loss:  0.5072, Accuracy: 0.8438\n",
      "Batch number : 197, Training: Loss:  0.5761, Accuracy: 0.8281\n",
      "Batch number : 198, Training: Loss:  0.3492, Accuracy: 0.9375\n",
      "Batch number : 199, Training: Loss:  0.5094, Accuracy: 0.8281\n",
      "Batch number : 200, Training: Loss:  0.5937, Accuracy: 0.8438\n",
      "Batch number : 201, Training: Loss:  0.6243, Accuracy: 0.8281\n",
      "Batch number : 202, Training: Loss:  0.3348, Accuracy: 0.8438\n",
      "Batch number : 203, Training: Loss:  0.3866, Accuracy: 0.8906\n",
      "Batch number : 204, Training: Loss:  0.5228, Accuracy: 0.8594\n",
      "Batch number : 205, Training: Loss:  0.6405, Accuracy: 0.8125\n",
      "Batch number : 206, Training: Loss:  0.4283, Accuracy: 0.8906\n",
      "Batch number : 207, Training: Loss:  0.3409, Accuracy: 0.9062\n",
      "Batch number : 208, Training: Loss:  0.4254, Accuracy: 0.8750\n",
      "Batch number : 209, Training: Loss:  0.5788, Accuracy: 0.8125\n",
      "Batch number : 210, Training: Loss:  0.5819, Accuracy: 0.8281\n",
      "Batch number : 211, Training: Loss:  0.4945, Accuracy: 0.8594\n",
      "Batch number : 212, Training: Loss:  0.4358, Accuracy: 0.9375\n",
      "Batch number : 213, Training: Loss:  0.3285, Accuracy: 0.9062\n",
      "Batch number : 214, Training: Loss:  0.3872, Accuracy: 0.8906\n",
      "Batch number : 215, Training: Loss:  0.4703, Accuracy: 0.8594\n",
      "Batch number : 216, Training: Loss:  0.3571, Accuracy: 0.8906\n",
      "Batch number : 217, Training: Loss:  0.3913, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.5238, Accuracy: 0.8281\n",
      "Batch number : 219, Training: Loss:  0.6731, Accuracy: 0.7812\n",
      "Batch number : 220, Training: Loss:  0.5225, Accuracy: 0.8281\n",
      "Batch number : 221, Training: Loss:  0.5180, Accuracy: 0.8594\n",
      "Batch number : 222, Training: Loss:  0.5620, Accuracy: 0.8281\n",
      "Batch number : 223, Training: Loss:  0.3782, Accuracy: 0.9062\n",
      "Batch number : 224, Training: Loss:  0.5790, Accuracy: 0.8281\n",
      "Batch number : 225, Training: Loss:  0.7959, Accuracy: 0.7031\n",
      "Batch number : 226, Training: Loss:  0.5136, Accuracy: 0.8438\n",
      "Batch number : 227, Training: Loss:  0.6067, Accuracy: 0.7969\n",
      "Batch number : 228, Training: Loss:  0.4903, Accuracy: 0.8281\n",
      "Batch number : 229, Training: Loss:  0.4768, Accuracy: 0.8906\n",
      "Batch number : 230, Training: Loss:  0.5751, Accuracy: 0.8281\n",
      "Batch number : 231, Training: Loss:  0.4485, Accuracy: 0.8594\n",
      "Batch number : 232, Training: Loss:  0.4616, Accuracy: 0.8125\n",
      "Batch number : 233, Training: Loss:  0.4215, Accuracy: 0.8750\n",
      "Batch number : 234, Training: Loss:  0.3711, Accuracy: 0.9062\n",
      "Batch number : 235, Training: Loss:  0.4807, Accuracy: 0.8438\n",
      "Batch number : 236, Training: Loss:  0.4529, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.2727, Accuracy: 0.9062\n",
      "Batch number : 238, Training: Loss:  0.4121, Accuracy: 0.8906\n",
      "Batch number : 239, Training: Loss:  0.3194, Accuracy: 0.8750\n",
      "Batch number : 240, Training: Loss:  0.3841, Accuracy: 0.9062\n",
      "Batch number : 241, Training: Loss:  0.4979, Accuracy: 0.8594\n",
      "Batch number : 242, Training: Loss:  0.3060, Accuracy: 0.9375\n",
      "Batch number : 243, Training: Loss:  0.6724, Accuracy: 0.8281\n",
      "Batch number : 244, Training: Loss:  0.3375, Accuracy: 0.9219\n",
      "Batch number : 245, Training: Loss:  0.4417, Accuracy: 0.8906\n",
      "Batch number : 246, Training: Loss:  0.3896, Accuracy: 0.9062\n",
      "Batch number : 247, Training: Loss:  0.4861, Accuracy: 0.8438\n",
      "Batch number : 248, Training: Loss:  0.3942, Accuracy: 0.8906\n",
      "Batch number : 249, Training: Loss:  0.3483, Accuracy: 0.9062\n",
      "Batch number : 250, Training: Loss:  0.3113, Accuracy: 0.9219\n",
      "Batch number : 251, Training: Loss:  0.4578, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.3831, Accuracy: 0.8750\n",
      "Batch number : 253, Training: Loss:  0.4287, Accuracy: 0.8438\n",
      "Batch number : 254, Training: Loss:  0.5761, Accuracy: 0.8438\n",
      "Batch number : 255, Training: Loss:  0.5039, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.4841, Accuracy: 0.8594\n",
      "Batch number : 257, Training: Loss:  0.4350, Accuracy: 0.8594\n",
      "Batch number : 258, Training: Loss:  0.4336, Accuracy: 0.8438\n",
      "Batch number : 259, Training: Loss:  0.5746, Accuracy: 0.7969\n",
      "Batch number : 260, Training: Loss:  0.2940, Accuracy: 0.9219\n",
      "Batch number : 261, Training: Loss:  0.3474, Accuracy: 0.8906\n",
      "Batch number : 262, Training: Loss:  0.3922, Accuracy: 0.8750\n",
      "Batch number : 263, Training: Loss:  0.3478, Accuracy: 0.8906\n",
      "Batch number : 264, Training: Loss:  0.4947, Accuracy: 0.8750\n",
      "Batch number : 265, Training: Loss:  0.3082, Accuracy: 0.8906\n",
      "Batch number : 266, Training: Loss:  0.2482, Accuracy: 0.9219\n",
      "Batch number : 267, Training: Loss:  0.3766, Accuracy: 0.8906\n",
      "Batch number : 268, Training: Loss:  0.4247, Accuracy: 0.8906\n",
      "Batch number : 269, Training: Loss:  0.2682, Accuracy: 0.9531\n",
      "Batch number : 270, Training: Loss:  0.4075, Accuracy: 0.9062\n",
      "Batch number : 271, Training: Loss:  0.4441, Accuracy: 0.8594\n",
      "Batch number : 272, Training: Loss:  0.4353, Accuracy: 0.9219\n",
      "Batch number : 273, Training: Loss:  0.6015, Accuracy: 0.8281\n",
      "Batch number : 274, Training: Loss:  0.4317, Accuracy: 0.8906\n",
      "Batch number : 275, Training: Loss:  0.4068, Accuracy: 0.8594\n",
      "Batch number : 276, Training: Loss:  0.5059, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.6457, Accuracy: 0.8281\n",
      "Batch number : 278, Training: Loss:  0.5163, Accuracy: 0.8594\n",
      "Batch number : 279, Training: Loss:  0.7037, Accuracy: 0.7500\n",
      "Batch number : 280, Training: Loss:  0.5968, Accuracy: 0.8125\n",
      "Batch number : 281, Training: Loss:  0.5189, Accuracy: 0.7969\n",
      "Batch number : 282, Training: Loss:  0.6816, Accuracy: 0.7812\n",
      "Batch number : 283, Training: Loss:  0.5001, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.7586, Accuracy: 0.7656\n",
      "Batch number : 285, Training: Loss:  0.4064, Accuracy: 0.8750\n",
      "Batch number : 286, Training: Loss:  0.3607, Accuracy: 0.9219\n",
      "Batch number : 287, Training: Loss:  0.4151, Accuracy: 0.8594\n",
      "Batch number : 288, Training: Loss:  0.4748, Accuracy: 0.8750\n",
      "Batch number : 289, Training: Loss:  0.5767, Accuracy: 0.8281\n",
      "Batch number : 290, Training: Loss:  0.4346, Accuracy: 0.8906\n",
      "Batch number : 291, Training: Loss:  0.5877, Accuracy: 0.8281\n",
      "Batch number : 292, Training: Loss:  0.5132, Accuracy: 0.8281\n",
      "Batch number : 293, Training: Loss:  0.3673, Accuracy: 0.8750\n",
      "Batch number : 294, Training: Loss:  0.3658, Accuracy: 0.8906\n",
      "Batch number : 295, Training: Loss:  0.3354, Accuracy: 0.8750\n",
      "Batch number : 296, Training: Loss:  0.3544, Accuracy: 0.8750\n",
      "Batch number : 297, Training: Loss:  0.5101, Accuracy: 0.8594\n",
      "Batch number : 298, Training: Loss:  0.6943, Accuracy: 0.7969\n",
      "Batch number : 299, Training: Loss:  0.6029, Accuracy: 0.8438\n",
      "Batch number : 300, Training: Loss:  0.3619, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.4912, Accuracy: 0.8438\n",
      "Batch number : 302, Training: Loss:  0.7032, Accuracy: 0.7500\n",
      "Batch number : 303, Training: Loss:  0.5919, Accuracy: 0.7656\n",
      "Batch number : 304, Training: Loss:  0.3996, Accuracy: 0.8750\n",
      "Batch number : 305, Training: Loss:  0.3748, Accuracy: 0.9219\n",
      "Batch number : 306, Training: Loss:  0.5046, Accuracy: 0.8438\n",
      "Batch number : 307, Training: Loss:  0.3995, Accuracy: 0.8750\n",
      "Batch number : 308, Training: Loss:  0.3470, Accuracy: 0.8906\n",
      "Batch number : 309, Training: Loss:  0.4745, Accuracy: 0.8594\n",
      "Batch number : 310, Training: Loss:  0.5074, Accuracy: 0.8438\n",
      "Batch number : 311, Training: Loss:  0.5786, Accuracy: 0.7969\n",
      "Batch number : 312, Training: Loss:  0.4820, Accuracy: 0.8438\n",
      "Batch number : 313, Training: Loss:  0.5444, Accuracy: 0.8594\n",
      "Batch number : 314, Training: Loss:  0.3568, Accuracy: 0.8906\n",
      "Batch number : 315, Training: Loss:  0.3637, Accuracy: 0.9219\n",
      "Batch number : 316, Training: Loss:  0.3920, Accuracy: 0.8906\n",
      "Batch number : 317, Training: Loss:  0.5795, Accuracy: 0.8750\n",
      "Batch number : 318, Training: Loss:  0.4488, Accuracy: 0.8125\n",
      "Batch number : 319, Training: Loss:  0.3862, Accuracy: 0.9219\n",
      "Batch number : 320, Training: Loss:  0.5750, Accuracy: 0.8594\n",
      "Batch number : 321, Training: Loss:  0.3168, Accuracy: 0.9062\n",
      "Batch number : 322, Training: Loss:  0.3189, Accuracy: 0.8906\n",
      "Batch number : 323, Training: Loss:  0.3647, Accuracy: 0.8750\n",
      "Batch number : 324, Training: Loss:  0.6293, Accuracy: 0.7969\n",
      "Batch number : 325, Training: Loss:  0.6857, Accuracy: 0.7969\n",
      "Batch number : 326, Training: Loss:  0.4864, Accuracy: 0.8594\n",
      "Batch number : 327, Training: Loss:  0.2832, Accuracy: 0.9062\n",
      "Batch number : 328, Training: Loss:  0.5843, Accuracy: 0.8125\n",
      "Batch number : 329, Training: Loss:  0.4816, Accuracy: 0.8438\n",
      "Batch number : 330, Training: Loss:  0.4659, Accuracy: 0.8750\n",
      "Batch number : 331, Training: Loss:  0.6202, Accuracy: 0.7969\n",
      "Batch number : 332, Training: Loss:  0.3283, Accuracy: 0.9219\n",
      "Batch number : 333, Training: Loss:  0.5074, Accuracy: 0.8906\n",
      "Batch number : 334, Training: Loss:  0.3226, Accuracy: 0.9219\n",
      "Batch number : 335, Training: Loss:  0.6217, Accuracy: 0.8125\n",
      "Batch number : 336, Training: Loss:  0.3575, Accuracy: 0.9219\n",
      "Batch number : 337, Training: Loss:  0.5581, Accuracy: 0.8438\n",
      "Batch number : 338, Training: Loss:  0.3415, Accuracy: 0.8906\n",
      "Batch number : 339, Training: Loss:  0.3020, Accuracy: 0.9062\n",
      "Batch number : 340, Training: Loss:  0.2845, Accuracy: 0.9219\n",
      "Batch number : 341, Training: Loss:  0.4040, Accuracy: 0.8906\n",
      "Batch number : 342, Training: Loss:  0.4562, Accuracy: 0.8281\n",
      "Batch number : 343, Training: Loss:  0.4538, Accuracy: 0.9062\n",
      "Batch number : 344, Training: Loss:  0.9859, Accuracy: 0.7500\n",
      "Batch number : 345, Training: Loss:  0.4228, Accuracy: 0.8594\n",
      "Batch number : 346, Training: Loss:  0.3535, Accuracy: 0.9219\n",
      "Batch number : 347, Training: Loss:  0.6719, Accuracy: 0.7500\n",
      "Batch number : 348, Training: Loss:  0.4754, Accuracy: 0.8594\n",
      "Batch number : 349, Training: Loss:  0.3191, Accuracy: 0.9219\n",
      "Batch number : 350, Training: Loss:  0.6378, Accuracy: 0.8281\n",
      "Batch number : 351, Training: Loss:  0.4452, Accuracy: 0.9062\n",
      "Batch number : 352, Training: Loss:  0.4722, Accuracy: 0.8281\n",
      "Batch number : 353, Training: Loss:  0.5579, Accuracy: 0.7969\n",
      "Batch number : 354, Training: Loss:  0.4051, Accuracy: 0.8906\n",
      "Batch number : 355, Training: Loss:  0.4151, Accuracy: 0.8906\n",
      "Batch number : 356, Training: Loss:  0.2925, Accuracy: 0.9219\n",
      "Batch number : 357, Training: Loss:  0.3933, Accuracy: 0.9062\n",
      "Batch number : 358, Training: Loss:  0.3806, Accuracy: 0.8750\n",
      "Batch number : 359, Training: Loss:  0.5719, Accuracy: 0.7969\n",
      "Batch number : 360, Training: Loss:  0.4412, Accuracy: 0.8594\n",
      "Batch number : 361, Training: Loss:  0.5369, Accuracy: 0.9062\n",
      "Batch number : 362, Training: Loss:  0.7077, Accuracy: 0.8281\n",
      "Batch number : 363, Training: Loss:  0.5247, Accuracy: 0.8125\n",
      "Batch number : 364, Training: Loss:  0.6112, Accuracy: 0.8281\n",
      "Batch number : 365, Training: Loss:  0.4537, Accuracy: 0.8750\n",
      "Batch number : 366, Training: Loss:  0.5124, Accuracy: 0.8594\n",
      "Batch number : 367, Training: Loss:  0.3978, Accuracy: 0.8750\n",
      "Batch number : 368, Training: Loss:  0.4590, Accuracy: 0.8125\n",
      "Batch number : 369, Training: Loss:  0.3773, Accuracy: 0.9062\n",
      "Batch number : 370, Training: Loss:  0.4228, Accuracy: 0.9219\n",
      "Batch number : 371, Training: Loss:  0.5291, Accuracy: 0.9062\n",
      "Batch number : 372, Training: Loss:  0.4817, Accuracy: 0.8594\n",
      "Batch number : 373, Training: Loss:  0.4844, Accuracy: 0.8281\n",
      "Batch number : 374, Training: Loss:  0.3706, Accuracy: 0.9062\n",
      "Batch number : 375, Training: Loss:  0.5969, Accuracy: 0.8125\n",
      "Batch number : 376, Training: Loss:  0.4434, Accuracy: 0.8281\n",
      "Batch number : 377, Training: Loss:  0.5057, Accuracy: 0.8750\n",
      "Epoch: 17/20\n",
      "Batch number : 000, Training: Loss:  0.3870, Accuracy: 0.8750\n",
      "Batch number : 001, Training: Loss:  0.2170, Accuracy: 0.9531\n",
      "Batch number : 002, Training: Loss:  0.3268, Accuracy: 0.9219\n",
      "Batch number : 003, Training: Loss:  0.8004, Accuracy: 0.7969\n",
      "Batch number : 004, Training: Loss:  0.5063, Accuracy: 0.8125\n",
      "Batch number : 005, Training: Loss:  0.5097, Accuracy: 0.8281\n",
      "Batch number : 006, Training: Loss:  0.4279, Accuracy: 0.8594\n",
      "Batch number : 007, Training: Loss:  0.4429, Accuracy: 0.8906\n",
      "Batch number : 008, Training: Loss:  0.5237, Accuracy: 0.8438\n",
      "Batch number : 009, Training: Loss:  0.3376, Accuracy: 0.8750\n",
      "Batch number : 010, Training: Loss:  0.3018, Accuracy: 0.9062\n",
      "Batch number : 011, Training: Loss:  0.3281, Accuracy: 0.8906\n",
      "Batch number : 012, Training: Loss:  0.3666, Accuracy: 0.8906\n",
      "Batch number : 013, Training: Loss:  0.2635, Accuracy: 0.9375\n",
      "Batch number : 014, Training: Loss:  0.6247, Accuracy: 0.7812\n",
      "Batch number : 015, Training: Loss:  0.2693, Accuracy: 0.9375\n",
      "Batch number : 016, Training: Loss:  0.5471, Accuracy: 0.8281\n",
      "Batch number : 017, Training: Loss:  0.5597, Accuracy: 0.8125\n",
      "Batch number : 018, Training: Loss:  0.5391, Accuracy: 0.8125\n",
      "Batch number : 019, Training: Loss:  0.3475, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.8657, Accuracy: 0.7812\n",
      "Batch number : 021, Training: Loss:  0.4834, Accuracy: 0.8594\n",
      "Batch number : 022, Training: Loss:  0.7039, Accuracy: 0.7812\n",
      "Batch number : 023, Training: Loss:  0.4010, Accuracy: 0.8750\n",
      "Batch number : 024, Training: Loss:  0.7035, Accuracy: 0.7500\n",
      "Batch number : 025, Training: Loss:  0.5793, Accuracy: 0.8281\n",
      "Batch number : 026, Training: Loss:  0.3663, Accuracy: 0.9375\n",
      "Batch number : 027, Training: Loss:  0.5564, Accuracy: 0.8438\n",
      "Batch number : 028, Training: Loss:  0.4012, Accuracy: 0.9062\n",
      "Batch number : 029, Training: Loss:  0.5717, Accuracy: 0.8125\n",
      "Batch number : 030, Training: Loss:  0.3775, Accuracy: 0.8906\n",
      "Batch number : 031, Training: Loss:  0.4193, Accuracy: 0.8906\n",
      "Batch number : 032, Training: Loss:  0.4920, Accuracy: 0.8594\n",
      "Batch number : 033, Training: Loss:  0.4509, Accuracy: 0.8750\n",
      "Batch number : 034, Training: Loss:  0.4828, Accuracy: 0.8438\n",
      "Batch number : 035, Training: Loss:  0.5255, Accuracy: 0.8125\n",
      "Batch number : 036, Training: Loss:  0.2282, Accuracy: 0.9531\n",
      "Batch number : 037, Training: Loss:  0.7217, Accuracy: 0.7656\n",
      "Batch number : 038, Training: Loss:  0.4380, Accuracy: 0.8750\n",
      "Batch number : 039, Training: Loss:  0.8741, Accuracy: 0.7344\n",
      "Batch number : 040, Training: Loss:  0.3886, Accuracy: 0.8906\n",
      "Batch number : 041, Training: Loss:  0.4227, Accuracy: 0.8594\n",
      "Batch number : 042, Training: Loss:  0.3007, Accuracy: 0.9375\n",
      "Batch number : 043, Training: Loss:  0.3054, Accuracy: 0.9062\n",
      "Batch number : 044, Training: Loss:  0.4794, Accuracy: 0.8750\n",
      "Batch number : 045, Training: Loss:  0.3953, Accuracy: 0.8906\n",
      "Batch number : 046, Training: Loss:  0.2858, Accuracy: 0.9062\n",
      "Batch number : 047, Training: Loss:  0.4460, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.2958, Accuracy: 0.9219\n",
      "Batch number : 049, Training: Loss:  0.3993, Accuracy: 0.8438\n",
      "Batch number : 050, Training: Loss:  0.4441, Accuracy: 0.8594\n",
      "Batch number : 051, Training: Loss:  0.3983, Accuracy: 0.8750\n",
      "Batch number : 052, Training: Loss:  0.5642, Accuracy: 0.7969\n",
      "Batch number : 053, Training: Loss:  0.4940, Accuracy: 0.8594\n",
      "Batch number : 054, Training: Loss:  0.6525, Accuracy: 0.8281\n",
      "Batch number : 055, Training: Loss:  0.4715, Accuracy: 0.8438\n",
      "Batch number : 056, Training: Loss:  0.6976, Accuracy: 0.7969\n",
      "Batch number : 057, Training: Loss:  0.3322, Accuracy: 0.9219\n",
      "Batch number : 058, Training: Loss:  0.4803, Accuracy: 0.8281\n",
      "Batch number : 059, Training: Loss:  0.4006, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.5697, Accuracy: 0.8438\n",
      "Batch number : 061, Training: Loss:  0.5366, Accuracy: 0.8281\n",
      "Batch number : 062, Training: Loss:  0.4816, Accuracy: 0.8438\n",
      "Batch number : 063, Training: Loss:  0.3658, Accuracy: 0.8594\n",
      "Batch number : 064, Training: Loss:  0.6007, Accuracy: 0.8125\n",
      "Batch number : 065, Training: Loss:  0.3697, Accuracy: 0.9375\n",
      "Batch number : 066, Training: Loss:  0.4696, Accuracy: 0.8281\n",
      "Batch number : 067, Training: Loss:  0.5314, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.2396, Accuracy: 0.9219\n",
      "Batch number : 069, Training: Loss:  0.4762, Accuracy: 0.8594\n",
      "Batch number : 070, Training: Loss:  0.2585, Accuracy: 0.9375\n",
      "Batch number : 071, Training: Loss:  0.4709, Accuracy: 0.8594\n",
      "Batch number : 072, Training: Loss:  0.4778, Accuracy: 0.8438\n",
      "Batch number : 073, Training: Loss:  0.4575, Accuracy: 0.8906\n",
      "Batch number : 074, Training: Loss:  0.5530, Accuracy: 0.8906\n",
      "Batch number : 075, Training: Loss:  0.8030, Accuracy: 0.7969\n",
      "Batch number : 076, Training: Loss:  0.6264, Accuracy: 0.8125\n",
      "Batch number : 077, Training: Loss:  0.4642, Accuracy: 0.8281\n",
      "Batch number : 078, Training: Loss:  0.4431, Accuracy: 0.8750\n",
      "Batch number : 079, Training: Loss:  0.4212, Accuracy: 0.8750\n",
      "Batch number : 080, Training: Loss:  0.5439, Accuracy: 0.8438\n",
      "Batch number : 081, Training: Loss:  0.4605, Accuracy: 0.8750\n",
      "Batch number : 082, Training: Loss:  0.4332, Accuracy: 0.8906\n",
      "Batch number : 083, Training: Loss:  0.3408, Accuracy: 0.9219\n",
      "Batch number : 084, Training: Loss:  0.4803, Accuracy: 0.8906\n",
      "Batch number : 085, Training: Loss:  0.5383, Accuracy: 0.8125\n",
      "Batch number : 086, Training: Loss:  0.5602, Accuracy: 0.8750\n",
      "Batch number : 087, Training: Loss:  0.4709, Accuracy: 0.8594\n",
      "Batch number : 088, Training: Loss:  0.4790, Accuracy: 0.8750\n",
      "Batch number : 089, Training: Loss:  0.3083, Accuracy: 0.9062\n",
      "Batch number : 090, Training: Loss:  0.3202, Accuracy: 0.9219\n",
      "Batch number : 091, Training: Loss:  0.4511, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.5026, Accuracy: 0.8281\n",
      "Batch number : 093, Training: Loss:  0.6262, Accuracy: 0.8281\n",
      "Batch number : 094, Training: Loss:  0.4011, Accuracy: 0.8906\n",
      "Batch number : 095, Training: Loss:  0.4271, Accuracy: 0.8750\n",
      "Batch number : 096, Training: Loss:  0.5762, Accuracy: 0.8125\n",
      "Batch number : 097, Training: Loss:  0.3577, Accuracy: 0.9062\n",
      "Batch number : 098, Training: Loss:  0.2993, Accuracy: 0.9062\n",
      "Batch number : 099, Training: Loss:  0.6856, Accuracy: 0.8281\n",
      "Batch number : 100, Training: Loss:  0.4359, Accuracy: 0.8906\n",
      "Batch number : 101, Training: Loss:  0.5735, Accuracy: 0.8438\n",
      "Batch number : 102, Training: Loss:  0.3704, Accuracy: 0.9219\n",
      "Batch number : 103, Training: Loss:  0.6139, Accuracy: 0.7969\n",
      "Batch number : 104, Training: Loss:  0.3967, Accuracy: 0.8906\n",
      "Batch number : 105, Training: Loss:  0.7875, Accuracy: 0.7500\n",
      "Batch number : 106, Training: Loss:  0.5457, Accuracy: 0.8438\n",
      "Batch number : 107, Training: Loss:  0.6567, Accuracy: 0.7656\n",
      "Batch number : 108, Training: Loss:  0.5852, Accuracy: 0.8594\n",
      "Batch number : 109, Training: Loss:  0.4737, Accuracy: 0.8438\n",
      "Batch number : 110, Training: Loss:  0.3573, Accuracy: 0.9375\n",
      "Batch number : 111, Training: Loss:  0.3278, Accuracy: 0.9375\n",
      "Batch number : 112, Training: Loss:  0.4872, Accuracy: 0.8438\n",
      "Batch number : 113, Training: Loss:  0.3505, Accuracy: 0.9219\n",
      "Batch number : 114, Training: Loss:  0.5866, Accuracy: 0.8438\n",
      "Batch number : 115, Training: Loss:  0.4960, Accuracy: 0.8438\n",
      "Batch number : 116, Training: Loss:  0.5256, Accuracy: 0.8438\n",
      "Batch number : 117, Training: Loss:  0.4703, Accuracy: 0.8438\n",
      "Batch number : 118, Training: Loss:  0.3226, Accuracy: 0.9219\n",
      "Batch number : 119, Training: Loss:  0.5525, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.4025, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.3134, Accuracy: 0.8906\n",
      "Batch number : 122, Training: Loss:  0.3715, Accuracy: 0.8906\n",
      "Batch number : 123, Training: Loss:  0.6056, Accuracy: 0.8594\n",
      "Batch number : 124, Training: Loss:  0.3340, Accuracy: 0.9062\n",
      "Batch number : 125, Training: Loss:  0.4485, Accuracy: 0.8594\n",
      "Batch number : 126, Training: Loss:  0.4437, Accuracy: 0.8906\n",
      "Batch number : 127, Training: Loss:  0.3498, Accuracy: 0.9219\n",
      "Batch number : 128, Training: Loss:  0.3255, Accuracy: 0.8906\n",
      "Batch number : 129, Training: Loss:  0.3902, Accuracy: 0.9219\n",
      "Batch number : 130, Training: Loss:  0.3820, Accuracy: 0.8906\n",
      "Batch number : 131, Training: Loss:  0.5038, Accuracy: 0.8438\n",
      "Batch number : 132, Training: Loss:  0.4096, Accuracy: 0.8750\n",
      "Batch number : 133, Training: Loss:  0.8061, Accuracy: 0.7656\n",
      "Batch number : 134, Training: Loss:  0.5979, Accuracy: 0.7812\n",
      "Batch number : 135, Training: Loss:  0.5078, Accuracy: 0.8750\n",
      "Batch number : 136, Training: Loss:  0.4784, Accuracy: 0.8750\n",
      "Batch number : 137, Training: Loss:  0.4774, Accuracy: 0.8594\n",
      "Batch number : 138, Training: Loss:  0.3951, Accuracy: 0.8281\n",
      "Batch number : 139, Training: Loss:  0.4169, Accuracy: 0.8906\n",
      "Batch number : 140, Training: Loss:  0.4168, Accuracy: 0.8594\n",
      "Batch number : 141, Training: Loss:  0.4755, Accuracy: 0.8281\n",
      "Batch number : 142, Training: Loss:  0.4668, Accuracy: 0.8281\n",
      "Batch number : 143, Training: Loss:  0.4282, Accuracy: 0.8594\n",
      "Batch number : 144, Training: Loss:  0.3028, Accuracy: 0.9375\n",
      "Batch number : 145, Training: Loss:  0.5573, Accuracy: 0.8750\n",
      "Batch number : 146, Training: Loss:  0.4182, Accuracy: 0.8594\n",
      "Batch number : 147, Training: Loss:  0.4925, Accuracy: 0.8594\n",
      "Batch number : 148, Training: Loss:  0.4983, Accuracy: 0.8125\n",
      "Batch number : 149, Training: Loss:  0.3615, Accuracy: 0.8906\n",
      "Batch number : 150, Training: Loss:  0.5092, Accuracy: 0.8750\n",
      "Batch number : 151, Training: Loss:  0.7349, Accuracy: 0.7031\n",
      "Batch number : 152, Training: Loss:  0.4235, Accuracy: 0.9062\n",
      "Batch number : 153, Training: Loss:  0.7262, Accuracy: 0.7969\n",
      "Batch number : 154, Training: Loss:  0.4151, Accuracy: 0.8750\n",
      "Batch number : 155, Training: Loss:  0.4860, Accuracy: 0.8594\n",
      "Batch number : 156, Training: Loss:  0.3823, Accuracy: 0.8906\n",
      "Batch number : 157, Training: Loss:  0.3124, Accuracy: 0.9531\n",
      "Batch number : 158, Training: Loss:  0.4828, Accuracy: 0.8594\n",
      "Batch number : 159, Training: Loss:  0.6846, Accuracy: 0.7969\n",
      "Batch number : 160, Training: Loss:  0.3958, Accuracy: 0.8438\n",
      "Batch number : 161, Training: Loss:  0.6413, Accuracy: 0.8281\n",
      "Batch number : 162, Training: Loss:  0.3111, Accuracy: 0.9062\n",
      "Batch number : 163, Training: Loss:  0.4566, Accuracy: 0.8594\n",
      "Batch number : 164, Training: Loss:  0.4794, Accuracy: 0.8281\n",
      "Batch number : 165, Training: Loss:  0.4415, Accuracy: 0.8438\n",
      "Batch number : 166, Training: Loss:  0.4595, Accuracy: 0.8906\n",
      "Batch number : 167, Training: Loss:  0.4451, Accuracy: 0.9062\n",
      "Batch number : 168, Training: Loss:  0.4287, Accuracy: 0.8906\n",
      "Batch number : 169, Training: Loss:  0.5295, Accuracy: 0.8594\n",
      "Batch number : 170, Training: Loss:  0.2549, Accuracy: 0.9531\n",
      "Batch number : 171, Training: Loss:  0.8595, Accuracy: 0.7344\n",
      "Batch number : 172, Training: Loss:  0.5658, Accuracy: 0.8438\n",
      "Batch number : 173, Training: Loss:  0.5311, Accuracy: 0.8438\n",
      "Batch number : 174, Training: Loss:  0.4935, Accuracy: 0.8750\n",
      "Batch number : 175, Training: Loss:  0.5128, Accuracy: 0.7969\n",
      "Batch number : 176, Training: Loss:  0.3799, Accuracy: 0.9062\n",
      "Batch number : 177, Training: Loss:  0.5006, Accuracy: 0.8594\n",
      "Batch number : 178, Training: Loss:  0.3812, Accuracy: 0.8906\n",
      "Batch number : 179, Training: Loss:  0.4539, Accuracy: 0.8594\n",
      "Batch number : 180, Training: Loss:  0.6199, Accuracy: 0.7812\n",
      "Batch number : 181, Training: Loss:  0.5091, Accuracy: 0.8438\n",
      "Batch number : 182, Training: Loss:  0.6372, Accuracy: 0.7812\n",
      "Batch number : 183, Training: Loss:  0.5207, Accuracy: 0.8125\n",
      "Batch number : 184, Training: Loss:  0.3437, Accuracy: 0.9062\n",
      "Batch number : 185, Training: Loss:  0.5391, Accuracy: 0.8281\n",
      "Batch number : 186, Training: Loss:  0.4923, Accuracy: 0.8281\n",
      "Batch number : 187, Training: Loss:  0.4621, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.5854, Accuracy: 0.8125\n",
      "Batch number : 189, Training: Loss:  0.4226, Accuracy: 0.8750\n",
      "Batch number : 190, Training: Loss:  0.3445, Accuracy: 0.8906\n",
      "Batch number : 191, Training: Loss:  0.4929, Accuracy: 0.9062\n",
      "Batch number : 192, Training: Loss:  0.6902, Accuracy: 0.7969\n",
      "Batch number : 193, Training: Loss:  0.3379, Accuracy: 0.8750\n",
      "Batch number : 194, Training: Loss:  0.4246, Accuracy: 0.8281\n",
      "Batch number : 195, Training: Loss:  0.6151, Accuracy: 0.8125\n",
      "Batch number : 196, Training: Loss:  0.4472, Accuracy: 0.8594\n",
      "Batch number : 197, Training: Loss:  0.7258, Accuracy: 0.7656\n",
      "Batch number : 198, Training: Loss:  0.6725, Accuracy: 0.7344\n",
      "Batch number : 199, Training: Loss:  0.5467, Accuracy: 0.8594\n",
      "Batch number : 200, Training: Loss:  0.7283, Accuracy: 0.7969\n",
      "Batch number : 201, Training: Loss:  0.5578, Accuracy: 0.8438\n",
      "Batch number : 202, Training: Loss:  0.3630, Accuracy: 0.8750\n",
      "Batch number : 203, Training: Loss:  0.6047, Accuracy: 0.7969\n",
      "Batch number : 204, Training: Loss:  0.4561, Accuracy: 0.8750\n",
      "Batch number : 205, Training: Loss:  0.5020, Accuracy: 0.8750\n",
      "Batch number : 206, Training: Loss:  0.5783, Accuracy: 0.7812\n",
      "Batch number : 207, Training: Loss:  0.5075, Accuracy: 0.8281\n",
      "Batch number : 208, Training: Loss:  0.3996, Accuracy: 0.8906\n",
      "Batch number : 209, Training: Loss:  0.5595, Accuracy: 0.8281\n",
      "Batch number : 210, Training: Loss:  0.4378, Accuracy: 0.8750\n",
      "Batch number : 211, Training: Loss:  0.5092, Accuracy: 0.8594\n",
      "Batch number : 212, Training: Loss:  0.5291, Accuracy: 0.8438\n",
      "Batch number : 213, Training: Loss:  0.4540, Accuracy: 0.8750\n",
      "Batch number : 214, Training: Loss:  0.5848, Accuracy: 0.8594\n",
      "Batch number : 215, Training: Loss:  0.6212, Accuracy: 0.8438\n",
      "Batch number : 216, Training: Loss:  0.3681, Accuracy: 0.9219\n",
      "Batch number : 217, Training: Loss:  0.3553, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.6903, Accuracy: 0.7500\n",
      "Batch number : 219, Training: Loss:  0.4969, Accuracy: 0.8281\n",
      "Batch number : 220, Training: Loss:  0.4850, Accuracy: 0.8438\n",
      "Batch number : 221, Training: Loss:  0.5460, Accuracy: 0.7812\n",
      "Batch number : 222, Training: Loss:  0.4131, Accuracy: 0.8594\n",
      "Batch number : 223, Training: Loss:  0.4343, Accuracy: 0.8594\n",
      "Batch number : 224, Training: Loss:  0.3179, Accuracy: 0.9219\n",
      "Batch number : 225, Training: Loss:  0.3679, Accuracy: 0.9219\n",
      "Batch number : 226, Training: Loss:  0.3339, Accuracy: 0.9219\n",
      "Batch number : 227, Training: Loss:  0.6513, Accuracy: 0.8125\n",
      "Batch number : 228, Training: Loss:  0.5800, Accuracy: 0.8438\n",
      "Batch number : 229, Training: Loss:  0.2502, Accuracy: 0.9219\n",
      "Batch number : 230, Training: Loss:  0.4531, Accuracy: 0.8438\n",
      "Batch number : 231, Training: Loss:  0.6063, Accuracy: 0.8281\n",
      "Batch number : 232, Training: Loss:  0.4856, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.7265, Accuracy: 0.7812\n",
      "Batch number : 234, Training: Loss:  0.4632, Accuracy: 0.8750\n",
      "Batch number : 235, Training: Loss:  0.5815, Accuracy: 0.8281\n",
      "Batch number : 236, Training: Loss:  0.3196, Accuracy: 0.9219\n",
      "Batch number : 237, Training: Loss:  0.4387, Accuracy: 0.9219\n",
      "Batch number : 238, Training: Loss:  0.5461, Accuracy: 0.8125\n",
      "Batch number : 239, Training: Loss:  0.4005, Accuracy: 0.8594\n",
      "Batch number : 240, Training: Loss:  0.6612, Accuracy: 0.7969\n",
      "Batch number : 241, Training: Loss:  0.5348, Accuracy: 0.8438\n",
      "Batch number : 242, Training: Loss:  0.3828, Accuracy: 0.8906\n",
      "Batch number : 243, Training: Loss:  0.4128, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.4661, Accuracy: 0.8750\n",
      "Batch number : 245, Training: Loss:  0.3617, Accuracy: 0.8906\n",
      "Batch number : 246, Training: Loss:  0.2863, Accuracy: 0.9219\n",
      "Batch number : 247, Training: Loss:  0.3685, Accuracy: 0.8906\n",
      "Batch number : 248, Training: Loss:  0.4437, Accuracy: 0.8594\n",
      "Batch number : 249, Training: Loss:  0.4371, Accuracy: 0.8906\n",
      "Batch number : 250, Training: Loss:  0.7800, Accuracy: 0.7500\n",
      "Batch number : 251, Training: Loss:  0.4826, Accuracy: 0.8438\n",
      "Batch number : 252, Training: Loss:  0.4890, Accuracy: 0.8438\n",
      "Batch number : 253, Training: Loss:  0.5959, Accuracy: 0.8281\n",
      "Batch number : 254, Training: Loss:  0.3509, Accuracy: 0.8906\n",
      "Batch number : 255, Training: Loss:  0.3846, Accuracy: 0.8906\n",
      "Batch number : 256, Training: Loss:  0.4865, Accuracy: 0.8438\n",
      "Batch number : 257, Training: Loss:  0.3389, Accuracy: 0.9062\n",
      "Batch number : 258, Training: Loss:  0.3582, Accuracy: 0.8906\n",
      "Batch number : 259, Training: Loss:  0.4691, Accuracy: 0.8594\n",
      "Batch number : 260, Training: Loss:  0.5541, Accuracy: 0.8281\n",
      "Batch number : 261, Training: Loss:  0.6433, Accuracy: 0.7969\n",
      "Batch number : 262, Training: Loss:  0.7260, Accuracy: 0.7969\n",
      "Batch number : 263, Training: Loss:  0.5774, Accuracy: 0.7812\n",
      "Batch number : 264, Training: Loss:  0.4158, Accuracy: 0.8906\n",
      "Batch number : 265, Training: Loss:  0.3385, Accuracy: 0.9375\n",
      "Batch number : 266, Training: Loss:  0.5326, Accuracy: 0.8594\n",
      "Batch number : 267, Training: Loss:  0.5161, Accuracy: 0.8281\n",
      "Batch number : 268, Training: Loss:  0.3878, Accuracy: 0.8750\n",
      "Batch number : 269, Training: Loss:  0.7960, Accuracy: 0.7969\n",
      "Batch number : 270, Training: Loss:  0.4945, Accuracy: 0.8594\n",
      "Batch number : 271, Training: Loss:  0.4150, Accuracy: 0.8906\n",
      "Batch number : 272, Training: Loss:  0.3515, Accuracy: 0.9062\n",
      "Batch number : 273, Training: Loss:  0.4812, Accuracy: 0.8594\n",
      "Batch number : 274, Training: Loss:  0.6409, Accuracy: 0.8281\n",
      "Batch number : 275, Training: Loss:  0.4554, Accuracy: 0.8438\n",
      "Batch number : 276, Training: Loss:  0.3295, Accuracy: 0.8750\n",
      "Batch number : 277, Training: Loss:  0.3457, Accuracy: 0.9062\n",
      "Batch number : 278, Training: Loss:  0.5002, Accuracy: 0.8594\n",
      "Batch number : 279, Training: Loss:  0.4988, Accuracy: 0.8438\n",
      "Batch number : 280, Training: Loss:  0.4188, Accuracy: 0.8438\n",
      "Batch number : 281, Training: Loss:  0.8687, Accuracy: 0.7188\n",
      "Batch number : 282, Training: Loss:  0.3561, Accuracy: 0.8750\n",
      "Batch number : 283, Training: Loss:  0.5170, Accuracy: 0.8594\n",
      "Batch number : 284, Training: Loss:  0.2945, Accuracy: 0.9375\n",
      "Batch number : 285, Training: Loss:  0.5660, Accuracy: 0.8125\n",
      "Batch number : 286, Training: Loss:  0.4330, Accuracy: 0.8906\n",
      "Batch number : 287, Training: Loss:  0.3517, Accuracy: 0.9219\n",
      "Batch number : 288, Training: Loss:  0.5615, Accuracy: 0.8125\n",
      "Batch number : 289, Training: Loss:  0.2874, Accuracy: 0.9219\n",
      "Batch number : 290, Training: Loss:  0.5014, Accuracy: 0.8438\n",
      "Batch number : 291, Training: Loss:  0.4401, Accuracy: 0.8594\n",
      "Batch number : 292, Training: Loss:  0.3067, Accuracy: 0.9219\n",
      "Batch number : 293, Training: Loss:  0.9837, Accuracy: 0.7500\n",
      "Batch number : 294, Training: Loss:  0.4473, Accuracy: 0.8750\n",
      "Batch number : 295, Training: Loss:  0.5245, Accuracy: 0.8594\n",
      "Batch number : 296, Training: Loss:  0.3950, Accuracy: 0.8750\n",
      "Batch number : 297, Training: Loss:  0.4715, Accuracy: 0.8594\n",
      "Batch number : 298, Training: Loss:  0.3857, Accuracy: 0.8750\n",
      "Batch number : 299, Training: Loss:  0.5989, Accuracy: 0.8281\n",
      "Batch number : 300, Training: Loss:  0.3511, Accuracy: 0.9062\n",
      "Batch number : 301, Training: Loss:  0.4308, Accuracy: 0.8594\n",
      "Batch number : 302, Training: Loss:  0.4642, Accuracy: 0.8281\n",
      "Batch number : 303, Training: Loss:  0.3956, Accuracy: 0.8594\n",
      "Batch number : 304, Training: Loss:  0.4383, Accuracy: 0.8906\n",
      "Batch number : 305, Training: Loss:  0.3752, Accuracy: 0.9062\n",
      "Batch number : 306, Training: Loss:  0.2579, Accuracy: 0.9219\n",
      "Batch number : 307, Training: Loss:  0.6386, Accuracy: 0.8125\n",
      "Batch number : 308, Training: Loss:  0.5021, Accuracy: 0.8281\n",
      "Batch number : 309, Training: Loss:  0.5890, Accuracy: 0.7969\n",
      "Batch number : 310, Training: Loss:  0.5042, Accuracy: 0.9062\n",
      "Batch number : 311, Training: Loss:  0.1768, Accuracy: 0.9688\n",
      "Batch number : 312, Training: Loss:  0.3479, Accuracy: 0.8906\n",
      "Batch number : 313, Training: Loss:  0.5241, Accuracy: 0.8438\n",
      "Batch number : 314, Training: Loss:  0.4415, Accuracy: 0.8594\n",
      "Batch number : 315, Training: Loss:  0.5100, Accuracy: 0.8438\n",
      "Batch number : 316, Training: Loss:  0.4958, Accuracy: 0.8594\n",
      "Batch number : 317, Training: Loss:  0.4971, Accuracy: 0.8438\n",
      "Batch number : 318, Training: Loss:  0.5037, Accuracy: 0.8281\n",
      "Batch number : 319, Training: Loss:  0.2473, Accuracy: 0.9531\n",
      "Batch number : 320, Training: Loss:  0.4442, Accuracy: 0.8750\n",
      "Batch number : 321, Training: Loss:  0.5982, Accuracy: 0.8281\n",
      "Batch number : 322, Training: Loss:  0.4143, Accuracy: 0.8750\n",
      "Batch number : 323, Training: Loss:  0.5016, Accuracy: 0.8281\n",
      "Batch number : 324, Training: Loss:  0.4932, Accuracy: 0.8594\n",
      "Batch number : 325, Training: Loss:  0.5389, Accuracy: 0.8125\n",
      "Batch number : 326, Training: Loss:  0.3179, Accuracy: 0.9219\n",
      "Batch number : 327, Training: Loss:  0.4200, Accuracy: 0.8750\n",
      "Batch number : 328, Training: Loss:  0.3766, Accuracy: 0.8906\n",
      "Batch number : 329, Training: Loss:  0.5088, Accuracy: 0.8125\n",
      "Batch number : 330, Training: Loss:  0.6125, Accuracy: 0.8125\n",
      "Batch number : 331, Training: Loss:  0.3222, Accuracy: 0.8906\n",
      "Batch number : 332, Training: Loss:  0.4836, Accuracy: 0.8281\n",
      "Batch number : 333, Training: Loss:  0.5316, Accuracy: 0.8438\n",
      "Batch number : 334, Training: Loss:  0.3450, Accuracy: 0.9062\n",
      "Batch number : 335, Training: Loss:  0.6609, Accuracy: 0.7500\n",
      "Batch number : 336, Training: Loss:  0.3802, Accuracy: 0.8594\n",
      "Batch number : 337, Training: Loss:  0.6453, Accuracy: 0.8281\n",
      "Batch number : 338, Training: Loss:  1.0554, Accuracy: 0.6875\n",
      "Batch number : 339, Training: Loss:  0.3333, Accuracy: 0.8906\n",
      "Batch number : 340, Training: Loss:  0.3600, Accuracy: 0.8906\n",
      "Batch number : 341, Training: Loss:  0.4087, Accuracy: 0.8906\n",
      "Batch number : 342, Training: Loss:  0.4079, Accuracy: 0.9062\n",
      "Batch number : 343, Training: Loss:  0.3481, Accuracy: 0.9219\n",
      "Batch number : 344, Training: Loss:  0.2557, Accuracy: 0.9375\n",
      "Batch number : 345, Training: Loss:  0.4978, Accuracy: 0.8438\n",
      "Batch number : 346, Training: Loss:  0.4492, Accuracy: 0.8750\n",
      "Batch number : 347, Training: Loss:  0.6715, Accuracy: 0.8281\n",
      "Batch number : 348, Training: Loss:  0.5159, Accuracy: 0.8438\n",
      "Batch number : 349, Training: Loss:  0.5072, Accuracy: 0.8281\n",
      "Batch number : 350, Training: Loss:  0.4466, Accuracy: 0.8281\n",
      "Batch number : 351, Training: Loss:  0.3484, Accuracy: 0.9062\n",
      "Batch number : 352, Training: Loss:  0.5491, Accuracy: 0.8594\n",
      "Batch number : 353, Training: Loss:  0.3915, Accuracy: 0.8906\n",
      "Batch number : 354, Training: Loss:  0.3539, Accuracy: 0.8906\n",
      "Batch number : 355, Training: Loss:  0.4240, Accuracy: 0.8750\n",
      "Batch number : 356, Training: Loss:  0.2322, Accuracy: 0.9531\n",
      "Batch number : 357, Training: Loss:  0.3957, Accuracy: 0.8750\n",
      "Batch number : 358, Training: Loss:  0.4468, Accuracy: 0.8594\n",
      "Batch number : 359, Training: Loss:  0.4894, Accuracy: 0.8594\n",
      "Batch number : 360, Training: Loss:  0.6095, Accuracy: 0.8438\n",
      "Batch number : 361, Training: Loss:  0.3645, Accuracy: 0.8750\n",
      "Batch number : 362, Training: Loss:  0.4527, Accuracy: 0.8750\n",
      "Batch number : 363, Training: Loss:  0.3233, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.4017, Accuracy: 0.8750\n",
      "Batch number : 365, Training: Loss:  0.5163, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.4625, Accuracy: 0.9062\n",
      "Batch number : 367, Training: Loss:  0.5684, Accuracy: 0.8438\n",
      "Batch number : 368, Training: Loss:  0.3855, Accuracy: 0.9219\n",
      "Batch number : 369, Training: Loss:  0.4002, Accuracy: 0.8750\n",
      "Batch number : 370, Training: Loss:  0.5063, Accuracy: 0.8594\n",
      "Batch number : 371, Training: Loss:  0.5380, Accuracy: 0.8438\n",
      "Batch number : 372, Training: Loss:  0.4474, Accuracy: 0.8750\n",
      "Batch number : 373, Training: Loss:  0.3186, Accuracy: 0.9219\n",
      "Batch number : 374, Training: Loss:  0.3071, Accuracy: 0.8906\n",
      "Batch number : 375, Training: Loss:  0.6189, Accuracy: 0.7969\n",
      "Batch number : 376, Training: Loss:  0.6069, Accuracy: 0.8125\n",
      "Batch number : 377, Training: Loss:  0.5254, Accuracy: 0.8750\n",
      "Epoch: 18/20\n",
      "Batch number : 000, Training: Loss:  0.7580, Accuracy: 0.7656\n",
      "Batch number : 001, Training: Loss:  0.4336, Accuracy: 0.8750\n",
      "Batch number : 002, Training: Loss:  0.3674, Accuracy: 0.8906\n",
      "Batch number : 003, Training: Loss:  0.3305, Accuracy: 0.9219\n",
      "Batch number : 004, Training: Loss:  0.4264, Accuracy: 0.8594\n",
      "Batch number : 005, Training: Loss:  0.4499, Accuracy: 0.8750\n",
      "Batch number : 006, Training: Loss:  0.4184, Accuracy: 0.9062\n",
      "Batch number : 007, Training: Loss:  0.3609, Accuracy: 0.9062\n",
      "Batch number : 008, Training: Loss:  0.5858, Accuracy: 0.8750\n",
      "Batch number : 009, Training: Loss:  0.5047, Accuracy: 0.8438\n",
      "Batch number : 010, Training: Loss:  0.6087, Accuracy: 0.8281\n",
      "Batch number : 011, Training: Loss:  0.4604, Accuracy: 0.8281\n",
      "Batch number : 012, Training: Loss:  0.5867, Accuracy: 0.7812\n",
      "Batch number : 013, Training: Loss:  0.4116, Accuracy: 0.8750\n",
      "Batch number : 014, Training: Loss:  0.4597, Accuracy: 0.8594\n",
      "Batch number : 015, Training: Loss:  0.3811, Accuracy: 0.8906\n",
      "Batch number : 016, Training: Loss:  0.4461, Accuracy: 0.8594\n",
      "Batch number : 017, Training: Loss:  0.3581, Accuracy: 0.9062\n",
      "Batch number : 018, Training: Loss:  0.4721, Accuracy: 0.8750\n",
      "Batch number : 019, Training: Loss:  0.2643, Accuracy: 0.9375\n",
      "Batch number : 020, Training: Loss:  0.5387, Accuracy: 0.7969\n",
      "Batch number : 021, Training: Loss:  0.4877, Accuracy: 0.8438\n",
      "Batch number : 022, Training: Loss:  0.1606, Accuracy: 0.9844\n",
      "Batch number : 023, Training: Loss:  0.4703, Accuracy: 0.8750\n",
      "Batch number : 024, Training: Loss:  0.6204, Accuracy: 0.8125\n",
      "Batch number : 025, Training: Loss:  0.5137, Accuracy: 0.8281\n",
      "Batch number : 026, Training: Loss:  0.3795, Accuracy: 0.9219\n",
      "Batch number : 027, Training: Loss:  0.6090, Accuracy: 0.8125\n",
      "Batch number : 028, Training: Loss:  0.6459, Accuracy: 0.7812\n",
      "Batch number : 029, Training: Loss:  0.3798, Accuracy: 0.8906\n",
      "Batch number : 030, Training: Loss:  0.4229, Accuracy: 0.8438\n",
      "Batch number : 031, Training: Loss:  0.3808, Accuracy: 0.8750\n",
      "Batch number : 032, Training: Loss:  0.4774, Accuracy: 0.8281\n",
      "Batch number : 033, Training: Loss:  0.6943, Accuracy: 0.8438\n",
      "Batch number : 034, Training: Loss:  0.5727, Accuracy: 0.8281\n",
      "Batch number : 035, Training: Loss:  0.3315, Accuracy: 0.8906\n",
      "Batch number : 036, Training: Loss:  0.7057, Accuracy: 0.7812\n",
      "Batch number : 037, Training: Loss:  0.2385, Accuracy: 0.9531\n",
      "Batch number : 038, Training: Loss:  0.5007, Accuracy: 0.8594\n",
      "Batch number : 039, Training: Loss:  0.4649, Accuracy: 0.8438\n",
      "Batch number : 040, Training: Loss:  0.4562, Accuracy: 0.8906\n",
      "Batch number : 041, Training: Loss:  0.2931, Accuracy: 0.9219\n",
      "Batch number : 042, Training: Loss:  0.4715, Accuracy: 0.8750\n",
      "Batch number : 043, Training: Loss:  0.3878, Accuracy: 0.8594\n",
      "Batch number : 044, Training: Loss:  0.4869, Accuracy: 0.8281\n",
      "Batch number : 045, Training: Loss:  0.5251, Accuracy: 0.8281\n",
      "Batch number : 046, Training: Loss:  0.7520, Accuracy: 0.7656\n",
      "Batch number : 047, Training: Loss:  0.6451, Accuracy: 0.7812\n",
      "Batch number : 048, Training: Loss:  0.4442, Accuracy: 0.8438\n",
      "Batch number : 049, Training: Loss:  0.5178, Accuracy: 0.8594\n",
      "Batch number : 050, Training: Loss:  0.5478, Accuracy: 0.8438\n",
      "Batch number : 051, Training: Loss:  0.5091, Accuracy: 0.8438\n",
      "Batch number : 052, Training: Loss:  0.5693, Accuracy: 0.8438\n",
      "Batch number : 053, Training: Loss:  0.4674, Accuracy: 0.8594\n",
      "Batch number : 054, Training: Loss:  0.3885, Accuracy: 0.9219\n",
      "Batch number : 055, Training: Loss:  0.3875, Accuracy: 0.8750\n",
      "Batch number : 056, Training: Loss:  0.5661, Accuracy: 0.8438\n",
      "Batch number : 057, Training: Loss:  0.4888, Accuracy: 0.8438\n",
      "Batch number : 058, Training: Loss:  0.5318, Accuracy: 0.8594\n",
      "Batch number : 059, Training: Loss:  0.4386, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.4766, Accuracy: 0.8438\n",
      "Batch number : 061, Training: Loss:  0.4822, Accuracy: 0.8281\n",
      "Batch number : 062, Training: Loss:  0.5388, Accuracy: 0.8594\n",
      "Batch number : 063, Training: Loss:  0.5492, Accuracy: 0.8438\n",
      "Batch number : 064, Training: Loss:  0.3988, Accuracy: 0.8750\n",
      "Batch number : 065, Training: Loss:  0.3225, Accuracy: 0.8906\n",
      "Batch number : 066, Training: Loss:  0.5533, Accuracy: 0.8438\n",
      "Batch number : 067, Training: Loss:  0.6137, Accuracy: 0.8125\n",
      "Batch number : 068, Training: Loss:  0.3233, Accuracy: 0.9062\n",
      "Batch number : 069, Training: Loss:  0.5467, Accuracy: 0.8594\n",
      "Batch number : 070, Training: Loss:  0.4730, Accuracy: 0.8438\n",
      "Batch number : 071, Training: Loss:  0.3700, Accuracy: 0.8906\n",
      "Batch number : 072, Training: Loss:  0.3925, Accuracy: 0.8594\n",
      "Batch number : 073, Training: Loss:  0.5381, Accuracy: 0.8594\n",
      "Batch number : 074, Training: Loss:  0.6251, Accuracy: 0.8281\n",
      "Batch number : 075, Training: Loss:  0.5354, Accuracy: 0.8125\n",
      "Batch number : 076, Training: Loss:  0.2929, Accuracy: 0.9219\n",
      "Batch number : 077, Training: Loss:  0.5179, Accuracy: 0.8125\n",
      "Batch number : 078, Training: Loss:  0.4328, Accuracy: 0.8906\n",
      "Batch number : 079, Training: Loss:  0.4741, Accuracy: 0.8594\n",
      "Batch number : 080, Training: Loss:  0.3434, Accuracy: 0.8750\n",
      "Batch number : 081, Training: Loss:  0.5547, Accuracy: 0.8125\n",
      "Batch number : 082, Training: Loss:  0.2960, Accuracy: 0.9375\n",
      "Batch number : 083, Training: Loss:  0.4232, Accuracy: 0.8594\n",
      "Batch number : 084, Training: Loss:  0.4457, Accuracy: 0.8438\n",
      "Batch number : 085, Training: Loss:  0.4550, Accuracy: 0.8750\n",
      "Batch number : 086, Training: Loss:  0.4093, Accuracy: 0.8906\n",
      "Batch number : 087, Training: Loss:  0.2792, Accuracy: 0.9375\n",
      "Batch number : 088, Training: Loss:  0.5444, Accuracy: 0.8438\n",
      "Batch number : 089, Training: Loss:  0.6513, Accuracy: 0.7969\n",
      "Batch number : 090, Training: Loss:  0.6202, Accuracy: 0.8281\n",
      "Batch number : 091, Training: Loss:  0.6641, Accuracy: 0.7812\n",
      "Batch number : 092, Training: Loss:  0.4098, Accuracy: 0.8906\n",
      "Batch number : 093, Training: Loss:  0.4403, Accuracy: 0.8750\n",
      "Batch number : 094, Training: Loss:  0.4295, Accuracy: 0.8594\n",
      "Batch number : 095, Training: Loss:  0.6254, Accuracy: 0.8438\n",
      "Batch number : 096, Training: Loss:  0.5072, Accuracy: 0.8438\n",
      "Batch number : 097, Training: Loss:  0.5517, Accuracy: 0.8281\n",
      "Batch number : 098, Training: Loss:  0.3508, Accuracy: 0.9375\n",
      "Batch number : 099, Training: Loss:  0.2838, Accuracy: 0.9062\n",
      "Batch number : 100, Training: Loss:  0.3913, Accuracy: 0.8750\n",
      "Batch number : 101, Training: Loss:  0.4363, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.5636, Accuracy: 0.8594\n",
      "Batch number : 103, Training: Loss:  0.5961, Accuracy: 0.8438\n",
      "Batch number : 104, Training: Loss:  0.4500, Accuracy: 0.9062\n",
      "Batch number : 105, Training: Loss:  0.1855, Accuracy: 0.9688\n",
      "Batch number : 106, Training: Loss:  0.8377, Accuracy: 0.7812\n",
      "Batch number : 107, Training: Loss:  0.6629, Accuracy: 0.8438\n",
      "Batch number : 108, Training: Loss:  0.9890, Accuracy: 0.7031\n",
      "Batch number : 109, Training: Loss:  0.5603, Accuracy: 0.8438\n",
      "Batch number : 110, Training: Loss:  0.3178, Accuracy: 0.9219\n",
      "Batch number : 111, Training: Loss:  0.3957, Accuracy: 0.8906\n",
      "Batch number : 112, Training: Loss:  0.3460, Accuracy: 0.9062\n",
      "Batch number : 113, Training: Loss:  0.4850, Accuracy: 0.8750\n",
      "Batch number : 114, Training: Loss:  0.6497, Accuracy: 0.7812\n",
      "Batch number : 115, Training: Loss:  0.4198, Accuracy: 0.8906\n",
      "Batch number : 116, Training: Loss:  0.6400, Accuracy: 0.8125\n",
      "Batch number : 117, Training: Loss:  0.4301, Accuracy: 0.8906\n",
      "Batch number : 118, Training: Loss:  0.4287, Accuracy: 0.8750\n",
      "Batch number : 119, Training: Loss:  0.4534, Accuracy: 0.8594\n",
      "Batch number : 120, Training: Loss:  0.5106, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.3218, Accuracy: 0.9219\n",
      "Batch number : 122, Training: Loss:  0.3908, Accuracy: 0.8750\n",
      "Batch number : 123, Training: Loss:  0.5263, Accuracy: 0.8594\n",
      "Batch number : 124, Training: Loss:  0.4498, Accuracy: 0.8750\n",
      "Batch number : 125, Training: Loss:  0.6284, Accuracy: 0.7812\n",
      "Batch number : 126, Training: Loss:  0.4080, Accuracy: 0.8750\n",
      "Batch number : 127, Training: Loss:  0.6753, Accuracy: 0.7812\n",
      "Batch number : 128, Training: Loss:  0.4371, Accuracy: 0.8750\n",
      "Batch number : 129, Training: Loss:  0.4332, Accuracy: 0.8750\n",
      "Batch number : 130, Training: Loss:  0.5863, Accuracy: 0.8438\n",
      "Batch number : 131, Training: Loss:  0.4853, Accuracy: 0.7812\n",
      "Batch number : 132, Training: Loss:  0.8055, Accuracy: 0.7656\n",
      "Batch number : 133, Training: Loss:  0.4399, Accuracy: 0.8438\n",
      "Batch number : 134, Training: Loss:  0.6134, Accuracy: 0.8281\n",
      "Batch number : 135, Training: Loss:  0.4828, Accuracy: 0.8906\n",
      "Batch number : 136, Training: Loss:  0.5712, Accuracy: 0.7812\n",
      "Batch number : 137, Training: Loss:  0.4695, Accuracy: 0.8281\n",
      "Batch number : 138, Training: Loss:  0.5025, Accuracy: 0.8438\n",
      "Batch number : 139, Training: Loss:  0.4823, Accuracy: 0.8281\n",
      "Batch number : 140, Training: Loss:  0.2770, Accuracy: 0.9688\n",
      "Batch number : 141, Training: Loss:  0.4686, Accuracy: 0.8594\n",
      "Batch number : 142, Training: Loss:  0.6436, Accuracy: 0.8438\n",
      "Batch number : 143, Training: Loss:  0.2250, Accuracy: 0.9531\n",
      "Batch number : 144, Training: Loss:  0.3206, Accuracy: 0.9062\n",
      "Batch number : 145, Training: Loss:  0.5519, Accuracy: 0.8281\n",
      "Batch number : 146, Training: Loss:  0.4003, Accuracy: 0.9219\n",
      "Batch number : 147, Training: Loss:  0.3623, Accuracy: 0.9062\n",
      "Batch number : 148, Training: Loss:  0.4687, Accuracy: 0.8750\n",
      "Batch number : 149, Training: Loss:  0.6433, Accuracy: 0.8125\n",
      "Batch number : 150, Training: Loss:  0.4155, Accuracy: 0.9219\n",
      "Batch number : 151, Training: Loss:  0.4528, Accuracy: 0.8750\n",
      "Batch number : 152, Training: Loss:  0.4290, Accuracy: 0.8594\n",
      "Batch number : 153, Training: Loss:  0.8479, Accuracy: 0.7344\n",
      "Batch number : 154, Training: Loss:  0.4966, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.4801, Accuracy: 0.8594\n",
      "Batch number : 156, Training: Loss:  0.3517, Accuracy: 0.8906\n",
      "Batch number : 157, Training: Loss:  0.4290, Accuracy: 0.8906\n",
      "Batch number : 158, Training: Loss:  0.3901, Accuracy: 0.8906\n",
      "Batch number : 159, Training: Loss:  0.4801, Accuracy: 0.8594\n",
      "Batch number : 160, Training: Loss:  0.4678, Accuracy: 0.8594\n",
      "Batch number : 161, Training: Loss:  0.5148, Accuracy: 0.8281\n",
      "Batch number : 162, Training: Loss:  0.5524, Accuracy: 0.8125\n",
      "Batch number : 163, Training: Loss:  0.3542, Accuracy: 0.8906\n",
      "Batch number : 164, Training: Loss:  0.6326, Accuracy: 0.7969\n",
      "Batch number : 165, Training: Loss:  0.4676, Accuracy: 0.8906\n",
      "Batch number : 166, Training: Loss:  0.4242, Accuracy: 0.8594\n",
      "Batch number : 167, Training: Loss:  0.4178, Accuracy: 0.8750\n",
      "Batch number : 168, Training: Loss:  0.4970, Accuracy: 0.8125\n",
      "Batch number : 169, Training: Loss:  0.6660, Accuracy: 0.8125\n",
      "Batch number : 170, Training: Loss:  0.5741, Accuracy: 0.8125\n",
      "Batch number : 171, Training: Loss:  0.4317, Accuracy: 0.8750\n",
      "Batch number : 172, Training: Loss:  0.4465, Accuracy: 0.8750\n",
      "Batch number : 173, Training: Loss:  0.4817, Accuracy: 0.9062\n",
      "Batch number : 174, Training: Loss:  0.4224, Accuracy: 0.8438\n",
      "Batch number : 175, Training: Loss:  0.2598, Accuracy: 0.9219\n",
      "Batch number : 176, Training: Loss:  0.5022, Accuracy: 0.8438\n",
      "Batch number : 177, Training: Loss:  0.4258, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.3534, Accuracy: 0.9531\n",
      "Batch number : 179, Training: Loss:  0.4295, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.2587, Accuracy: 0.9219\n",
      "Batch number : 181, Training: Loss:  0.3113, Accuracy: 0.9062\n",
      "Batch number : 182, Training: Loss:  0.5642, Accuracy: 0.8125\n",
      "Batch number : 183, Training: Loss:  0.4823, Accuracy: 0.8438\n",
      "Batch number : 184, Training: Loss:  0.2030, Accuracy: 0.9531\n",
      "Batch number : 185, Training: Loss:  0.3439, Accuracy: 0.8906\n",
      "Batch number : 186, Training: Loss:  0.5749, Accuracy: 0.8281\n",
      "Batch number : 187, Training: Loss:  0.2718, Accuracy: 0.9219\n",
      "Batch number : 188, Training: Loss:  0.4494, Accuracy: 0.8438\n",
      "Batch number : 189, Training: Loss:  0.3189, Accuracy: 0.9219\n",
      "Batch number : 190, Training: Loss:  0.4285, Accuracy: 0.8906\n",
      "Batch number : 191, Training: Loss:  0.4967, Accuracy: 0.8750\n",
      "Batch number : 192, Training: Loss:  0.3655, Accuracy: 0.8906\n",
      "Batch number : 193, Training: Loss:  0.5275, Accuracy: 0.8281\n",
      "Batch number : 194, Training: Loss:  0.4912, Accuracy: 0.8750\n",
      "Batch number : 195, Training: Loss:  0.5133, Accuracy: 0.8594\n",
      "Batch number : 196, Training: Loss:  0.5327, Accuracy: 0.8438\n",
      "Batch number : 197, Training: Loss:  0.2988, Accuracy: 0.8906\n",
      "Batch number : 198, Training: Loss:  0.4533, Accuracy: 0.8438\n",
      "Batch number : 199, Training: Loss:  0.6711, Accuracy: 0.8125\n",
      "Batch number : 200, Training: Loss:  0.4597, Accuracy: 0.8906\n",
      "Batch number : 201, Training: Loss:  0.3216, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.5143, Accuracy: 0.8594\n",
      "Batch number : 203, Training: Loss:  0.3594, Accuracy: 0.8750\n",
      "Batch number : 204, Training: Loss:  0.2967, Accuracy: 0.9375\n",
      "Batch number : 205, Training: Loss:  0.5374, Accuracy: 0.8594\n",
      "Batch number : 206, Training: Loss:  0.6355, Accuracy: 0.7969\n",
      "Batch number : 207, Training: Loss:  0.4557, Accuracy: 0.8750\n",
      "Batch number : 208, Training: Loss:  0.5972, Accuracy: 0.8281\n",
      "Batch number : 209, Training: Loss:  0.2752, Accuracy: 0.9375\n",
      "Batch number : 210, Training: Loss:  0.6331, Accuracy: 0.7969\n",
      "Batch number : 211, Training: Loss:  0.3427, Accuracy: 0.8750\n",
      "Batch number : 212, Training: Loss:  0.6420, Accuracy: 0.8125\n",
      "Batch number : 213, Training: Loss:  0.3100, Accuracy: 0.9062\n",
      "Batch number : 214, Training: Loss:  0.2887, Accuracy: 0.9531\n",
      "Batch number : 215, Training: Loss:  0.4810, Accuracy: 0.8750\n",
      "Batch number : 216, Training: Loss:  0.3698, Accuracy: 0.9219\n",
      "Batch number : 217, Training: Loss:  0.3739, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.6203, Accuracy: 0.8438\n",
      "Batch number : 219, Training: Loss:  0.5654, Accuracy: 0.8281\n",
      "Batch number : 220, Training: Loss:  0.5527, Accuracy: 0.8281\n",
      "Batch number : 221, Training: Loss:  0.3743, Accuracy: 0.8750\n",
      "Batch number : 222, Training: Loss:  0.3537, Accuracy: 0.8906\n",
      "Batch number : 223, Training: Loss:  0.5296, Accuracy: 0.8281\n",
      "Batch number : 224, Training: Loss:  0.5624, Accuracy: 0.8281\n",
      "Batch number : 225, Training: Loss:  0.4840, Accuracy: 0.8281\n",
      "Batch number : 226, Training: Loss:  0.2947, Accuracy: 0.9062\n",
      "Batch number : 227, Training: Loss:  0.5884, Accuracy: 0.8281\n",
      "Batch number : 228, Training: Loss:  0.4286, Accuracy: 0.8594\n",
      "Batch number : 229, Training: Loss:  0.5022, Accuracy: 0.8281\n",
      "Batch number : 230, Training: Loss:  0.7614, Accuracy: 0.8125\n",
      "Batch number : 231, Training: Loss:  0.3633, Accuracy: 0.8750\n",
      "Batch number : 232, Training: Loss:  0.3135, Accuracy: 0.9062\n",
      "Batch number : 233, Training: Loss:  0.4942, Accuracy: 0.8281\n",
      "Batch number : 234, Training: Loss:  0.6859, Accuracy: 0.7969\n",
      "Batch number : 235, Training: Loss:  0.3467, Accuracy: 0.9062\n",
      "Batch number : 236, Training: Loss:  0.5665, Accuracy: 0.7969\n",
      "Batch number : 237, Training: Loss:  0.4975, Accuracy: 0.8438\n",
      "Batch number : 238, Training: Loss:  0.5136, Accuracy: 0.7969\n",
      "Batch number : 239, Training: Loss:  0.6434, Accuracy: 0.7812\n",
      "Batch number : 240, Training: Loss:  0.2017, Accuracy: 0.9844\n",
      "Batch number : 241, Training: Loss:  0.5690, Accuracy: 0.8438\n",
      "Batch number : 242, Training: Loss:  0.5963, Accuracy: 0.7969\n",
      "Batch number : 243, Training: Loss:  0.3827, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.7953, Accuracy: 0.7500\n",
      "Batch number : 245, Training: Loss:  0.3815, Accuracy: 0.8750\n",
      "Batch number : 246, Training: Loss:  0.4993, Accuracy: 0.8281\n",
      "Batch number : 247, Training: Loss:  0.3010, Accuracy: 0.9375\n",
      "Batch number : 248, Training: Loss:  0.3078, Accuracy: 0.9375\n",
      "Batch number : 249, Training: Loss:  0.5804, Accuracy: 0.7969\n",
      "Batch number : 250, Training: Loss:  0.3933, Accuracy: 0.9062\n",
      "Batch number : 251, Training: Loss:  0.4881, Accuracy: 0.8594\n",
      "Batch number : 252, Training: Loss:  0.4444, Accuracy: 0.9062\n",
      "Batch number : 253, Training: Loss:  0.3847, Accuracy: 0.8750\n",
      "Batch number : 254, Training: Loss:  0.3489, Accuracy: 0.9062\n",
      "Batch number : 255, Training: Loss:  0.5420, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.4319, Accuracy: 0.8750\n",
      "Batch number : 257, Training: Loss:  0.3016, Accuracy: 0.9219\n",
      "Batch number : 258, Training: Loss:  0.6863, Accuracy: 0.7656\n",
      "Batch number : 259, Training: Loss:  0.5286, Accuracy: 0.8906\n",
      "Batch number : 260, Training: Loss:  0.4753, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.2041, Accuracy: 0.9688\n",
      "Batch number : 262, Training: Loss:  0.3765, Accuracy: 0.9062\n",
      "Batch number : 263, Training: Loss:  0.4837, Accuracy: 0.8594\n",
      "Batch number : 264, Training: Loss:  0.4892, Accuracy: 0.8438\n",
      "Batch number : 265, Training: Loss:  0.6079, Accuracy: 0.8125\n",
      "Batch number : 266, Training: Loss:  0.4145, Accuracy: 0.9062\n",
      "Batch number : 267, Training: Loss:  0.4132, Accuracy: 0.8594\n",
      "Batch number : 268, Training: Loss:  0.5571, Accuracy: 0.8281\n",
      "Batch number : 269, Training: Loss:  0.5145, Accuracy: 0.8438\n",
      "Batch number : 270, Training: Loss:  0.5356, Accuracy: 0.8438\n",
      "Batch number : 271, Training: Loss:  0.4613, Accuracy: 0.8906\n",
      "Batch number : 272, Training: Loss:  0.5296, Accuracy: 0.8281\n",
      "Batch number : 273, Training: Loss:  0.6014, Accuracy: 0.7969\n",
      "Batch number : 274, Training: Loss:  0.5621, Accuracy: 0.8281\n",
      "Batch number : 275, Training: Loss:  0.6274, Accuracy: 0.8125\n",
      "Batch number : 276, Training: Loss:  0.4309, Accuracy: 0.8750\n",
      "Batch number : 277, Training: Loss:  0.5745, Accuracy: 0.7812\n",
      "Batch number : 278, Training: Loss:  0.5380, Accuracy: 0.8438\n",
      "Batch number : 279, Training: Loss:  0.3735, Accuracy: 0.8750\n",
      "Batch number : 280, Training: Loss:  0.4601, Accuracy: 0.8438\n",
      "Batch number : 281, Training: Loss:  0.5686, Accuracy: 0.7969\n",
      "Batch number : 282, Training: Loss:  0.4574, Accuracy: 0.8594\n",
      "Batch number : 283, Training: Loss:  0.4464, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.6667, Accuracy: 0.8281\n",
      "Batch number : 285, Training: Loss:  0.4249, Accuracy: 0.8906\n",
      "Batch number : 286, Training: Loss:  0.3187, Accuracy: 0.9062\n",
      "Batch number : 287, Training: Loss:  0.5750, Accuracy: 0.8125\n",
      "Batch number : 288, Training: Loss:  0.4969, Accuracy: 0.8438\n",
      "Batch number : 289, Training: Loss:  0.4012, Accuracy: 0.8906\n",
      "Batch number : 290, Training: Loss:  0.6060, Accuracy: 0.8594\n",
      "Batch number : 291, Training: Loss:  0.6136, Accuracy: 0.7969\n",
      "Batch number : 292, Training: Loss:  0.3843, Accuracy: 0.8438\n",
      "Batch number : 293, Training: Loss:  0.4678, Accuracy: 0.8594\n",
      "Batch number : 294, Training: Loss:  0.4710, Accuracy: 0.8594\n",
      "Batch number : 295, Training: Loss:  0.6504, Accuracy: 0.7812\n",
      "Batch number : 296, Training: Loss:  0.3483, Accuracy: 0.9062\n",
      "Batch number : 297, Training: Loss:  0.3893, Accuracy: 0.8594\n",
      "Batch number : 298, Training: Loss:  0.5005, Accuracy: 0.8750\n",
      "Batch number : 299, Training: Loss:  0.6293, Accuracy: 0.7656\n",
      "Batch number : 300, Training: Loss:  0.4450, Accuracy: 0.8750\n",
      "Batch number : 301, Training: Loss:  0.3801, Accuracy: 0.8906\n",
      "Batch number : 302, Training: Loss:  0.7338, Accuracy: 0.7500\n",
      "Batch number : 303, Training: Loss:  0.3972, Accuracy: 0.8906\n",
      "Batch number : 304, Training: Loss:  0.4629, Accuracy: 0.8750\n",
      "Batch number : 305, Training: Loss:  0.3463, Accuracy: 0.9219\n",
      "Batch number : 306, Training: Loss:  0.6223, Accuracy: 0.7812\n",
      "Batch number : 307, Training: Loss:  0.3700, Accuracy: 0.8906\n",
      "Batch number : 308, Training: Loss:  0.2553, Accuracy: 0.9375\n",
      "Batch number : 309, Training: Loss:  0.2961, Accuracy: 0.9219\n",
      "Batch number : 310, Training: Loss:  0.4431, Accuracy: 0.8594\n",
      "Batch number : 311, Training: Loss:  0.5024, Accuracy: 0.8594\n",
      "Batch number : 312, Training: Loss:  0.4356, Accuracy: 0.8750\n",
      "Batch number : 313, Training: Loss:  0.6743, Accuracy: 0.7812\n",
      "Batch number : 314, Training: Loss:  0.5964, Accuracy: 0.8438\n",
      "Batch number : 315, Training: Loss:  0.5164, Accuracy: 0.8438\n",
      "Batch number : 316, Training: Loss:  0.4588, Accuracy: 0.8594\n",
      "Batch number : 317, Training: Loss:  0.4463, Accuracy: 0.8594\n",
      "Batch number : 318, Training: Loss:  0.7348, Accuracy: 0.7969\n",
      "Batch number : 319, Training: Loss:  0.5280, Accuracy: 0.8281\n",
      "Batch number : 320, Training: Loss:  0.3275, Accuracy: 0.9062\n",
      "Batch number : 321, Training: Loss:  0.4076, Accuracy: 0.8750\n",
      "Batch number : 322, Training: Loss:  0.5675, Accuracy: 0.8281\n",
      "Batch number : 323, Training: Loss:  0.2553, Accuracy: 0.9219\n",
      "Batch number : 324, Training: Loss:  0.6069, Accuracy: 0.7500\n",
      "Batch number : 325, Training: Loss:  0.4393, Accuracy: 0.8750\n",
      "Batch number : 326, Training: Loss:  0.5884, Accuracy: 0.7656\n",
      "Batch number : 327, Training: Loss:  0.5427, Accuracy: 0.8594\n",
      "Batch number : 328, Training: Loss:  0.2740, Accuracy: 0.9219\n",
      "Batch number : 329, Training: Loss:  0.4020, Accuracy: 0.8906\n",
      "Batch number : 330, Training: Loss:  0.4532, Accuracy: 0.8438\n",
      "Batch number : 331, Training: Loss:  0.2155, Accuracy: 0.9531\n",
      "Batch number : 332, Training: Loss:  0.4707, Accuracy: 0.8438\n",
      "Batch number : 333, Training: Loss:  0.4249, Accuracy: 0.8594\n",
      "Batch number : 334, Training: Loss:  0.4688, Accuracy: 0.8594\n",
      "Batch number : 335, Training: Loss:  0.3455, Accuracy: 0.8750\n",
      "Batch number : 336, Training: Loss:  0.3725, Accuracy: 0.9062\n",
      "Batch number : 337, Training: Loss:  0.5759, Accuracy: 0.8750\n",
      "Batch number : 338, Training: Loss:  0.3629, Accuracy: 0.8906\n",
      "Batch number : 339, Training: Loss:  0.5312, Accuracy: 0.8438\n",
      "Batch number : 340, Training: Loss:  0.4652, Accuracy: 0.8906\n",
      "Batch number : 341, Training: Loss:  0.5121, Accuracy: 0.8281\n",
      "Batch number : 342, Training: Loss:  0.3916, Accuracy: 0.9062\n",
      "Batch number : 343, Training: Loss:  0.5667, Accuracy: 0.7969\n",
      "Batch number : 344, Training: Loss:  0.3866, Accuracy: 0.8750\n",
      "Batch number : 345, Training: Loss:  0.3469, Accuracy: 0.9219\n",
      "Batch number : 346, Training: Loss:  0.5233, Accuracy: 0.8438\n",
      "Batch number : 347, Training: Loss:  0.4581, Accuracy: 0.8438\n",
      "Batch number : 348, Training: Loss:  0.7325, Accuracy: 0.7656\n",
      "Batch number : 349, Training: Loss:  0.4292, Accuracy: 0.8750\n",
      "Batch number : 350, Training: Loss:  0.6102, Accuracy: 0.8125\n",
      "Batch number : 351, Training: Loss:  0.4284, Accuracy: 0.8750\n",
      "Batch number : 352, Training: Loss:  0.5711, Accuracy: 0.7969\n",
      "Batch number : 353, Training: Loss:  0.6711, Accuracy: 0.7969\n",
      "Batch number : 354, Training: Loss:  0.4020, Accuracy: 0.8594\n",
      "Batch number : 355, Training: Loss:  0.3428, Accuracy: 0.9062\n",
      "Batch number : 356, Training: Loss:  0.3757, Accuracy: 0.9062\n",
      "Batch number : 357, Training: Loss:  0.4739, Accuracy: 0.8594\n",
      "Batch number : 358, Training: Loss:  0.5324, Accuracy: 0.8750\n",
      "Batch number : 359, Training: Loss:  0.6268, Accuracy: 0.7969\n",
      "Batch number : 360, Training: Loss:  0.4276, Accuracy: 0.8906\n",
      "Batch number : 361, Training: Loss:  0.6206, Accuracy: 0.8281\n",
      "Batch number : 362, Training: Loss:  0.4220, Accuracy: 0.8594\n",
      "Batch number : 363, Training: Loss:  0.3771, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.5167, Accuracy: 0.8594\n",
      "Batch number : 365, Training: Loss:  0.4690, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.7151, Accuracy: 0.7656\n",
      "Batch number : 367, Training: Loss:  0.4129, Accuracy: 0.8750\n",
      "Batch number : 368, Training: Loss:  0.4225, Accuracy: 0.8438\n",
      "Batch number : 369, Training: Loss:  0.3577, Accuracy: 0.8750\n",
      "Batch number : 370, Training: Loss:  0.5162, Accuracy: 0.8281\n",
      "Batch number : 371, Training: Loss:  0.2952, Accuracy: 0.9375\n",
      "Batch number : 372, Training: Loss:  0.2991, Accuracy: 0.9062\n",
      "Batch number : 373, Training: Loss:  0.5025, Accuracy: 0.8438\n",
      "Batch number : 374, Training: Loss:  0.3084, Accuracy: 0.9531\n",
      "Batch number : 375, Training: Loss:  0.4611, Accuracy: 0.8281\n",
      "Batch number : 376, Training: Loss:  0.4846, Accuracy: 0.8906\n",
      "Batch number : 377, Training: Loss:  0.6477, Accuracy: 0.7750\n",
      "Epoch: 19/20\n",
      "Batch number : 000, Training: Loss:  0.5871, Accuracy: 0.7969\n",
      "Batch number : 001, Training: Loss:  0.4164, Accuracy: 0.8750\n",
      "Batch number : 002, Training: Loss:  0.4585, Accuracy: 0.8594\n",
      "Batch number : 003, Training: Loss:  0.3212, Accuracy: 0.9375\n",
      "Batch number : 004, Training: Loss:  0.5459, Accuracy: 0.8281\n",
      "Batch number : 005, Training: Loss:  0.3742, Accuracy: 0.8906\n",
      "Batch number : 006, Training: Loss:  0.3065, Accuracy: 0.9219\n",
      "Batch number : 007, Training: Loss:  0.7318, Accuracy: 0.7656\n",
      "Batch number : 008, Training: Loss:  0.4549, Accuracy: 0.8594\n",
      "Batch number : 009, Training: Loss:  0.6008, Accuracy: 0.8125\n",
      "Batch number : 010, Training: Loss:  0.4858, Accuracy: 0.8594\n",
      "Batch number : 011, Training: Loss:  0.4669, Accuracy: 0.8750\n",
      "Batch number : 012, Training: Loss:  0.5350, Accuracy: 0.8750\n",
      "Batch number : 013, Training: Loss:  0.3891, Accuracy: 0.8594\n",
      "Batch number : 014, Training: Loss:  0.3490, Accuracy: 0.9219\n",
      "Batch number : 015, Training: Loss:  0.2961, Accuracy: 0.9062\n",
      "Batch number : 016, Training: Loss:  0.7080, Accuracy: 0.7188\n",
      "Batch number : 017, Training: Loss:  0.3492, Accuracy: 0.8594\n",
      "Batch number : 018, Training: Loss:  0.3138, Accuracy: 0.9219\n",
      "Batch number : 019, Training: Loss:  0.4100, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.3998, Accuracy: 0.8906\n",
      "Batch number : 021, Training: Loss:  0.5720, Accuracy: 0.8281\n",
      "Batch number : 022, Training: Loss:  0.3184, Accuracy: 0.9219\n",
      "Batch number : 023, Training: Loss:  0.4914, Accuracy: 0.8438\n",
      "Batch number : 024, Training: Loss:  0.4657, Accuracy: 0.8438\n",
      "Batch number : 025, Training: Loss:  0.3640, Accuracy: 0.9062\n",
      "Batch number : 026, Training: Loss:  0.3262, Accuracy: 0.9219\n",
      "Batch number : 027, Training: Loss:  0.6207, Accuracy: 0.8281\n",
      "Batch number : 028, Training: Loss:  0.5637, Accuracy: 0.8281\n",
      "Batch number : 029, Training: Loss:  0.3112, Accuracy: 0.9219\n",
      "Batch number : 030, Training: Loss:  0.3850, Accuracy: 0.8594\n",
      "Batch number : 031, Training: Loss:  0.3275, Accuracy: 0.8906\n",
      "Batch number : 032, Training: Loss:  0.6758, Accuracy: 0.7812\n",
      "Batch number : 033, Training: Loss:  0.5751, Accuracy: 0.8125\n",
      "Batch number : 034, Training: Loss:  0.3784, Accuracy: 0.8906\n",
      "Batch number : 035, Training: Loss:  0.6969, Accuracy: 0.7969\n",
      "Batch number : 036, Training: Loss:  0.4015, Accuracy: 0.8750\n",
      "Batch number : 037, Training: Loss:  0.4917, Accuracy: 0.8281\n",
      "Batch number : 038, Training: Loss:  0.3215, Accuracy: 0.8906\n",
      "Batch number : 039, Training: Loss:  0.4525, Accuracy: 0.8906\n",
      "Batch number : 040, Training: Loss:  0.4758, Accuracy: 0.8438\n",
      "Batch number : 041, Training: Loss:  0.4989, Accuracy: 0.8281\n",
      "Batch number : 042, Training: Loss:  0.4155, Accuracy: 0.8906\n",
      "Batch number : 043, Training: Loss:  0.6239, Accuracy: 0.8125\n",
      "Batch number : 044, Training: Loss:  0.8433, Accuracy: 0.7500\n",
      "Batch number : 045, Training: Loss:  0.4636, Accuracy: 0.8438\n",
      "Batch number : 046, Training: Loss:  0.4119, Accuracy: 0.8594\n",
      "Batch number : 047, Training: Loss:  0.4764, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.5471, Accuracy: 0.8125\n",
      "Batch number : 049, Training: Loss:  0.4065, Accuracy: 0.9219\n",
      "Batch number : 050, Training: Loss:  0.5118, Accuracy: 0.8281\n",
      "Batch number : 051, Training: Loss:  0.4976, Accuracy: 0.8438\n",
      "Batch number : 052, Training: Loss:  0.3691, Accuracy: 0.8750\n",
      "Batch number : 053, Training: Loss:  0.5407, Accuracy: 0.8438\n",
      "Batch number : 054, Training: Loss:  0.4826, Accuracy: 0.8750\n",
      "Batch number : 055, Training: Loss:  0.5674, Accuracy: 0.8906\n",
      "Batch number : 056, Training: Loss:  0.4384, Accuracy: 0.8438\n",
      "Batch number : 057, Training: Loss:  0.4606, Accuracy: 0.8594\n",
      "Batch number : 058, Training: Loss:  0.3831, Accuracy: 0.8906\n",
      "Batch number : 059, Training: Loss:  0.3738, Accuracy: 0.8906\n",
      "Batch number : 060, Training: Loss:  0.3809, Accuracy: 0.9062\n",
      "Batch number : 061, Training: Loss:  0.3762, Accuracy: 0.8750\n",
      "Batch number : 062, Training: Loss:  0.3344, Accuracy: 0.8906\n",
      "Batch number : 063, Training: Loss:  0.5919, Accuracy: 0.7969\n",
      "Batch number : 064, Training: Loss:  0.2097, Accuracy: 0.9531\n",
      "Batch number : 065, Training: Loss:  0.3380, Accuracy: 0.9062\n",
      "Batch number : 066, Training: Loss:  0.4954, Accuracy: 0.8438\n",
      "Batch number : 067, Training: Loss:  0.4536, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.5579, Accuracy: 0.8281\n",
      "Batch number : 069, Training: Loss:  0.5131, Accuracy: 0.8281\n",
      "Batch number : 070, Training: Loss:  0.4732, Accuracy: 0.8438\n",
      "Batch number : 071, Training: Loss:  0.5827, Accuracy: 0.8438\n",
      "Batch number : 072, Training: Loss:  0.4255, Accuracy: 0.9062\n",
      "Batch number : 073, Training: Loss:  0.4580, Accuracy: 0.8594\n",
      "Batch number : 074, Training: Loss:  0.4976, Accuracy: 0.8594\n",
      "Batch number : 075, Training: Loss:  0.4782, Accuracy: 0.8750\n",
      "Batch number : 076, Training: Loss:  0.3376, Accuracy: 0.9062\n",
      "Batch number : 077, Training: Loss:  0.6627, Accuracy: 0.7656\n",
      "Batch number : 078, Training: Loss:  0.3244, Accuracy: 0.9219\n",
      "Batch number : 079, Training: Loss:  0.6479, Accuracy: 0.7500\n",
      "Batch number : 080, Training: Loss:  0.3707, Accuracy: 0.8594\n",
      "Batch number : 081, Training: Loss:  0.3543, Accuracy: 0.8906\n",
      "Batch number : 082, Training: Loss:  0.4985, Accuracy: 0.8281\n",
      "Batch number : 083, Training: Loss:  0.3155, Accuracy: 0.8906\n",
      "Batch number : 084, Training: Loss:  0.4735, Accuracy: 0.8750\n",
      "Batch number : 085, Training: Loss:  0.5414, Accuracy: 0.8438\n",
      "Batch number : 086, Training: Loss:  0.3909, Accuracy: 0.8906\n",
      "Batch number : 087, Training: Loss:  0.6308, Accuracy: 0.8281\n",
      "Batch number : 088, Training: Loss:  0.3930, Accuracy: 0.9219\n",
      "Batch number : 089, Training: Loss:  0.4664, Accuracy: 0.8750\n",
      "Batch number : 090, Training: Loss:  0.4067, Accuracy: 0.8750\n",
      "Batch number : 091, Training: Loss:  0.3913, Accuracy: 0.8594\n",
      "Batch number : 092, Training: Loss:  0.7668, Accuracy: 0.8125\n",
      "Batch number : 093, Training: Loss:  0.4259, Accuracy: 0.8594\n",
      "Batch number : 094, Training: Loss:  0.4433, Accuracy: 0.8750\n",
      "Batch number : 095, Training: Loss:  0.4712, Accuracy: 0.8281\n",
      "Batch number : 096, Training: Loss:  0.3556, Accuracy: 0.9219\n",
      "Batch number : 097, Training: Loss:  0.3245, Accuracy: 0.9219\n",
      "Batch number : 098, Training: Loss:  0.4688, Accuracy: 0.8281\n",
      "Batch number : 099, Training: Loss:  0.3245, Accuracy: 0.9062\n",
      "Batch number : 100, Training: Loss:  0.4953, Accuracy: 0.8438\n",
      "Batch number : 101, Training: Loss:  0.3534, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.5009, Accuracy: 0.8594\n",
      "Batch number : 103, Training: Loss:  0.5191, Accuracy: 0.8750\n",
      "Batch number : 104, Training: Loss:  0.5233, Accuracy: 0.8125\n",
      "Batch number : 105, Training: Loss:  0.3718, Accuracy: 0.9219\n",
      "Batch number : 106, Training: Loss:  0.4005, Accuracy: 0.8594\n",
      "Batch number : 107, Training: Loss:  0.5189, Accuracy: 0.8281\n",
      "Batch number : 108, Training: Loss:  0.4336, Accuracy: 0.8906\n",
      "Batch number : 109, Training: Loss:  0.3193, Accuracy: 0.9062\n",
      "Batch number : 110, Training: Loss:  0.4507, Accuracy: 0.8594\n",
      "Batch number : 111, Training: Loss:  0.2749, Accuracy: 0.9219\n",
      "Batch number : 112, Training: Loss:  0.3016, Accuracy: 0.9375\n",
      "Batch number : 113, Training: Loss:  0.4464, Accuracy: 0.8906\n",
      "Batch number : 114, Training: Loss:  0.5217, Accuracy: 0.8438\n",
      "Batch number : 115, Training: Loss:  0.4602, Accuracy: 0.8750\n",
      "Batch number : 116, Training: Loss:  0.5949, Accuracy: 0.8281\n",
      "Batch number : 117, Training: Loss:  0.4630, Accuracy: 0.8438\n",
      "Batch number : 118, Training: Loss:  0.4017, Accuracy: 0.9062\n",
      "Batch number : 119, Training: Loss:  0.3758, Accuracy: 0.8750\n",
      "Batch number : 120, Training: Loss:  0.4636, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.2347, Accuracy: 0.9531\n",
      "Batch number : 122, Training: Loss:  0.5572, Accuracy: 0.8125\n",
      "Batch number : 123, Training: Loss:  0.3852, Accuracy: 0.8594\n",
      "Batch number : 124, Training: Loss:  0.4326, Accuracy: 0.8594\n",
      "Batch number : 125, Training: Loss:  0.4341, Accuracy: 0.8594\n",
      "Batch number : 126, Training: Loss:  0.3658, Accuracy: 0.9062\n",
      "Batch number : 127, Training: Loss:  0.3533, Accuracy: 0.8594\n",
      "Batch number : 128, Training: Loss:  0.2335, Accuracy: 0.9531\n",
      "Batch number : 129, Training: Loss:  0.3235, Accuracy: 0.9062\n",
      "Batch number : 130, Training: Loss:  0.4249, Accuracy: 0.8750\n",
      "Batch number : 131, Training: Loss:  0.2414, Accuracy: 0.9531\n",
      "Batch number : 132, Training: Loss:  0.3569, Accuracy: 0.8906\n",
      "Batch number : 133, Training: Loss:  0.5100, Accuracy: 0.8594\n",
      "Batch number : 134, Training: Loss:  0.5321, Accuracy: 0.8281\n",
      "Batch number : 135, Training: Loss:  0.6037, Accuracy: 0.8750\n",
      "Batch number : 136, Training: Loss:  0.6798, Accuracy: 0.7969\n",
      "Batch number : 137, Training: Loss:  0.2336, Accuracy: 0.9531\n",
      "Batch number : 138, Training: Loss:  0.5886, Accuracy: 0.7969\n",
      "Batch number : 139, Training: Loss:  0.3684, Accuracy: 0.8906\n",
      "Batch number : 140, Training: Loss:  0.5241, Accuracy: 0.8125\n",
      "Batch number : 141, Training: Loss:  0.5614, Accuracy: 0.7969\n",
      "Batch number : 142, Training: Loss:  0.5940, Accuracy: 0.8438\n",
      "Batch number : 143, Training: Loss:  0.4333, Accuracy: 0.9062\n",
      "Batch number : 144, Training: Loss:  0.5072, Accuracy: 0.8281\n",
      "Batch number : 145, Training: Loss:  0.2866, Accuracy: 0.9375\n",
      "Batch number : 146, Training: Loss:  0.5650, Accuracy: 0.8281\n",
      "Batch number : 147, Training: Loss:  0.6501, Accuracy: 0.8750\n",
      "Batch number : 148, Training: Loss:  0.3943, Accuracy: 0.8906\n",
      "Batch number : 149, Training: Loss:  0.6347, Accuracy: 0.7344\n",
      "Batch number : 150, Training: Loss:  0.7229, Accuracy: 0.7500\n",
      "Batch number : 151, Training: Loss:  0.4370, Accuracy: 0.8594\n",
      "Batch number : 152, Training: Loss:  0.4048, Accuracy: 0.9062\n",
      "Batch number : 153, Training: Loss:  0.8132, Accuracy: 0.7344\n",
      "Batch number : 154, Training: Loss:  0.5591, Accuracy: 0.8594\n",
      "Batch number : 155, Training: Loss:  0.6607, Accuracy: 0.7969\n",
      "Batch number : 156, Training: Loss:  0.5230, Accuracy: 0.8438\n",
      "Batch number : 157, Training: Loss:  0.6257, Accuracy: 0.7969\n",
      "Batch number : 158, Training: Loss:  0.2803, Accuracy: 0.9531\n",
      "Batch number : 159, Training: Loss:  0.3723, Accuracy: 0.9219\n",
      "Batch number : 160, Training: Loss:  0.4545, Accuracy: 0.8750\n",
      "Batch number : 161, Training: Loss:  0.6831, Accuracy: 0.7188\n",
      "Batch number : 162, Training: Loss:  0.5746, Accuracy: 0.8438\n",
      "Batch number : 163, Training: Loss:  0.5207, Accuracy: 0.8125\n",
      "Batch number : 164, Training: Loss:  0.3753, Accuracy: 0.8906\n",
      "Batch number : 165, Training: Loss:  0.6791, Accuracy: 0.7500\n",
      "Batch number : 166, Training: Loss:  0.4758, Accuracy: 0.8594\n",
      "Batch number : 167, Training: Loss:  0.3609, Accuracy: 0.9062\n",
      "Batch number : 168, Training: Loss:  0.4111, Accuracy: 0.8906\n",
      "Batch number : 169, Training: Loss:  0.5241, Accuracy: 0.8281\n",
      "Batch number : 170, Training: Loss:  0.5623, Accuracy: 0.8281\n",
      "Batch number : 171, Training: Loss:  0.5393, Accuracy: 0.8750\n",
      "Batch number : 172, Training: Loss:  0.3758, Accuracy: 0.8906\n",
      "Batch number : 173, Training: Loss:  0.2630, Accuracy: 0.9219\n",
      "Batch number : 174, Training: Loss:  0.4239, Accuracy: 0.8906\n",
      "Batch number : 175, Training: Loss:  0.6047, Accuracy: 0.8125\n",
      "Batch number : 176, Training: Loss:  0.4730, Accuracy: 0.8438\n",
      "Batch number : 177, Training: Loss:  0.3007, Accuracy: 0.8906\n",
      "Batch number : 178, Training: Loss:  0.4115, Accuracy: 0.8750\n",
      "Batch number : 179, Training: Loss:  0.5957, Accuracy: 0.8594\n",
      "Batch number : 180, Training: Loss:  0.6376, Accuracy: 0.8125\n",
      "Batch number : 181, Training: Loss:  0.3421, Accuracy: 0.9219\n",
      "Batch number : 182, Training: Loss:  0.6128, Accuracy: 0.8125\n",
      "Batch number : 183, Training: Loss:  0.7568, Accuracy: 0.7656\n",
      "Batch number : 184, Training: Loss:  0.5277, Accuracy: 0.8281\n",
      "Batch number : 185, Training: Loss:  0.4026, Accuracy: 0.8906\n",
      "Batch number : 186, Training: Loss:  0.5887, Accuracy: 0.8438\n",
      "Batch number : 187, Training: Loss:  0.4468, Accuracy: 0.8750\n",
      "Batch number : 188, Training: Loss:  0.4492, Accuracy: 0.8594\n",
      "Batch number : 189, Training: Loss:  0.5493, Accuracy: 0.7812\n",
      "Batch number : 190, Training: Loss:  0.4534, Accuracy: 0.8594\n",
      "Batch number : 191, Training: Loss:  0.4895, Accuracy: 0.8281\n",
      "Batch number : 192, Training: Loss:  0.4917, Accuracy: 0.8281\n",
      "Batch number : 193, Training: Loss:  0.3313, Accuracy: 0.9062\n",
      "Batch number : 194, Training: Loss:  0.3614, Accuracy: 0.8906\n",
      "Batch number : 195, Training: Loss:  0.4575, Accuracy: 0.8438\n",
      "Batch number : 196, Training: Loss:  0.4580, Accuracy: 0.9062\n",
      "Batch number : 197, Training: Loss:  0.5094, Accuracy: 0.8438\n",
      "Batch number : 198, Training: Loss:  0.3891, Accuracy: 0.8906\n",
      "Batch number : 199, Training: Loss:  0.5512, Accuracy: 0.8438\n",
      "Batch number : 200, Training: Loss:  0.5725, Accuracy: 0.8438\n",
      "Batch number : 201, Training: Loss:  0.3365, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.2025, Accuracy: 0.9531\n",
      "Batch number : 203, Training: Loss:  0.5671, Accuracy: 0.8438\n",
      "Batch number : 204, Training: Loss:  0.3554, Accuracy: 0.9219\n",
      "Batch number : 205, Training: Loss:  0.4549, Accuracy: 0.8750\n",
      "Batch number : 206, Training: Loss:  0.4512, Accuracy: 0.8281\n",
      "Batch number : 207, Training: Loss:  0.5248, Accuracy: 0.8750\n",
      "Batch number : 208, Training: Loss:  0.5427, Accuracy: 0.8281\n",
      "Batch number : 209, Training: Loss:  0.3965, Accuracy: 0.8750\n",
      "Batch number : 210, Training: Loss:  0.4426, Accuracy: 0.8594\n",
      "Batch number : 211, Training: Loss:  0.4973, Accuracy: 0.8750\n",
      "Batch number : 212, Training: Loss:  0.4051, Accuracy: 0.8906\n",
      "Batch number : 213, Training: Loss:  0.3565, Accuracy: 0.8594\n",
      "Batch number : 214, Training: Loss:  0.3275, Accuracy: 0.9219\n",
      "Batch number : 215, Training: Loss:  0.6289, Accuracy: 0.7969\n",
      "Batch number : 216, Training: Loss:  0.4813, Accuracy: 0.8438\n",
      "Batch number : 217, Training: Loss:  0.3601, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.4033, Accuracy: 0.8750\n",
      "Batch number : 219, Training: Loss:  0.3517, Accuracy: 0.9062\n",
      "Batch number : 220, Training: Loss:  0.2968, Accuracy: 0.8906\n",
      "Batch number : 221, Training: Loss:  0.4865, Accuracy: 0.8281\n",
      "Batch number : 222, Training: Loss:  0.3843, Accuracy: 0.9062\n",
      "Batch number : 223, Training: Loss:  0.4190, Accuracy: 0.8750\n",
      "Batch number : 224, Training: Loss:  0.5148, Accuracy: 0.8125\n",
      "Batch number : 225, Training: Loss:  0.3782, Accuracy: 0.8906\n",
      "Batch number : 226, Training: Loss:  0.4026, Accuracy: 0.8594\n",
      "Batch number : 227, Training: Loss:  0.3531, Accuracy: 0.9219\n",
      "Batch number : 228, Training: Loss:  0.2827, Accuracy: 0.9219\n",
      "Batch number : 229, Training: Loss:  0.3744, Accuracy: 0.9062\n",
      "Batch number : 230, Training: Loss:  0.5242, Accuracy: 0.8281\n",
      "Batch number : 231, Training: Loss:  0.5412, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.4355, Accuracy: 0.8438\n",
      "Batch number : 233, Training: Loss:  0.7389, Accuracy: 0.7656\n",
      "Batch number : 234, Training: Loss:  0.5103, Accuracy: 0.8750\n",
      "Batch number : 235, Training: Loss:  0.4884, Accuracy: 0.8594\n",
      "Batch number : 236, Training: Loss:  0.5507, Accuracy: 0.8438\n",
      "Batch number : 237, Training: Loss:  0.5220, Accuracy: 0.8906\n",
      "Batch number : 238, Training: Loss:  0.6114, Accuracy: 0.7812\n",
      "Batch number : 239, Training: Loss:  0.5033, Accuracy: 0.8750\n",
      "Batch number : 240, Training: Loss:  0.4506, Accuracy: 0.9062\n",
      "Batch number : 241, Training: Loss:  0.4278, Accuracy: 0.8750\n",
      "Batch number : 242, Training: Loss:  0.3569, Accuracy: 0.9062\n",
      "Batch number : 243, Training: Loss:  0.3456, Accuracy: 0.8906\n",
      "Batch number : 244, Training: Loss:  0.5398, Accuracy: 0.8438\n",
      "Batch number : 245, Training: Loss:  0.4521, Accuracy: 0.8750\n",
      "Batch number : 246, Training: Loss:  0.4470, Accuracy: 0.8594\n",
      "Batch number : 247, Training: Loss:  0.5473, Accuracy: 0.8125\n",
      "Batch number : 248, Training: Loss:  0.4988, Accuracy: 0.8125\n",
      "Batch number : 249, Training: Loss:  0.5212, Accuracy: 0.8125\n",
      "Batch number : 250, Training: Loss:  0.4118, Accuracy: 0.8281\n",
      "Batch number : 251, Training: Loss:  0.4871, Accuracy: 0.8594\n",
      "Batch number : 252, Training: Loss:  0.2927, Accuracy: 0.9062\n",
      "Batch number : 253, Training: Loss:  0.4809, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.6428, Accuracy: 0.8438\n",
      "Batch number : 255, Training: Loss:  0.6368, Accuracy: 0.8438\n",
      "Batch number : 256, Training: Loss:  0.5576, Accuracy: 0.8594\n",
      "Batch number : 257, Training: Loss:  0.4803, Accuracy: 0.8594\n",
      "Batch number : 258, Training: Loss:  0.5529, Accuracy: 0.8125\n",
      "Batch number : 259, Training: Loss:  0.4387, Accuracy: 0.8438\n",
      "Batch number : 260, Training: Loss:  0.4854, Accuracy: 0.8750\n",
      "Batch number : 261, Training: Loss:  0.4383, Accuracy: 0.8906\n",
      "Batch number : 262, Training: Loss:  0.4540, Accuracy: 0.8281\n",
      "Batch number : 263, Training: Loss:  0.6229, Accuracy: 0.8125\n",
      "Batch number : 264, Training: Loss:  0.2879, Accuracy: 0.9062\n",
      "Batch number : 265, Training: Loss:  0.6129, Accuracy: 0.8281\n",
      "Batch number : 266, Training: Loss:  0.4059, Accuracy: 0.8750\n",
      "Batch number : 267, Training: Loss:  0.6709, Accuracy: 0.7969\n",
      "Batch number : 268, Training: Loss:  0.5691, Accuracy: 0.7969\n",
      "Batch number : 269, Training: Loss:  0.3761, Accuracy: 0.9062\n",
      "Batch number : 270, Training: Loss:  0.5502, Accuracy: 0.8281\n",
      "Batch number : 271, Training: Loss:  0.4779, Accuracy: 0.8438\n",
      "Batch number : 272, Training: Loss:  0.3513, Accuracy: 0.9219\n",
      "Batch number : 273, Training: Loss:  0.4578, Accuracy: 0.8281\n",
      "Batch number : 274, Training: Loss:  0.4692, Accuracy: 0.8438\n",
      "Batch number : 275, Training: Loss:  0.3764, Accuracy: 0.8750\n",
      "Batch number : 276, Training: Loss:  0.8254, Accuracy: 0.7500\n",
      "Batch number : 277, Training: Loss:  0.5437, Accuracy: 0.7812\n",
      "Batch number : 278, Training: Loss:  0.7090, Accuracy: 0.7656\n",
      "Batch number : 279, Training: Loss:  0.3488, Accuracy: 0.9219\n",
      "Batch number : 280, Training: Loss:  0.6455, Accuracy: 0.7812\n",
      "Batch number : 281, Training: Loss:  0.3302, Accuracy: 0.9219\n",
      "Batch number : 282, Training: Loss:  0.3312, Accuracy: 0.9219\n",
      "Batch number : 283, Training: Loss:  0.4508, Accuracy: 0.8438\n",
      "Batch number : 284, Training: Loss:  0.3845, Accuracy: 0.8438\n",
      "Batch number : 285, Training: Loss:  0.4076, Accuracy: 0.8594\n",
      "Batch number : 286, Training: Loss:  0.5742, Accuracy: 0.8125\n",
      "Batch number : 287, Training: Loss:  0.6531, Accuracy: 0.8125\n",
      "Batch number : 288, Training: Loss:  0.4054, Accuracy: 0.8906\n",
      "Batch number : 289, Training: Loss:  0.4491, Accuracy: 0.8750\n",
      "Batch number : 290, Training: Loss:  0.5433, Accuracy: 0.7969\n",
      "Batch number : 291, Training: Loss:  0.5704, Accuracy: 0.8125\n",
      "Batch number : 292, Training: Loss:  0.5759, Accuracy: 0.8125\n",
      "Batch number : 293, Training: Loss:  0.3034, Accuracy: 0.9375\n",
      "Batch number : 294, Training: Loss:  0.7941, Accuracy: 0.7656\n",
      "Batch number : 295, Training: Loss:  0.4904, Accuracy: 0.8594\n",
      "Batch number : 296, Training: Loss:  0.5332, Accuracy: 0.8438\n",
      "Batch number : 297, Training: Loss:  0.4133, Accuracy: 0.8750\n",
      "Batch number : 298, Training: Loss:  0.4772, Accuracy: 0.8750\n",
      "Batch number : 299, Training: Loss:  0.3384, Accuracy: 0.9062\n",
      "Batch number : 300, Training: Loss:  0.5183, Accuracy: 0.8594\n",
      "Batch number : 301, Training: Loss:  0.6022, Accuracy: 0.8125\n",
      "Batch number : 302, Training: Loss:  0.4230, Accuracy: 0.8906\n",
      "Batch number : 303, Training: Loss:  0.4706, Accuracy: 0.8750\n",
      "Batch number : 304, Training: Loss:  0.3559, Accuracy: 0.9062\n",
      "Batch number : 305, Training: Loss:  0.3693, Accuracy: 0.8906\n",
      "Batch number : 306, Training: Loss:  0.4209, Accuracy: 0.8750\n",
      "Batch number : 307, Training: Loss:  0.5525, Accuracy: 0.8281\n",
      "Batch number : 308, Training: Loss:  0.7568, Accuracy: 0.7812\n",
      "Batch number : 309, Training: Loss:  0.3981, Accuracy: 0.8750\n",
      "Batch number : 310, Training: Loss:  0.2602, Accuracy: 0.9375\n",
      "Batch number : 311, Training: Loss:  0.3865, Accuracy: 0.8438\n",
      "Batch number : 312, Training: Loss:  0.6677, Accuracy: 0.8125\n",
      "Batch number : 313, Training: Loss:  0.3835, Accuracy: 0.8906\n",
      "Batch number : 314, Training: Loss:  0.5157, Accuracy: 0.8438\n",
      "Batch number : 315, Training: Loss:  0.3838, Accuracy: 0.8750\n",
      "Batch number : 316, Training: Loss:  0.5387, Accuracy: 0.8906\n",
      "Batch number : 317, Training: Loss:  0.6394, Accuracy: 0.8125\n",
      "Batch number : 318, Training: Loss:  0.3537, Accuracy: 0.9062\n",
      "Batch number : 319, Training: Loss:  0.7236, Accuracy: 0.7344\n",
      "Batch number : 320, Training: Loss:  0.4806, Accuracy: 0.8594\n",
      "Batch number : 321, Training: Loss:  0.4845, Accuracy: 0.8438\n",
      "Batch number : 322, Training: Loss:  0.4673, Accuracy: 0.8750\n",
      "Batch number : 323, Training: Loss:  0.4056, Accuracy: 0.8906\n",
      "Batch number : 324, Training: Loss:  0.5586, Accuracy: 0.8281\n",
      "Batch number : 325, Training: Loss:  0.4777, Accuracy: 0.8594\n",
      "Batch number : 326, Training: Loss:  0.4462, Accuracy: 0.8281\n",
      "Batch number : 327, Training: Loss:  0.6058, Accuracy: 0.8281\n",
      "Batch number : 328, Training: Loss:  0.4931, Accuracy: 0.8438\n",
      "Batch number : 329, Training: Loss:  0.3013, Accuracy: 0.9219\n",
      "Batch number : 330, Training: Loss:  0.4234, Accuracy: 0.8594\n",
      "Batch number : 331, Training: Loss:  0.5302, Accuracy: 0.8438\n",
      "Batch number : 332, Training: Loss:  0.5247, Accuracy: 0.8906\n",
      "Batch number : 333, Training: Loss:  0.6398, Accuracy: 0.7656\n",
      "Batch number : 334, Training: Loss:  0.4451, Accuracy: 0.8906\n",
      "Batch number : 335, Training: Loss:  0.3832, Accuracy: 0.8906\n",
      "Batch number : 336, Training: Loss:  0.3472, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.7418, Accuracy: 0.7812\n",
      "Batch number : 338, Training: Loss:  0.4025, Accuracy: 0.8906\n",
      "Batch number : 339, Training: Loss:  0.4833, Accuracy: 0.8438\n",
      "Batch number : 340, Training: Loss:  0.4181, Accuracy: 0.8906\n",
      "Batch number : 341, Training: Loss:  0.4173, Accuracy: 0.8438\n",
      "Batch number : 342, Training: Loss:  0.3403, Accuracy: 0.9062\n",
      "Batch number : 343, Training: Loss:  0.4175, Accuracy: 0.8594\n",
      "Batch number : 344, Training: Loss:  0.4938, Accuracy: 0.8438\n",
      "Batch number : 345, Training: Loss:  0.4552, Accuracy: 0.8750\n",
      "Batch number : 346, Training: Loss:  0.3315, Accuracy: 0.8906\n",
      "Batch number : 347, Training: Loss:  0.5448, Accuracy: 0.7969\n",
      "Batch number : 348, Training: Loss:  0.6366, Accuracy: 0.8125\n",
      "Batch number : 349, Training: Loss:  0.5584, Accuracy: 0.7969\n",
      "Batch number : 350, Training: Loss:  0.3235, Accuracy: 0.9219\n",
      "Batch number : 351, Training: Loss:  0.3520, Accuracy: 0.8750\n",
      "Batch number : 352, Training: Loss:  0.4267, Accuracy: 0.9062\n",
      "Batch number : 353, Training: Loss:  0.5085, Accuracy: 0.8750\n",
      "Batch number : 354, Training: Loss:  0.4776, Accuracy: 0.8750\n",
      "Batch number : 355, Training: Loss:  0.5272, Accuracy: 0.8594\n",
      "Batch number : 356, Training: Loss:  0.7294, Accuracy: 0.7656\n",
      "Batch number : 357, Training: Loss:  0.4243, Accuracy: 0.8750\n",
      "Batch number : 358, Training: Loss:  0.3178, Accuracy: 0.9062\n",
      "Batch number : 359, Training: Loss:  0.3563, Accuracy: 0.8906\n",
      "Batch number : 360, Training: Loss:  0.5498, Accuracy: 0.8438\n",
      "Batch number : 361, Training: Loss:  0.2830, Accuracy: 0.9375\n",
      "Batch number : 362, Training: Loss:  0.2881, Accuracy: 0.9375\n",
      "Batch number : 363, Training: Loss:  0.3528, Accuracy: 0.8906\n",
      "Batch number : 364, Training: Loss:  0.3645, Accuracy: 0.8906\n",
      "Batch number : 365, Training: Loss:  0.7527, Accuracy: 0.7500\n",
      "Batch number : 366, Training: Loss:  0.5234, Accuracy: 0.8281\n",
      "Batch number : 367, Training: Loss:  0.5610, Accuracy: 0.8281\n",
      "Batch number : 368, Training: Loss:  0.6953, Accuracy: 0.8125\n",
      "Batch number : 369, Training: Loss:  0.2931, Accuracy: 0.9375\n",
      "Batch number : 370, Training: Loss:  0.3547, Accuracy: 0.9062\n",
      "Batch number : 371, Training: Loss:  0.5017, Accuracy: 0.8281\n",
      "Batch number : 372, Training: Loss:  0.5105, Accuracy: 0.8594\n",
      "Batch number : 373, Training: Loss:  0.5087, Accuracy: 0.8125\n",
      "Batch number : 374, Training: Loss:  0.5569, Accuracy: 0.8281\n",
      "Batch number : 375, Training: Loss:  0.3772, Accuracy: 0.9062\n",
      "Batch number : 376, Training: Loss:  0.4509, Accuracy: 0.8125\n",
      "Batch number : 377, Training: Loss:  0.7878, Accuracy: 0.7250\n",
      "Epoch: 20/20\n",
      "Batch number : 000, Training: Loss:  0.4596, Accuracy: 0.8438\n",
      "Batch number : 001, Training: Loss:  0.5671, Accuracy: 0.8438\n",
      "Batch number : 002, Training: Loss:  0.4199, Accuracy: 0.8594\n",
      "Batch number : 003, Training: Loss:  0.5904, Accuracy: 0.8281\n",
      "Batch number : 004, Training: Loss:  0.4686, Accuracy: 0.8438\n",
      "Batch number : 005, Training: Loss:  0.3805, Accuracy: 0.9219\n",
      "Batch number : 006, Training: Loss:  0.4787, Accuracy: 0.8594\n",
      "Batch number : 007, Training: Loss:  0.4872, Accuracy: 0.8281\n",
      "Batch number : 008, Training: Loss:  0.6473, Accuracy: 0.7656\n",
      "Batch number : 009, Training: Loss:  0.2478, Accuracy: 0.9375\n",
      "Batch number : 010, Training: Loss:  0.3900, Accuracy: 0.8594\n",
      "Batch number : 011, Training: Loss:  0.4698, Accuracy: 0.8438\n",
      "Batch number : 012, Training: Loss:  0.2728, Accuracy: 0.9219\n",
      "Batch number : 013, Training: Loss:  0.4007, Accuracy: 0.8906\n",
      "Batch number : 014, Training: Loss:  0.5037, Accuracy: 0.8594\n",
      "Batch number : 015, Training: Loss:  0.7795, Accuracy: 0.7656\n",
      "Batch number : 016, Training: Loss:  0.4473, Accuracy: 0.8594\n",
      "Batch number : 017, Training: Loss:  0.6754, Accuracy: 0.7969\n",
      "Batch number : 018, Training: Loss:  0.4791, Accuracy: 0.8281\n",
      "Batch number : 019, Training: Loss:  0.4284, Accuracy: 0.8750\n",
      "Batch number : 020, Training: Loss:  0.2562, Accuracy: 0.9219\n",
      "Batch number : 021, Training: Loss:  0.2362, Accuracy: 0.9375\n",
      "Batch number : 022, Training: Loss:  0.4532, Accuracy: 0.8438\n",
      "Batch number : 023, Training: Loss:  0.3401, Accuracy: 0.8750\n",
      "Batch number : 024, Training: Loss:  0.5671, Accuracy: 0.8281\n",
      "Batch number : 025, Training: Loss:  0.3515, Accuracy: 0.9219\n",
      "Batch number : 026, Training: Loss:  0.3508, Accuracy: 0.9062\n",
      "Batch number : 027, Training: Loss:  0.4749, Accuracy: 0.8438\n",
      "Batch number : 028, Training: Loss:  0.4291, Accuracy: 0.8906\n",
      "Batch number : 029, Training: Loss:  0.4499, Accuracy: 0.8750\n",
      "Batch number : 030, Training: Loss:  0.4226, Accuracy: 0.8750\n",
      "Batch number : 031, Training: Loss:  0.3484, Accuracy: 0.8906\n",
      "Batch number : 032, Training: Loss:  0.6574, Accuracy: 0.8125\n",
      "Batch number : 033, Training: Loss:  0.3654, Accuracy: 0.9062\n",
      "Batch number : 034, Training: Loss:  0.4545, Accuracy: 0.8750\n",
      "Batch number : 035, Training: Loss:  0.4256, Accuracy: 0.8750\n",
      "Batch number : 036, Training: Loss:  0.5075, Accuracy: 0.8281\n",
      "Batch number : 037, Training: Loss:  0.3617, Accuracy: 0.8906\n",
      "Batch number : 038, Training: Loss:  0.4073, Accuracy: 0.8906\n",
      "Batch number : 039, Training: Loss:  0.4951, Accuracy: 0.8594\n",
      "Batch number : 040, Training: Loss:  0.5076, Accuracy: 0.8281\n",
      "Batch number : 041, Training: Loss:  0.6931, Accuracy: 0.7969\n",
      "Batch number : 042, Training: Loss:  0.4847, Accuracy: 0.8594\n",
      "Batch number : 043, Training: Loss:  0.3569, Accuracy: 0.8906\n",
      "Batch number : 044, Training: Loss:  0.3810, Accuracy: 0.9062\n",
      "Batch number : 045, Training: Loss:  0.3780, Accuracy: 0.8906\n",
      "Batch number : 046, Training: Loss:  0.6963, Accuracy: 0.8125\n",
      "Batch number : 047, Training: Loss:  0.4663, Accuracy: 0.8594\n",
      "Batch number : 048, Training: Loss:  0.5519, Accuracy: 0.8125\n",
      "Batch number : 049, Training: Loss:  0.3571, Accuracy: 0.8594\n",
      "Batch number : 050, Training: Loss:  0.3659, Accuracy: 0.9062\n",
      "Batch number : 051, Training: Loss:  0.4977, Accuracy: 0.8125\n",
      "Batch number : 052, Training: Loss:  0.7412, Accuracy: 0.7812\n",
      "Batch number : 053, Training: Loss:  0.4367, Accuracy: 0.9219\n",
      "Batch number : 054, Training: Loss:  0.4669, Accuracy: 0.8594\n",
      "Batch number : 055, Training: Loss:  0.3397, Accuracy: 0.8906\n",
      "Batch number : 056, Training: Loss:  0.5988, Accuracy: 0.7969\n",
      "Batch number : 057, Training: Loss:  0.2905, Accuracy: 0.9375\n",
      "Batch number : 058, Training: Loss:  0.4879, Accuracy: 0.8438\n",
      "Batch number : 059, Training: Loss:  0.4805, Accuracy: 0.8750\n",
      "Batch number : 060, Training: Loss:  0.4047, Accuracy: 0.8906\n",
      "Batch number : 061, Training: Loss:  0.3853, Accuracy: 0.8906\n",
      "Batch number : 062, Training: Loss:  0.3511, Accuracy: 0.9219\n",
      "Batch number : 063, Training: Loss:  0.6652, Accuracy: 0.7969\n",
      "Batch number : 064, Training: Loss:  0.4048, Accuracy: 0.8750\n",
      "Batch number : 065, Training: Loss:  0.4276, Accuracy: 0.8438\n",
      "Batch number : 066, Training: Loss:  0.6172, Accuracy: 0.8125\n",
      "Batch number : 067, Training: Loss:  0.3956, Accuracy: 0.8750\n",
      "Batch number : 068, Training: Loss:  0.5900, Accuracy: 0.7969\n",
      "Batch number : 069, Training: Loss:  0.4830, Accuracy: 0.8594\n",
      "Batch number : 070, Training: Loss:  0.5041, Accuracy: 0.7969\n",
      "Batch number : 071, Training: Loss:  0.5139, Accuracy: 0.8750\n",
      "Batch number : 072, Training: Loss:  0.6341, Accuracy: 0.7656\n",
      "Batch number : 073, Training: Loss:  0.5333, Accuracy: 0.8438\n",
      "Batch number : 074, Training: Loss:  0.4422, Accuracy: 0.8750\n",
      "Batch number : 075, Training: Loss:  0.5510, Accuracy: 0.8438\n",
      "Batch number : 076, Training: Loss:  0.3900, Accuracy: 0.8906\n",
      "Batch number : 077, Training: Loss:  0.5626, Accuracy: 0.7812\n",
      "Batch number : 078, Training: Loss:  0.3741, Accuracy: 0.9062\n",
      "Batch number : 079, Training: Loss:  0.5014, Accuracy: 0.8438\n",
      "Batch number : 080, Training: Loss:  0.3817, Accuracy: 0.8750\n",
      "Batch number : 081, Training: Loss:  0.4757, Accuracy: 0.8750\n",
      "Batch number : 082, Training: Loss:  0.4534, Accuracy: 0.8750\n",
      "Batch number : 083, Training: Loss:  0.5300, Accuracy: 0.8438\n",
      "Batch number : 084, Training: Loss:  0.5851, Accuracy: 0.8281\n",
      "Batch number : 085, Training: Loss:  0.3811, Accuracy: 0.9062\n",
      "Batch number : 086, Training: Loss:  0.4755, Accuracy: 0.8594\n",
      "Batch number : 087, Training: Loss:  0.4317, Accuracy: 0.8594\n",
      "Batch number : 088, Training: Loss:  0.5913, Accuracy: 0.8281\n",
      "Batch number : 089, Training: Loss:  0.5083, Accuracy: 0.8594\n",
      "Batch number : 090, Training: Loss:  0.3479, Accuracy: 0.8906\n",
      "Batch number : 091, Training: Loss:  0.3649, Accuracy: 0.9062\n",
      "Batch number : 092, Training: Loss:  0.5271, Accuracy: 0.8750\n",
      "Batch number : 093, Training: Loss:  0.6067, Accuracy: 0.7969\n",
      "Batch number : 094, Training: Loss:  0.5520, Accuracy: 0.8438\n",
      "Batch number : 095, Training: Loss:  0.3834, Accuracy: 0.8906\n",
      "Batch number : 096, Training: Loss:  0.4525, Accuracy: 0.8750\n",
      "Batch number : 097, Training: Loss:  0.4687, Accuracy: 0.8906\n",
      "Batch number : 098, Training: Loss:  0.5217, Accuracy: 0.8281\n",
      "Batch number : 099, Training: Loss:  0.4604, Accuracy: 0.8438\n",
      "Batch number : 100, Training: Loss:  0.5728, Accuracy: 0.8438\n",
      "Batch number : 101, Training: Loss:  0.3519, Accuracy: 0.8906\n",
      "Batch number : 102, Training: Loss:  0.5528, Accuracy: 0.8125\n",
      "Batch number : 103, Training: Loss:  0.4130, Accuracy: 0.9062\n",
      "Batch number : 104, Training: Loss:  0.4770, Accuracy: 0.8594\n",
      "Batch number : 105, Training: Loss:  0.5166, Accuracy: 0.8281\n",
      "Batch number : 106, Training: Loss:  0.5823, Accuracy: 0.8281\n",
      "Batch number : 107, Training: Loss:  0.5697, Accuracy: 0.8281\n",
      "Batch number : 108, Training: Loss:  0.4653, Accuracy: 0.8438\n",
      "Batch number : 109, Training: Loss:  0.5781, Accuracy: 0.8125\n",
      "Batch number : 110, Training: Loss:  0.5165, Accuracy: 0.8281\n",
      "Batch number : 111, Training: Loss:  0.4389, Accuracy: 0.8594\n",
      "Batch number : 112, Training: Loss:  0.3909, Accuracy: 0.8594\n",
      "Batch number : 113, Training: Loss:  0.3738, Accuracy: 0.8594\n",
      "Batch number : 114, Training: Loss:  0.4468, Accuracy: 0.8594\n",
      "Batch number : 115, Training: Loss:  0.5059, Accuracy: 0.8594\n",
      "Batch number : 116, Training: Loss:  0.6089, Accuracy: 0.8594\n",
      "Batch number : 117, Training: Loss:  0.4511, Accuracy: 0.8906\n",
      "Batch number : 118, Training: Loss:  0.3692, Accuracy: 0.8906\n",
      "Batch number : 119, Training: Loss:  0.3666, Accuracy: 0.8906\n",
      "Batch number : 120, Training: Loss:  0.4182, Accuracy: 0.8750\n",
      "Batch number : 121, Training: Loss:  0.3416, Accuracy: 0.8594\n",
      "Batch number : 122, Training: Loss:  0.7671, Accuracy: 0.7656\n",
      "Batch number : 123, Training: Loss:  0.3998, Accuracy: 0.8594\n",
      "Batch number : 124, Training: Loss:  0.4793, Accuracy: 0.8125\n",
      "Batch number : 125, Training: Loss:  0.4858, Accuracy: 0.8125\n",
      "Batch number : 126, Training: Loss:  0.4193, Accuracy: 0.8594\n",
      "Batch number : 127, Training: Loss:  0.3655, Accuracy: 0.8750\n",
      "Batch number : 128, Training: Loss:  0.6792, Accuracy: 0.7969\n",
      "Batch number : 129, Training: Loss:  0.3495, Accuracy: 0.8750\n",
      "Batch number : 130, Training: Loss:  0.5780, Accuracy: 0.8125\n",
      "Batch number : 131, Training: Loss:  0.3457, Accuracy: 0.9062\n",
      "Batch number : 132, Training: Loss:  0.6947, Accuracy: 0.7969\n",
      "Batch number : 133, Training: Loss:  0.4212, Accuracy: 0.8750\n",
      "Batch number : 134, Training: Loss:  0.7403, Accuracy: 0.7656\n",
      "Batch number : 135, Training: Loss:  0.4108, Accuracy: 0.8750\n",
      "Batch number : 136, Training: Loss:  0.4685, Accuracy: 0.8906\n",
      "Batch number : 137, Training: Loss:  0.3943, Accuracy: 0.8750\n",
      "Batch number : 138, Training: Loss:  0.6166, Accuracy: 0.7969\n",
      "Batch number : 139, Training: Loss:  0.4724, Accuracy: 0.8594\n",
      "Batch number : 140, Training: Loss:  0.4307, Accuracy: 0.8750\n",
      "Batch number : 141, Training: Loss:  0.3541, Accuracy: 0.8906\n",
      "Batch number : 142, Training: Loss:  0.5016, Accuracy: 0.8750\n",
      "Batch number : 143, Training: Loss:  0.4718, Accuracy: 0.9062\n",
      "Batch number : 144, Training: Loss:  0.5237, Accuracy: 0.8438\n",
      "Batch number : 145, Training: Loss:  0.3396, Accuracy: 0.8750\n",
      "Batch number : 146, Training: Loss:  0.4053, Accuracy: 0.8906\n",
      "Batch number : 147, Training: Loss:  0.3751, Accuracy: 0.9062\n",
      "Batch number : 148, Training: Loss:  0.5070, Accuracy: 0.7969\n",
      "Batch number : 149, Training: Loss:  0.4002, Accuracy: 0.8750\n",
      "Batch number : 150, Training: Loss:  0.3347, Accuracy: 0.9062\n",
      "Batch number : 151, Training: Loss:  0.3953, Accuracy: 0.8906\n",
      "Batch number : 152, Training: Loss:  0.2969, Accuracy: 0.9688\n",
      "Batch number : 153, Training: Loss:  0.7674, Accuracy: 0.7188\n",
      "Batch number : 154, Training: Loss:  0.3175, Accuracy: 0.8906\n",
      "Batch number : 155, Training: Loss:  0.5320, Accuracy: 0.8281\n",
      "Batch number : 156, Training: Loss:  0.4909, Accuracy: 0.8594\n",
      "Batch number : 157, Training: Loss:  0.4309, Accuracy: 0.8594\n",
      "Batch number : 158, Training: Loss:  0.4357, Accuracy: 0.8750\n",
      "Batch number : 159, Training: Loss:  0.2741, Accuracy: 0.9062\n",
      "Batch number : 160, Training: Loss:  0.5033, Accuracy: 0.8594\n",
      "Batch number : 161, Training: Loss:  0.3448, Accuracy: 0.9062\n",
      "Batch number : 162, Training: Loss:  0.6581, Accuracy: 0.8281\n",
      "Batch number : 163, Training: Loss:  0.4212, Accuracy: 0.8750\n",
      "Batch number : 164, Training: Loss:  0.5866, Accuracy: 0.8125\n",
      "Batch number : 165, Training: Loss:  0.5753, Accuracy: 0.7969\n",
      "Batch number : 166, Training: Loss:  0.4722, Accuracy: 0.8594\n",
      "Batch number : 167, Training: Loss:  0.5426, Accuracy: 0.8594\n",
      "Batch number : 168, Training: Loss:  0.4227, Accuracy: 0.8750\n",
      "Batch number : 169, Training: Loss:  0.2558, Accuracy: 0.9531\n",
      "Batch number : 170, Training: Loss:  0.4431, Accuracy: 0.8906\n",
      "Batch number : 171, Training: Loss:  0.4056, Accuracy: 0.8594\n",
      "Batch number : 172, Training: Loss:  0.4910, Accuracy: 0.7812\n",
      "Batch number : 173, Training: Loss:  0.3499, Accuracy: 0.8750\n",
      "Batch number : 174, Training: Loss:  0.3976, Accuracy: 0.8750\n",
      "Batch number : 175, Training: Loss:  0.4723, Accuracy: 0.8438\n",
      "Batch number : 176, Training: Loss:  0.7167, Accuracy: 0.7969\n",
      "Batch number : 177, Training: Loss:  0.3377, Accuracy: 0.8750\n",
      "Batch number : 178, Training: Loss:  0.5214, Accuracy: 0.8438\n",
      "Batch number : 179, Training: Loss:  0.4172, Accuracy: 0.8750\n",
      "Batch number : 180, Training: Loss:  0.5265, Accuracy: 0.8125\n",
      "Batch number : 181, Training: Loss:  0.4004, Accuracy: 0.8594\n",
      "Batch number : 182, Training: Loss:  0.4102, Accuracy: 0.8750\n",
      "Batch number : 183, Training: Loss:  0.3751, Accuracy: 0.8750\n",
      "Batch number : 184, Training: Loss:  0.5784, Accuracy: 0.8125\n",
      "Batch number : 185, Training: Loss:  0.4334, Accuracy: 0.8594\n",
      "Batch number : 186, Training: Loss:  0.5213, Accuracy: 0.8438\n",
      "Batch number : 187, Training: Loss:  0.3336, Accuracy: 0.9219\n",
      "Batch number : 188, Training: Loss:  0.6350, Accuracy: 0.8281\n",
      "Batch number : 189, Training: Loss:  0.5890, Accuracy: 0.7812\n",
      "Batch number : 190, Training: Loss:  0.4890, Accuracy: 0.8438\n",
      "Batch number : 191, Training: Loss:  0.4431, Accuracy: 0.8594\n",
      "Batch number : 192, Training: Loss:  0.6184, Accuracy: 0.8750\n",
      "Batch number : 193, Training: Loss:  0.7766, Accuracy: 0.7656\n",
      "Batch number : 194, Training: Loss:  0.5400, Accuracy: 0.8125\n",
      "Batch number : 195, Training: Loss:  0.3875, Accuracy: 0.8750\n",
      "Batch number : 196, Training: Loss:  0.7418, Accuracy: 0.7812\n",
      "Batch number : 197, Training: Loss:  0.8538, Accuracy: 0.7500\n",
      "Batch number : 198, Training: Loss:  0.5139, Accuracy: 0.8750\n",
      "Batch number : 199, Training: Loss:  0.4035, Accuracy: 0.8906\n",
      "Batch number : 200, Training: Loss:  0.4763, Accuracy: 0.8750\n",
      "Batch number : 201, Training: Loss:  0.4033, Accuracy: 0.9062\n",
      "Batch number : 202, Training: Loss:  0.8134, Accuracy: 0.7812\n",
      "Batch number : 203, Training: Loss:  0.4949, Accuracy: 0.8594\n",
      "Batch number : 204, Training: Loss:  0.6396, Accuracy: 0.8281\n",
      "Batch number : 205, Training: Loss:  0.4604, Accuracy: 0.8750\n",
      "Batch number : 206, Training: Loss:  0.3671, Accuracy: 0.8750\n",
      "Batch number : 207, Training: Loss:  0.3676, Accuracy: 0.8906\n",
      "Batch number : 208, Training: Loss:  0.4891, Accuracy: 0.8438\n",
      "Batch number : 209, Training: Loss:  0.4509, Accuracy: 0.8438\n",
      "Batch number : 210, Training: Loss:  0.3987, Accuracy: 0.8906\n",
      "Batch number : 211, Training: Loss:  0.5841, Accuracy: 0.8125\n",
      "Batch number : 212, Training: Loss:  0.5035, Accuracy: 0.8438\n",
      "Batch number : 213, Training: Loss:  0.7092, Accuracy: 0.7656\n",
      "Batch number : 214, Training: Loss:  0.2961, Accuracy: 0.8906\n",
      "Batch number : 215, Training: Loss:  0.4142, Accuracy: 0.8906\n",
      "Batch number : 216, Training: Loss:  0.5222, Accuracy: 0.8281\n",
      "Batch number : 217, Training: Loss:  0.5070, Accuracy: 0.8906\n",
      "Batch number : 218, Training: Loss:  0.4157, Accuracy: 0.8750\n",
      "Batch number : 219, Training: Loss:  0.2659, Accuracy: 0.9219\n",
      "Batch number : 220, Training: Loss:  0.3475, Accuracy: 0.9062\n",
      "Batch number : 221, Training: Loss:  0.5712, Accuracy: 0.8594\n",
      "Batch number : 222, Training: Loss:  0.7831, Accuracy: 0.7188\n",
      "Batch number : 223, Training: Loss:  0.5593, Accuracy: 0.8125\n",
      "Batch number : 224, Training: Loss:  0.5211, Accuracy: 0.7969\n",
      "Batch number : 225, Training: Loss:  0.3390, Accuracy: 0.9219\n",
      "Batch number : 226, Training: Loss:  0.3998, Accuracy: 0.8750\n",
      "Batch number : 227, Training: Loss:  0.3962, Accuracy: 0.8594\n",
      "Batch number : 228, Training: Loss:  0.4632, Accuracy: 0.8594\n",
      "Batch number : 229, Training: Loss:  0.7475, Accuracy: 0.7500\n",
      "Batch number : 230, Training: Loss:  0.4167, Accuracy: 0.8750\n",
      "Batch number : 231, Training: Loss:  0.3913, Accuracy: 0.8438\n",
      "Batch number : 232, Training: Loss:  0.4619, Accuracy: 0.8750\n",
      "Batch number : 233, Training: Loss:  0.3611, Accuracy: 0.9062\n",
      "Batch number : 234, Training: Loss:  0.3457, Accuracy: 0.8750\n",
      "Batch number : 235, Training: Loss:  0.6910, Accuracy: 0.7969\n",
      "Batch number : 236, Training: Loss:  0.5157, Accuracy: 0.8594\n",
      "Batch number : 237, Training: Loss:  0.4785, Accuracy: 0.8125\n",
      "Batch number : 238, Training: Loss:  0.5771, Accuracy: 0.8281\n",
      "Batch number : 239, Training: Loss:  0.2396, Accuracy: 0.9688\n",
      "Batch number : 240, Training: Loss:  0.4745, Accuracy: 0.8125\n",
      "Batch number : 241, Training: Loss:  0.4389, Accuracy: 0.8594\n",
      "Batch number : 242, Training: Loss:  0.4989, Accuracy: 0.8281\n",
      "Batch number : 243, Training: Loss:  0.5085, Accuracy: 0.8438\n",
      "Batch number : 244, Training: Loss:  0.3330, Accuracy: 0.8750\n",
      "Batch number : 245, Training: Loss:  0.4705, Accuracy: 0.8281\n",
      "Batch number : 246, Training: Loss:  0.4375, Accuracy: 0.9219\n",
      "Batch number : 247, Training: Loss:  0.6109, Accuracy: 0.8594\n",
      "Batch number : 248, Training: Loss:  0.4668, Accuracy: 0.8594\n",
      "Batch number : 249, Training: Loss:  0.4508, Accuracy: 0.8438\n",
      "Batch number : 250, Training: Loss:  0.4109, Accuracy: 0.8750\n",
      "Batch number : 251, Training: Loss:  0.4777, Accuracy: 0.8750\n",
      "Batch number : 252, Training: Loss:  0.4961, Accuracy: 0.9062\n",
      "Batch number : 253, Training: Loss:  0.4883, Accuracy: 0.8594\n",
      "Batch number : 254, Training: Loss:  0.4391, Accuracy: 0.8750\n",
      "Batch number : 255, Training: Loss:  0.3855, Accuracy: 0.8906\n",
      "Batch number : 256, Training: Loss:  0.4756, Accuracy: 0.8438\n",
      "Batch number : 257, Training: Loss:  0.4184, Accuracy: 0.8438\n",
      "Batch number : 258, Training: Loss:  0.3865, Accuracy: 0.8750\n",
      "Batch number : 259, Training: Loss:  0.3919, Accuracy: 0.8906\n",
      "Batch number : 260, Training: Loss:  0.5106, Accuracy: 0.8438\n",
      "Batch number : 261, Training: Loss:  0.5874, Accuracy: 0.8125\n",
      "Batch number : 262, Training: Loss:  0.5825, Accuracy: 0.7969\n",
      "Batch number : 263, Training: Loss:  0.4582, Accuracy: 0.8594\n",
      "Batch number : 264, Training: Loss:  0.3470, Accuracy: 0.9375\n",
      "Batch number : 265, Training: Loss:  0.3857, Accuracy: 0.9062\n",
      "Batch number : 266, Training: Loss:  0.5491, Accuracy: 0.8125\n",
      "Batch number : 267, Training: Loss:  0.4300, Accuracy: 0.9062\n",
      "Batch number : 268, Training: Loss:  0.4322, Accuracy: 0.9062\n",
      "Batch number : 269, Training: Loss:  0.3662, Accuracy: 0.8750\n",
      "Batch number : 270, Training: Loss:  0.3546, Accuracy: 0.8906\n",
      "Batch number : 271, Training: Loss:  0.6594, Accuracy: 0.7969\n",
      "Batch number : 272, Training: Loss:  0.6143, Accuracy: 0.8281\n",
      "Batch number : 273, Training: Loss:  0.3694, Accuracy: 0.8906\n",
      "Batch number : 274, Training: Loss:  0.3870, Accuracy: 0.8906\n",
      "Batch number : 275, Training: Loss:  0.4359, Accuracy: 0.8594\n",
      "Batch number : 276, Training: Loss:  0.6365, Accuracy: 0.8281\n",
      "Batch number : 277, Training: Loss:  0.4013, Accuracy: 0.9219\n",
      "Batch number : 278, Training: Loss:  0.7096, Accuracy: 0.7969\n",
      "Batch number : 279, Training: Loss:  0.3637, Accuracy: 0.8906\n",
      "Batch number : 280, Training: Loss:  0.5794, Accuracy: 0.8438\n",
      "Batch number : 281, Training: Loss:  0.4430, Accuracy: 0.8438\n",
      "Batch number : 282, Training: Loss:  0.3862, Accuracy: 0.9062\n",
      "Batch number : 283, Training: Loss:  0.4926, Accuracy: 0.8750\n",
      "Batch number : 284, Training: Loss:  0.4596, Accuracy: 0.8594\n",
      "Batch number : 285, Training: Loss:  0.4623, Accuracy: 0.8906\n",
      "Batch number : 286, Training: Loss:  0.5267, Accuracy: 0.8594\n",
      "Batch number : 287, Training: Loss:  0.4110, Accuracy: 0.8750\n",
      "Batch number : 288, Training: Loss:  0.4658, Accuracy: 0.8281\n",
      "Batch number : 289, Training: Loss:  0.4986, Accuracy: 0.8750\n",
      "Batch number : 290, Training: Loss:  0.5430, Accuracy: 0.7969\n",
      "Batch number : 291, Training: Loss:  0.4260, Accuracy: 0.8906\n",
      "Batch number : 292, Training: Loss:  0.4864, Accuracy: 0.8125\n",
      "Batch number : 293, Training: Loss:  0.3987, Accuracy: 0.8750\n",
      "Batch number : 294, Training: Loss:  0.3704, Accuracy: 0.8906\n",
      "Batch number : 295, Training: Loss:  0.4382, Accuracy: 0.8594\n",
      "Batch number : 296, Training: Loss:  0.3293, Accuracy: 0.9219\n",
      "Batch number : 297, Training: Loss:  0.3938, Accuracy: 0.9375\n",
      "Batch number : 298, Training: Loss:  0.4046, Accuracy: 0.8438\n",
      "Batch number : 299, Training: Loss:  0.5232, Accuracy: 0.8438\n",
      "Batch number : 300, Training: Loss:  0.3902, Accuracy: 0.8906\n",
      "Batch number : 301, Training: Loss:  0.2884, Accuracy: 0.9062\n",
      "Batch number : 302, Training: Loss:  0.3700, Accuracy: 0.9062\n",
      "Batch number : 303, Training: Loss:  0.4230, Accuracy: 0.8594\n",
      "Batch number : 304, Training: Loss:  0.4247, Accuracy: 0.8906\n",
      "Batch number : 305, Training: Loss:  0.4567, Accuracy: 0.8750\n",
      "Batch number : 306, Training: Loss:  0.5835, Accuracy: 0.8438\n",
      "Batch number : 307, Training: Loss:  0.5915, Accuracy: 0.8750\n",
      "Batch number : 308, Training: Loss:  0.3356, Accuracy: 0.9219\n",
      "Batch number : 309, Training: Loss:  0.2957, Accuracy: 0.9375\n",
      "Batch number : 310, Training: Loss:  0.6421, Accuracy: 0.7812\n",
      "Batch number : 311, Training: Loss:  0.5615, Accuracy: 0.8281\n",
      "Batch number : 312, Training: Loss:  0.4526, Accuracy: 0.9062\n",
      "Batch number : 313, Training: Loss:  0.3818, Accuracy: 0.8750\n",
      "Batch number : 314, Training: Loss:  0.5094, Accuracy: 0.8594\n",
      "Batch number : 315, Training: Loss:  0.4335, Accuracy: 0.8750\n",
      "Batch number : 316, Training: Loss:  0.3518, Accuracy: 0.8750\n",
      "Batch number : 317, Training: Loss:  0.4460, Accuracy: 0.8750\n",
      "Batch number : 318, Training: Loss:  0.5924, Accuracy: 0.8125\n",
      "Batch number : 319, Training: Loss:  0.5090, Accuracy: 0.8594\n",
      "Batch number : 320, Training: Loss:  0.3193, Accuracy: 0.9062\n",
      "Batch number : 321, Training: Loss:  0.4777, Accuracy: 0.8906\n",
      "Batch number : 322, Training: Loss:  0.5080, Accuracy: 0.8438\n",
      "Batch number : 323, Training: Loss:  0.4515, Accuracy: 0.8906\n",
      "Batch number : 324, Training: Loss:  0.6483, Accuracy: 0.7656\n",
      "Batch number : 325, Training: Loss:  0.3286, Accuracy: 0.9375\n",
      "Batch number : 326, Training: Loss:  0.6090, Accuracy: 0.7812\n",
      "Batch number : 327, Training: Loss:  0.5107, Accuracy: 0.8281\n",
      "Batch number : 328, Training: Loss:  0.5261, Accuracy: 0.8594\n",
      "Batch number : 329, Training: Loss:  0.3191, Accuracy: 0.9219\n",
      "Batch number : 330, Training: Loss:  0.3941, Accuracy: 0.8750\n",
      "Batch number : 331, Training: Loss:  0.3223, Accuracy: 0.9062\n",
      "Batch number : 332, Training: Loss:  0.6338, Accuracy: 0.7812\n",
      "Batch number : 333, Training: Loss:  0.3652, Accuracy: 0.9062\n",
      "Batch number : 334, Training: Loss:  0.4148, Accuracy: 0.8750\n",
      "Batch number : 335, Training: Loss:  0.4884, Accuracy: 0.8438\n",
      "Batch number : 336, Training: Loss:  0.4161, Accuracy: 0.8906\n",
      "Batch number : 337, Training: Loss:  0.4704, Accuracy: 0.8594\n",
      "Batch number : 338, Training: Loss:  0.4427, Accuracy: 0.8750\n",
      "Batch number : 339, Training: Loss:  0.6642, Accuracy: 0.8125\n",
      "Batch number : 340, Training: Loss:  0.6049, Accuracy: 0.8281\n",
      "Batch number : 341, Training: Loss:  0.3647, Accuracy: 0.9219\n",
      "Batch number : 342, Training: Loss:  0.4762, Accuracy: 0.8906\n",
      "Batch number : 343, Training: Loss:  0.5506, Accuracy: 0.8125\n",
      "Batch number : 344, Training: Loss:  0.4497, Accuracy: 0.8750\n",
      "Batch number : 345, Training: Loss:  0.5020, Accuracy: 0.8281\n",
      "Batch number : 346, Training: Loss:  0.5261, Accuracy: 0.8281\n",
      "Batch number : 347, Training: Loss:  0.5821, Accuracy: 0.8281\n",
      "Batch number : 348, Training: Loss:  0.4860, Accuracy: 0.8594\n",
      "Batch number : 349, Training: Loss:  0.4626, Accuracy: 0.8438\n",
      "Batch number : 350, Training: Loss:  0.3998, Accuracy: 0.9062\n",
      "Batch number : 351, Training: Loss:  0.5840, Accuracy: 0.8438\n",
      "Batch number : 352, Training: Loss:  0.3078, Accuracy: 0.9062\n",
      "Batch number : 353, Training: Loss:  0.3144, Accuracy: 0.9219\n",
      "Batch number : 354, Training: Loss:  0.3970, Accuracy: 0.8906\n",
      "Batch number : 355, Training: Loss:  0.5332, Accuracy: 0.8594\n",
      "Batch number : 356, Training: Loss:  0.2233, Accuracy: 0.9531\n",
      "Batch number : 357, Training: Loss:  0.4940, Accuracy: 0.8438\n",
      "Batch number : 358, Training: Loss:  0.7505, Accuracy: 0.7812\n",
      "Batch number : 359, Training: Loss:  0.5115, Accuracy: 0.8750\n",
      "Batch number : 360, Training: Loss:  0.4343, Accuracy: 0.8750\n",
      "Batch number : 361, Training: Loss:  0.3169, Accuracy: 0.9219\n",
      "Batch number : 362, Training: Loss:  0.3808, Accuracy: 0.8906\n",
      "Batch number : 363, Training: Loss:  0.5703, Accuracy: 0.8281\n",
      "Batch number : 364, Training: Loss:  0.4111, Accuracy: 0.8906\n",
      "Batch number : 365, Training: Loss:  0.4451, Accuracy: 0.8594\n",
      "Batch number : 366, Training: Loss:  0.2764, Accuracy: 0.9375\n",
      "Batch number : 367, Training: Loss:  0.3716, Accuracy: 0.8750\n",
      "Batch number : 368, Training: Loss:  0.4933, Accuracy: 0.8281\n",
      "Batch number : 369, Training: Loss:  0.4796, Accuracy: 0.8750\n",
      "Batch number : 370, Training: Loss:  0.2570, Accuracy: 0.9375\n",
      "Batch number : 371, Training: Loss:  0.6297, Accuracy: 0.7969\n",
      "Batch number : 372, Training: Loss:  0.4875, Accuracy: 0.8594\n",
      "Batch number : 373, Training: Loss:  0.2912, Accuracy: 0.9062\n",
      "Batch number : 374, Training: Loss:  0.5603, Accuracy: 0.8125\n",
      "Batch number : 375, Training: Loss:  0.6603, Accuracy: 0.7812\n",
      "Batch number : 376, Training: Loss:  0.4720, Accuracy: 0.8438\n",
      "Batch number : 377, Training: Loss:  0.5687, Accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "    # set to training mode\n",
    "    model.train()\n",
    "    # loss and accuracy with in epochs\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_data):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # clean the existing gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        #cumpute loss\n",
    "        loss = loss_fn(outputs,labels)\n",
    "        #back propagate the gradients\n",
    "        loss.backward()\n",
    "        #update the parameters\n",
    "        optimizer.step()\n",
    "        #compute the total loss for the batch\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        #compute the accuracy\n",
    "        ret, predictions = torch.max(outputs.data,1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "        # convert the correct counts to float and then compute the mean\n",
    "        acc= torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        #compute the whole accuracy in the batch\n",
    "        train_acc += acc.item() * inputs.size(0)\n",
    "        print(\"Batch number : {:03d}, Training: Loss:  {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(),acc.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 000, Validation: Loss: 0.5872, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0062,Accuracy: 0.9082%, Time: 291.7234s\n",
      "Validation Batch number: 001, Validation: Loss: 0.6307, Accuracy: 0.7500\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0129,Accuracy: 1.7008%, Time: 292.6065s\n",
      "Validation Batch number: 002, Validation: Loss: 0.5961, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0192,Accuracy: 2.5760%, Time: 293.4182s\n",
      "Validation Batch number: 003, Validation: Loss: 0.2556, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0219,Accuracy: 3.5667%, Time: 294.2586s\n",
      "Validation Batch number: 004, Validation: Loss: 0.3738, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0258,Accuracy: 4.5244%, Time: 295.1335s\n",
      "Validation Batch number: 005, Validation: Loss: 0.4209, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0303,Accuracy: 5.4161%, Time: 295.9417s\n",
      "Validation Batch number: 006, Validation: Loss: 0.4993, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0355,Accuracy: 6.3078%, Time: 296.7386s\n",
      "Validation Batch number: 007, Validation: Loss: 0.4485, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0403,Accuracy: 7.2325%, Time: 297.5578s\n",
      "Validation Batch number: 008, Validation: Loss: 0.4806, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0454,Accuracy: 8.1077%, Time: 298.3547s\n",
      "Validation Batch number: 009, Validation: Loss: 0.4451, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0501,Accuracy: 8.9993%, Time: 299.1574s\n",
      "Validation Batch number: 010, Validation: Loss: 0.2798, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0530,Accuracy: 9.9571%, Time: 299.9758s\n",
      "Validation Batch number: 011, Validation: Loss: 0.3798, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0570,Accuracy: 10.9148%, Time: 300.7861s\n",
      "Validation Batch number: 012, Validation: Loss: 0.4419, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0617,Accuracy: 11.8395%, Time: 301.5895s\n",
      "Validation Batch number: 013, Validation: Loss: 0.4884, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0669,Accuracy: 12.7312%, Time: 302.4328s\n",
      "Validation Batch number: 014, Validation: Loss: 0.5756, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0730,Accuracy: 13.5898%, Time: 303.2551s\n",
      "Validation Batch number: 015, Validation: Loss: 0.5526, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0788,Accuracy: 14.5145%, Time: 304.0864s\n",
      "Validation Batch number: 016, Validation: Loss: 0.4245, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0833,Accuracy: 15.4392%, Time: 304.9177s\n",
      "Validation Batch number: 017, Validation: Loss: 0.6583, Accuracy: 0.7812\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0902,Accuracy: 16.2649%, Time: 305.8140s\n",
      "Validation Batch number: 018, Validation: Loss: 0.3360, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0938,Accuracy: 17.2391%, Time: 306.6184s\n",
      "Validation Batch number: 019, Validation: Loss: 0.5068, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.0991,Accuracy: 18.1473%, Time: 307.3963s\n",
      "Validation Batch number: 020, Validation: Loss: 0.5425, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1049,Accuracy: 19.0390%, Time: 308.2158s\n",
      "Validation Batch number: 021, Validation: Loss: 0.3975, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1091,Accuracy: 19.9802%, Time: 309.0526s\n",
      "Validation Batch number: 022, Validation: Loss: 0.3832, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1131,Accuracy: 20.8719%, Time: 309.8483s\n",
      "Validation Batch number: 023, Validation: Loss: 0.2401, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1157,Accuracy: 21.8626%, Time: 310.6811s\n",
      "Validation Batch number: 024, Validation: Loss: 0.4430, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1203,Accuracy: 22.7543%, Time: 311.4985s\n",
      "Validation Batch number: 025, Validation: Loss: 0.4102, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1247,Accuracy: 23.6625%, Time: 312.3326s\n",
      "Validation Batch number: 026, Validation: Loss: 0.2942, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1278,Accuracy: 24.6367%, Time: 313.1840s\n",
      "Validation Batch number: 027, Validation: Loss: 0.3592, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1316,Accuracy: 25.5614%, Time: 313.9975s\n",
      "Validation Batch number: 028, Validation: Loss: 0.3658, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1355,Accuracy: 26.5026%, Time: 314.7923s\n",
      "Validation Batch number: 029, Validation: Loss: 0.4817, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1405,Accuracy: 27.3778%, Time: 315.6002s\n",
      "Validation Batch number: 030, Validation: Loss: 0.5029, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1459,Accuracy: 28.2860%, Time: 316.4341s\n",
      "Validation Batch number: 031, Validation: Loss: 0.3477, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1495,Accuracy: 29.2272%, Time: 317.2384s\n",
      "Validation Batch number: 032, Validation: Loss: 0.2640, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1523,Accuracy: 30.2180%, Time: 318.0867s\n",
      "Validation Batch number: 033, Validation: Loss: 0.5546, Accuracy: 0.7969\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1582,Accuracy: 31.0601%, Time: 318.9276s\n",
      "Validation Batch number: 034, Validation: Loss: 0.5091, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1636,Accuracy: 31.9188%, Time: 319.7982s\n",
      "Validation Batch number: 035, Validation: Loss: 0.3622, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1674,Accuracy: 32.8600%, Time: 320.6738s\n",
      "Validation Batch number: 036, Validation: Loss: 0.5056, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1727,Accuracy: 33.7517%, Time: 321.4862s\n",
      "Validation Batch number: 037, Validation: Loss: 0.4909, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1779,Accuracy: 34.6433%, Time: 322.2926s\n",
      "Validation Batch number: 038, Validation: Loss: 0.5092, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1833,Accuracy: 35.5350%, Time: 323.1226s\n",
      "Validation Batch number: 039, Validation: Loss: 0.3479, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1870,Accuracy: 36.4597%, Time: 323.9186s\n",
      "Validation Batch number: 040, Validation: Loss: 0.6388, Accuracy: 0.7969\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1937,Accuracy: 37.3018%, Time: 324.7297s\n",
      "Validation Batch number: 041, Validation: Loss: 0.3459, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.1974,Accuracy: 38.2431%, Time: 325.5100s\n",
      "Validation Batch number: 042, Validation: Loss: 0.6426, Accuracy: 0.7812\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2042,Accuracy: 39.0687%, Time: 326.3156s\n",
      "Validation Batch number: 043, Validation: Loss: 0.4877, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2093,Accuracy: 39.9439%, Time: 327.1441s\n",
      "Validation Batch number: 044, Validation: Loss: 0.6264, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2160,Accuracy: 40.8520%, Time: 327.9717s\n",
      "Validation Batch number: 045, Validation: Loss: 0.5751, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2220,Accuracy: 41.7437%, Time: 328.7796s\n",
      "Validation Batch number: 046, Validation: Loss: 0.5623, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2280,Accuracy: 42.6024%, Time: 329.6218s\n",
      "Validation Batch number: 047, Validation: Loss: 0.3896, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2321,Accuracy: 43.5271%, Time: 330.5663s\n",
      "Validation Batch number: 048, Validation: Loss: 0.4353, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2367,Accuracy: 44.4353%, Time: 331.4494s\n",
      "Validation Batch number: 049, Validation: Loss: 0.3642, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2405,Accuracy: 45.3435%, Time: 332.2599s\n",
      "Validation Batch number: 050, Validation: Loss: 0.5956, Accuracy: 0.7656\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2468,Accuracy: 46.1526%, Time: 333.1227s\n",
      "Validation Batch number: 051, Validation: Loss: 0.5893, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2531,Accuracy: 47.0277%, Time: 333.9625s\n",
      "Validation Batch number: 052, Validation: Loss: 0.2632, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2558,Accuracy: 48.0185%, Time: 334.7584s\n",
      "Validation Batch number: 053, Validation: Loss: 0.4795, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2609,Accuracy: 48.9267%, Time: 335.5339s\n",
      "Validation Batch number: 054, Validation: Loss: 0.2906, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2640,Accuracy: 49.9009%, Time: 336.3566s\n",
      "Validation Batch number: 055, Validation: Loss: 0.5623, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2699,Accuracy: 50.7596%, Time: 337.1970s\n",
      "Validation Batch number: 056, Validation: Loss: 0.5058, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2753,Accuracy: 51.6678%, Time: 338.1088s\n",
      "Validation Batch number: 057, Validation: Loss: 0.3779, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2793,Accuracy: 52.5760%, Time: 338.8904s\n",
      "Validation Batch number: 058, Validation: Loss: 0.2965, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2824,Accuracy: 53.5337%, Time: 339.7052s\n",
      "Validation Batch number: 059, Validation: Loss: 0.3654, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2863,Accuracy: 54.4584%, Time: 340.5285s\n",
      "Validation Batch number: 060, Validation: Loss: 0.4693, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2912,Accuracy: 55.3501%, Time: 341.3296s\n",
      "Validation Batch number: 061, Validation: Loss: 0.3039, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.2944,Accuracy: 56.3243%, Time: 342.1604s\n",
      "Validation Batch number: 062, Validation: Loss: 0.5446, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3002,Accuracy: 57.2325%, Time: 342.9685s\n",
      "Validation Batch number: 063, Validation: Loss: 0.2876, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3032,Accuracy: 58.2067%, Time: 343.8028s\n",
      "Validation Batch number: 064, Validation: Loss: 0.4573, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3081,Accuracy: 59.1480%, Time: 344.6410s\n",
      "Validation Batch number: 065, Validation: Loss: 0.4693, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3130,Accuracy: 60.0231%, Time: 345.4710s\n",
      "Validation Batch number: 066, Validation: Loss: 0.6300, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3197,Accuracy: 60.8818%, Time: 346.3122s\n",
      "Validation Batch number: 067, Validation: Loss: 0.2881, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3227,Accuracy: 61.8395%, Time: 347.1864s\n",
      "Validation Batch number: 068, Validation: Loss: 0.5129, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3281,Accuracy: 62.7147%, Time: 347.9901s\n",
      "Validation Batch number: 069, Validation: Loss: 0.4283, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3327,Accuracy: 63.6394%, Time: 348.8011s\n",
      "Validation Batch number: 070, Validation: Loss: 0.4446, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3374,Accuracy: 64.5145%, Time: 349.5956s\n",
      "Validation Batch number: 071, Validation: Loss: 0.6856, Accuracy: 0.7812\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3446,Accuracy: 65.3402%, Time: 350.3644s\n",
      "Validation Batch number: 072, Validation: Loss: 0.3388, Accuracy: 0.8906\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3482,Accuracy: 66.2814%, Time: 351.2119s\n",
      "Validation Batch number: 073, Validation: Loss: 0.3124, Accuracy: 0.9375\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3515,Accuracy: 67.2721%, Time: 352.0544s\n",
      "Validation Batch number: 074, Validation: Loss: 0.5810, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3576,Accuracy: 68.1308%, Time: 352.8777s\n",
      "Validation Batch number: 075, Validation: Loss: 0.5049, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3630,Accuracy: 69.0059%, Time: 353.7055s\n",
      "Validation Batch number: 076, Validation: Loss: 0.3011, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3661,Accuracy: 69.9637%, Time: 354.5120s\n",
      "Validation Batch number: 077, Validation: Loss: 0.2737, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3690,Accuracy: 70.9214%, Time: 355.4215s\n",
      "Validation Batch number: 078, Validation: Loss: 0.2513, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3717,Accuracy: 71.8956%, Time: 356.2472s\n",
      "Validation Batch number: 079, Validation: Loss: 0.3671, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3756,Accuracy: 72.8203%, Time: 357.0495s\n",
      "Validation Batch number: 080, Validation: Loss: 0.5530, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3814,Accuracy: 73.6790%, Time: 357.8584s\n",
      "Validation Batch number: 081, Validation: Loss: 0.3418, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3850,Accuracy: 74.6037%, Time: 358.6374s\n",
      "Validation Batch number: 082, Validation: Loss: 0.3071, Accuracy: 0.9062\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3883,Accuracy: 75.5614%, Time: 359.4655s\n",
      "Validation Batch number: 083, Validation: Loss: 0.6226, Accuracy: 0.7656\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3949,Accuracy: 76.3705%, Time: 360.2753s\n",
      "Validation Batch number: 084, Validation: Loss: 0.4110, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.3992,Accuracy: 77.3448%, Time: 361.0848s\n",
      "Validation Batch number: 085, Validation: Loss: 0.6056, Accuracy: 0.8281\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4056,Accuracy: 78.2199%, Time: 361.8792s\n",
      "Validation Batch number: 086, Validation: Loss: 0.5901, Accuracy: 0.8438\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4118,Accuracy: 79.1116%, Time: 362.6982s\n",
      "Validation Batch number: 087, Validation: Loss: 0.3266, Accuracy: 0.9219\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4153,Accuracy: 80.0859%, Time: 363.5187s\n",
      "Validation Batch number: 088, Validation: Loss: 0.4730, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4203,Accuracy: 80.9941%, Time: 364.3408s\n",
      "Validation Batch number: 089, Validation: Loss: 0.5726, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4263,Accuracy: 81.9022%, Time: 365.1108s\n",
      "Validation Batch number: 090, Validation: Loss: 0.3709, Accuracy: 0.8750\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4303,Accuracy: 82.8269%, Time: 365.9058s\n",
      "Validation Batch number: 091, Validation: Loss: 0.6535, Accuracy: 0.7969\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4372,Accuracy: 83.6691%, Time: 366.7218s\n",
      "Validation Batch number: 092, Validation: Loss: 0.4380, Accuracy: 0.8594\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4418,Accuracy: 84.5773%, Time: 367.4939s\n",
      "Validation Batch number: 093, Validation: Loss: 0.4963, Accuracy: 0.8125\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4470,Accuracy: 85.4359%, Time: 368.3360s\n",
      "Validation Batch number: 094, Validation: Loss: 0.5386, Accuracy: 0.8250\n",
      "Epoch : 019, Training: Loss: 0.4694, Accuracy: 85.9442%, nttValidation : Loss : 0.4506,Accuracy: 85.9808%, Time: 368.8372s\n"
     ]
    }
   ],
   "source": [
    "# Validation - No gradient tracking needed\n",
    "history =[]\n",
    "with torch.no_grad():\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    # Validation loop\n",
    "    for j, (inputs, labels) in enumerate(valid_data):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = model(inputs)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        valid_loss += loss.item() * inputs.size(0)\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        valid_acc += acc.item() * inputs.size(0)\n",
    "        print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/float(train_data_size)\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/float(valid_data_size)\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "        epoch_end = time.time()\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f},Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_image_name):\n",
    "    transform = image_transforms['test']\n",
    "    test_image = Image.open(test_image_name)\n",
    "    plt.imshow(test_image)\n",
    "    test_image_tensor = transform(test_image)\n",
    "    if torch.cuda.is_available():\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
    "    else:\n",
    "        test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Model outputs log probabilities\n",
    "        out = model(test_image_tensor)\n",
    "        ps = torch.exp(out)\n",
    "        topk, topclass = ps.topk(1, dim=1)\n",
    "        print(\"Output class :  \", idx_to_class[topclass.cpu().numpy()[0][0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5c6f10dc4b242dc12fc0ba4737889adb5eaae4c45e53ca09201bf06b18980fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
